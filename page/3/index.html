<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32*32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16*16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.css" integrity="sha256-no0c5ccDODBwp+9hSmV5VvPpKwHCpbVzXHexIkupM6U=" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js" integrity="sha256-a5YRB27CcBwBFcT5EF/f3E4vzIqyHrSR878nseNYw64=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;lemcoden.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Pisces&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="Lemcoden">
<meta property="og:url" content="https://lemcoden.github.io/page/3/index.html">
<meta property="og:site_name" content="Lemcoden">
<meta property="og:locale">
<meta property="article:author" content="Lemcoden">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://lemcoden.github.io/page/3/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;zh-Hans&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;3&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Lemcoden</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?5559683623f16c6f10fe5b4f1cb2f062"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Lemcoden</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">来自于大数据攻城狮的分享</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lemcoden"
      src="http://picture.lemcoden.xyz/avater.jpeg">
  <p class="site-author-name" itemprop="name">Lemcoden</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/11/09/Centos7%E7%9A%84mysql%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/09/Centos7%E7%9A%84mysql%E5%AE%89%E8%A3%85/" class="post-title-link" itemprop="url">Centos7的mysql安装</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-11-09 13:29:47" itemprop="dateCreated datePublished" datetime="2020-11-09T13:29:47+08:00">2020-11-09</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-15 16:46:40" itemprop="dateModified" datetime="2020-11-15T16:46:40+08:00">2020-11-15</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/linux%E7%8E%AF%E5%A2%83/" itemprop="url" rel="index"><span itemprop="name">linux环境</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="下载官方mysql源"><a href="#下载官方mysql源" class="headerlink" title="下载官方mysql源"></a>下载官方mysql源</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure>

<h4 id="加载rpm源"><a href="#加载rpm源" class="headerlink" title="加载rpm源"></a>加载rpm源</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/11/09/Centos7%E7%9A%84mysql%E5%AE%89%E8%A3%85/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/09/22/%E6%9A%82%E6%97%B6%E8%AE%B0%E5%BD%95%E7%9A%84tips/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/22/%E6%9A%82%E6%97%B6%E8%AE%B0%E5%BD%95%E7%9A%84tips/" class="post-title-link" itemprop="url">暂时记录的tips</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-22 10:59:19" itemprop="dateCreated datePublished" datetime="2020-09-22T10:59:19+08:00">2020-09-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-12-28 22:16:01" itemprop="dateModified" datetime="2020-12-28T22:16:01+08:00">2020-12-28</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="hadoop-mr-HistoryServer的配置和启动命令"><a href="#hadoop-mr-HistoryServer的配置和启动命令" class="headerlink" title="hadoop mr HistoryServer的配置和启动命令"></a>hadoop mr HistoryServer的配置和启动命令</h4><p>mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;node04:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;node04:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/09/22/%E6%9A%82%E6%97%B6%E8%AE%B0%E5%BD%95%E7%9A%84tips/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/09/15/hbase%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/15/hbase%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/" class="post-title-link" itemprop="url">hbase笔记总结02</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-15 21:56:01" itemprop="dateCreated datePublished" datetime="2020-09-15T21:56:01+08:00">2020-09-15</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-01-07 22:27:34" itemprop="dateModified" datetime="2021-01-07T22:27:34+08:00">2021-01-07</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="HBase优化设计"><a href="#HBase优化设计" class="headerlink" title="HBase优化设计"></a>HBase优化设计</h1><h3 id="1、表的设计"><a href="#1、表的设计" class="headerlink" title="1、表的设计"></a>1、表的设计</h3><h5 id="1、Pre-Creating-Regions"><a href="#1、Pre-Creating-Regions" class="headerlink" title="1、Pre-Creating Regions"></a>1、Pre-Creating Regions</h5><p>​        默认情况下，在创建HBase表的时候会自动创建一个region分区，当导入数据的时候，所有的HBase客户端都向这一个region写数据，直到这个region足够大了才进行切分。一种可以加快批量写入速度的方法是通过预先创建一些空的regions，这样当数据写入HBase时，会按照region分区情况，在集群内做数据的负载均衡。    </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//第一种实现方式是使用admin对象的切分策略</span></span><br><span class="line"><span class="keyword">byte</span>[] startKey = ...;      <span class="comment">// your lowest key</span></span><br><span class="line"><span class="keyword">byte</span>[] endKey = ...;        <span class="comment">// your highest key</span></span><br><span class="line"><span class="keyword">int</span> numberOfRegions = ...;  <span class="comment">// # of regions to create</span></span><br><span class="line">admin.createTable(table, startKey, endKey, numberOfRegions);</span><br><span class="line"><span class="comment">//第二种实现方式是用户自定义切片</span></span><br><span class="line"><span class="keyword">byte</span>[][] splits = ...;   <span class="comment">// create your own splits</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">byte[][] splits = new byte[][] &#123; Bytes.toBytes(&quot;100&quot;),</span></span><br><span class="line"><span class="comment">                Bytes.toBytes(&quot;200&quot;), Bytes.toBytes(&quot;400&quot;),</span></span><br><span class="line"><span class="comment">                Bytes.toBytes(&quot;500&quot;) &#125;;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">admin.createTable(table, splits);</span><br></pre></td></tr></table></figure>

<h5 id="2、Rowkey设计"><a href="#2、Rowkey设计" class="headerlink" title="2、Rowkey设计"></a>2、Rowkey设计</h5><p>​        HBase中row key用来检索表中的记录，支持以下三种方式：</p>
<p>​            1、通过单个row key访问：即按照某个row key键值进行get操作；</p>
<p>​            2、通过row key的range进行scan：即通过设置startRowKey和endRowKey，在这个范围内进行扫描；</p>
<p>​            3、全表扫描：即直接扫描整张表中所有行记录。</p>
<p>​        在HBase中，rowkey可以是任意字符串，最大长度64KB，实际应用中一般为10~100bytes，存为byte[]字节数组，一般设计成定长的。<br>​        rowkey是按照字典序存储，因此，设计row key时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。</p>
<p>​        <strong>Rowkey设计原则：</strong></p>
<p>​        <strong>1、越短越好，提高效率</strong></p>
<p>​        （1）数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如操作100字节，1000万行数据，单单是存储rowkey的数据就要占用10亿个字节，将近1G数据，这样会影响HFile的存储效率。</p>
<p>​        （2）HBase中包含缓存机制，每次会将查询的结果暂时缓存到HBase的内存中，如果rowkey字段过长，内存的利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。</p>
<p>​        <strong>2、散列原则–实现负载均衡</strong></p>
<p>​        如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。</p>
<p>​        （1）加盐：添加随机值</p>
<p>​        （2）hash：采用md5散列算法取前4位做前缀</p>
<p>​        （3）反转：将手机号反转</p>
<p>​        <strong>3、唯一原则–字典序排序存储</strong></p>
<p>​        必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。        </p>
<h5 id="3、列族的设计"><a href="#3、列族的设计" class="headerlink" title="3、列族的设计"></a>3、列族的设计</h5><p>​        <strong>不要在一张表里定义太多的column family</strong>。目前Hbase并不能很好的处理超过2~3个column family的表。因为某个column family在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O。    </p>
<p>​        原因：</p>
<p>​        1、当开始向hbase中插入数据的时候，数据会首先写入到memstore，而memstore是一个内存结构，每个列族对应一个memstore，当包含更多的列族的时候，会导致存在多个memstore，每一个memstore在flush的时候会对应一个hfile的文件，因此会产生很多的hfile文件，更加严重的是，flush操作时region级别，当region中的某个memstore被flush的时候，同一个region的其他memstore也会进行flush操作，当某一张表拥有很多列族的时候，且列族之间的数据分布不均匀的时候，会产生更多的磁盘文件。</p>
<p>​        2、当hbase表的某个region过大，会被拆分成两个，如果我们有多个列族，且这些列族之间的数据量相差悬殊的时候，region的split操作会导致原本数据量小的文件被进一步的拆分，而产生更多的小文件</p>
<p>​        3、与 Flush 操作一样，目前 HBase 的 Compaction 操作也是 Region 级别的，过多的列族也会产生不必要的 IO。                    </p>
<p>​        4、HDFS 其实对一个目录下的文件数有限制的（<code>dfs.namenode.fs-limits.max-directory-items</code>）。如果我们有 N 个列族，M 个 Region，那么我们持久化到 HDFS 至少会产生 N<em>M 个文件；而每个列族对应底层的 HFile 文件往往不止一个，我们假设为 K 个，那么最终表在 HDFS 目录下的文件数将是 N</em>M*K，这可能会操作 HDFS 的限制。</p>
<h5 id="4、in-memory"><a href="#4、in-memory" class="headerlink" title="4、in memory"></a>4、in memory</h5><p>​        hbase在LRU缓存基础之上采用了分层设计，整个blockcache分成了三个部分，分别是single、multi和inMemory。</p>
<p>​        三者区别如下：<br>​        single：如果一个block第一次被访问，放在该优先队列中；<br>​        multi：如果一个block被多次访问，则从single队列转移到multi队列<br>​        inMemory：优先级最高，常驻cache，因此一般只有hbase系统的元数据，如meta表之类的才会放到inMemory队列中。</p>
<h5 id="5、Max-Version"><a href="#5、Max-Version" class="headerlink" title="5、Max Version"></a>5、Max Version</h5><p>​        创建表的时候，可以通过ColumnFamilyDescriptorBuilder.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)，保留更多的版本信息会占用更多的存储空间。</p>
<h5 id="6、Time-to-Live"><a href="#6、Time-to-Live" class="headerlink" title="6、Time to Live"></a>6、Time to Live</h5><p>​        创建表的时候，可以通过ColumnFamilyDescriptorBuilder.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 * 24 * 60 * 60)。</p>
<h5 id="7、Compaction"><a href="#7、Compaction" class="headerlink" title="7、Compaction"></a>7、Compaction</h5><p>​        在HBase中，数据在更新时首先写入WAL 日志(HLog)和内存(MemStore)中，MemStore中的数据是排序的，当MemStore累计到一定阈值时，就会创建一个新的MemStore，并且将老的MemStore添加到flush队列，由单独的线程flush到磁盘上，成为一个StoreFile。于此同时， 系统会在zookeeper中记录一个redo point，表示这个时刻之前的变更已经持久化了**(minor compact)**。</p>
<p>​        StoreFile是只读的，一旦创建后就不可以再修改。因此Hbase的更新其实是不断追加的操作。当一个Store中的StoreFile达到一定的阈值后，就会进行一次合并**(major compact)<strong>，将对同一个key的修改合并到一起，形成一个大的StoreFile，当StoreFile的大小达到一定阈值后，又会对 StoreFile进行分割</strong>(split)**，等分为两个StoreFile。</p>
<p>​        由于对表的更新是不断追加的，处理读请求时，需要访问Store中全部的StoreFile和MemStore，将它们按照row key进行合并，由于StoreFile和MemStore都是经过排序的，并且StoreFile带有内存中索引，通常合并过程还是比较快的。</p>
<p>​        实际应用中，可以考虑必要时手动进行major compact，将同一个row key的修改进行合并形成一个大的StoreFile。同时，可以将StoreFile设置大些，减少split的发生。</p>
<p>​        hbase为了防止小文件（被刷到磁盘的menstore）过多，以保证保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。在hbase中，主要存在两种类型的compaction：minor  compaction和major compaction。</p>
<p>​        1、minor compaction:的是较小、很少文件的合并。</p>
<p>​        minor compaction的运行机制要复杂一些，它由一下几个参数共同决定：</p>
<p>​                hbase.hstore.compaction.min :默认值为 3，表示至少需要三个满足条件的store file时，minor compaction才会启动</p>
<p>​                hbase.hstore.compaction.max 默认值为10，表示一次minor compaction中最多选取10个store file</p>
<p>​                hbase.hstore.compaction.min.size 表示文件大小小于该值的store file 一定会加入到minor compaction的store file中</p>
<p>​                hbase.hstore.compaction.max.size 表示文件大小大于该值的store file 一定不会被添加到minor compaction</p>
<p>​                hbase.hstore.compaction.ratio ：将 StoreFile 按照文件年龄排序，minor compaction 总是从 older store file 开始选择，如果该文件的 size 小于后面 hbase.hstore.compaction.max 个 store file size 之和乘以 ratio 的值，那么该 store file 将加入到 minor compaction 中。如果满足 minor compaction 条件的文件数量大于 hbase.hstore.compaction.min，才会启动。</p>
<p>​        2、major compaction 的功能是将所有的store file合并成一个，触发major compaction的可能条件有：</p>
<p>​            1、major_compact 命令、</p>
<p>​            2、majorCompact() API、</p>
<p>​            3、region server自动运行</p>
<p>​                （1）hbase.hregion.majorcompaction 默认为24 小时</p>
<p>​                （2）hbase.hregion.majorcompaction.jetter 默认值为0.2 防止region server 在同一时间进行major compaction）。</p>
<p>​                    hbase.hregion.majorcompaction.jetter参数的作用是：对参数hbase.hregion.majorcompaction 规定的值起到浮动的作用，假如两个参数都为默认值24和0,2，那么major compact最终使用的数值为：19.2~28.8 这个范围。</p>
<h3 id="2、hbase写表操作"><a href="#2、hbase写表操作" class="headerlink" title="2、hbase写表操作"></a>2、hbase写表操作</h3><h5 id="1、是否需要写WAL？WAL是否需要同步写入？"><a href="#1、是否需要写WAL？WAL是否需要同步写入？" class="headerlink" title="1、是否需要写WAL？WAL是否需要同步写入？"></a>1、是否需要写WAL？WAL是否需要同步写入？</h5><p>优化原理：</p>
<p>​        数据写入流程可以理解为一次顺序写WAL+一次写缓存，通常情况下写缓存延迟很低，因此提升写性能就只能从WAL入手。WAL机制一方面是为了确保数据即使写入缓存丢失也可以恢复，另一方面是为了集群之间异步复制。默认WAL机制开启且使用同步机制写入WAL。首先考虑业务是否需要写WAL，通常情况下大多数业务都会开启WAL机制（默认），但是对于部分业务可能并不特别关心异常情况下部分数据的丢失，而更关心数据写入吞吐量，比如某些推荐业务，这类业务即使丢失一部分用户行为数据可能对推荐结果并不构成很大影响，但是对于写入吞吐量要求很高，不能造成数据队列阻塞。这种场景下可以考虑关闭WAL写入，写入吞吐量可以提升2x~3x。退而求其次，有些业务不能接受不写WAL，但可以接受WAL异步写入，也是可以考虑优化的，通常也会带来1x～2x的性能提升。 </p>
<p>优化推荐：</p>
<p>​        根据业务关注点在WAL机制与写入吞吐量之间做出选择  </p>
<h5 id="2、Put是否可以同步批量提交？"><a href="#2、Put是否可以同步批量提交？" class="headerlink" title="2、Put是否可以同步批量提交？"></a>2、Put是否可以同步批量提交？</h5><p>优化原理：</p>
<p>​        HBase分别提供了单条put以及批量put的API接口，使用批量put接口可以减少客户端到RegionServer之间的RPC连接数，提高写入性能。另外需要注意的是，批量put请求要么全部成功返回，要么抛出异常。</p>
<p>优化建议：</p>
<p>​        使用批量put进行写入请求</p>
<h5 id="3、Put是否可以异步批量提交？"><a href="#3、Put是否可以异步批量提交？" class="headerlink" title="3、Put是否可以异步批量提交？"></a>3、Put是否可以异步批量提交？</h5><p>优化原理：</p>
<p>​        业务如果可以接受异常情况下少量数据丢失的话，还可以使用异步批量提交的方式提交请求。提交分为两阶段执行：用户提交写请求之后，数据会写入客户端缓存，并返回用户写入成功；当客户端缓存达到阈值（默认2M）之后批量提交给RegionServer。需要注意的是，在某些情况下客户端异常的情况下缓存数据有可能丢失。</p>
<p>优化建议：</p>
<p>​        在业务可以接受的情况下开启异步批量提交</p>
<p>使用方式：</p>
<p>​        setAutoFlush(false)</p>
<h5 id="4-Region是否太少？"><a href="#4-Region是否太少？" class="headerlink" title="4. Region是否太少？"></a>4. Region是否太少？</h5><p>优化原理：</p>
<p>​        当前集群中表的Region个数如果小于RegionServer个数，即Num(Region of Table) &lt; Num(RegionServer)，可以考虑切分Region并尽可能分布到不同RegionServer来提高系统请求并发度，如果Num(Region of Table) &gt; Num(RegionServer)，再增加Region个数效果并不明显。</p>
<p>优化建议：</p>
<p>​        在Num(Region of Table) &lt; Num(RegionServer)的场景下切分部分请求负载高的Region并迁移到其他RegionServer；</p>
<h5 id="5-写入请求是否不均衡？"><a href="#5-写入请求是否不均衡？" class="headerlink" title="5. 写入请求是否不均衡？"></a><strong>5. 写入请求是否不均衡？</strong></h5><p>优化原理：</p>
<p>​        另一个需要考虑的问题是写入请求是否均衡，如果不均衡，一方面会导致系统并发度较低，另一方面也有可能造成部分节点负载很高，进而影响其他业务。分布式系统中特别害怕一个节点负载很高的情况，一个节点负载很高可能会拖慢整个集群，这是因为很多业务会使用Mutli批量提交读写请求，一旦其中一部分请求落到该节点无法得到及时响应，就会导致整个批量请求超时。因此不怕节点宕掉，就怕节点奄奄一息！</p>
<p>优化建议：</p>
<p>​        检查RowKey设计以及预分区策略，保证写入请求均衡。</p>
<h5 id="6-写入KeyValue数据是否太大？"><a href="#6-写入KeyValue数据是否太大？" class="headerlink" title="6. 写入KeyValue数据是否太大？"></a><strong>6. 写入KeyValue数据是否太大？</strong></h5><p>​        KeyValue大小对写入性能的影响巨大，一旦遇到写入性能比较差的情况，需要考虑是否由于写入KeyValue数据太大导致。KeyValue大小对写入性能影响曲线图如下：</p>
<p><img src="https://github.com/msbbigdata/hbase/blob/master/image/%E5%AF%B9%E6%AF%94.png" alt="对比"></p>
<p>​        图中横坐标是写入的一行数据（每行数据10列）大小，左纵坐标是写入吞吐量，右坐标是写入平均延迟（ms）。可以看出随着单行数据大小不断变大，写入吞吐量急剧下降，写入延迟在100K之后急剧增大。</p>
<h5 id="7、Utilize-Flash-storage-for-WAL-HBASE-12848"><a href="#7、Utilize-Flash-storage-for-WAL-HBASE-12848" class="headerlink" title="7、Utilize Flash storage for WAL(HBASE-12848)"></a>7、Utilize Flash storage for WAL(HBASE-12848)</h5><p>​        这个特性意味着可以将WAL单独置于SSD上，这样即使在默认情况下（WALSync），写性能也会有很大的提升。需要注意的是，该特性建立在HDFS 2.6.0+的基础上，HDFS以前版本不支持该特性。具体可以参考官方jira：<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/HBASE-12848">https://issues.apache.org/jira/browse/HBASE-12848</a></p>
<h5 id="8、Multiple-WALs-HBASE-14457"><a href="#8、Multiple-WALs-HBASE-14457" class="headerlink" title="8、Multiple WALs(HBASE-14457)"></a>8、Multiple WALs(HBASE-14457)</h5><p>​        该特性也是对WAL进行改造，当前WAL设计为一个RegionServer上所有Region共享一个WAL，可以想象在写入吞吐量较高的时候必然存在资源竞争，降低整体性能。针对这个问题，社区小伙伴（阿里巴巴大神）提出Multiple WALs机制，管理员可以为每个Namespace下的所有表设置一个共享WAL，通过这种方式，写性能大约可以提升20%～40%左右。具体可以参考官方jira：<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/HBASE-14457">https://issues.apache.org/jira/browse/HBASE-14457</a></p>
<h3 id="3、hbase读表优化"><a href="#3、hbase读表优化" class="headerlink" title="3、hbase读表优化"></a>3、hbase读表优化</h3><h5 id="1-scan缓存是否设置合理？"><a href="#1-scan缓存是否设置合理？" class="headerlink" title="1. scan缓存是否设置合理？"></a>1. scan缓存是否设置合理？</h5><p>优化原理：</p>
<p>​        在解释这个问题之前，首先需要解释什么是scan缓存，通常来讲一次scan会返回大量数据，因此客户端发起一次scan请求，实际并不会一次就将所有数据加载到本地，而是分成多次RPC请求进行加载，这样设计一方面是因为大量数据请求可能会导致网络带宽严重消耗进而影响其他业务，另一方面也有可能因为数据量太大导致本地客户端发生OOM。在这样的设计体系下用户会首先加载一部分数据到本地，然后遍历处理，再加载下一部分数据到本地处理，如此往复，直至所有数据都加载完成。数据加载到本地就存放在scan缓存中，默认100条数据大小。</p>
<p>通常情况下，默认的scan缓存设置就可以正常工作的。但是在一些大scan（一次scan可能需要查询几万甚至几十万行数据）来说，每次请求100条数据意味着一次scan需要几百甚至几千次RPC请求，这种交互的代价无疑是很大的。因此可以考虑将scan缓存设置增大，比如设为500或者1000就可能更加合适。笔者之前做过一次试验，在一次scan扫描10w+条数据量的条件下，将scan缓存从100增加到1000，可以有效降低scan请求的总体延迟，延迟基本降低了25%左右。</p>
<p>优化建议：</p>
<p>​        大scan场景下将scan缓存从100增大到500或者1000，用以减少RPC次数</p>
<h5 id="2-get请求是否可以使用批量请求？"><a href="#2-get请求是否可以使用批量请求？" class="headerlink" title="2. get请求是否可以使用批量请求？"></a>2. get请求是否可以使用批量请求？</h5><p>优化原理：</p>
<p>​        HBase分别提供了单条get以及批量get的API接口，使用批量get接口可以减少客户端到RegionServer之间的RPC连接数，提高读取性能。另外需要注意的是，批量get请求要么成功返回所有请求数据，要么抛出异常。</p>
<p>优化建议：</p>
<p>​        使用批量get进行读取请求</p>
<h5 id="3-请求是否可以显示指定列族或者列？"><a href="#3-请求是否可以显示指定列族或者列？" class="headerlink" title="3. 请求是否可以显示指定列族或者列？"></a>3. 请求是否可以显示指定列族或者列？</h5><p>优化原理：</p>
<p>​        HBase是典型的列族数据库，意味着同一列族的数据存储在一起，不同列族的数据分开存储在不同的目录下。如果一个表有多个列族，只是根据Rowkey而不指定列族进行检索的话不同列族的数据需要独立进行检索，性能必然会比指定列族的查询差很多，很多情况下甚至会有2倍～3倍的性能损失。</p>
<p>优化建议：</p>
<p>​        可以指定列族或者列进行精确查找的尽量指定查找</p>
<h5 id="4-离线批量读取请求是否设置禁止缓存？"><a href="#4-离线批量读取请求是否设置禁止缓存？" class="headerlink" title="4. 离线批量读取请求是否设置禁止缓存？"></a>4. 离线批量读取请求是否设置禁止缓存？</h5><p>优化原理：</p>
<p>​        通常离线批量读取数据会进行一次性全表扫描，一方面数据量很大，另一方面请求只会执行一次。这种场景下如果使用scan默认设置，就会将数据从HDFS加载出来之后放到缓存。可想而知，大量数据进入缓存必将其他实时业务热点数据挤出，其他业务不得不从HDFS加载，进而会造成明显的读延迟</p>
<p>优化建议：</p>
<p>​        离线批量读取请求设置禁用缓存，scan.setBlockCache(false)</p>
<h5 id="5-读请求是否均衡？"><a href="#5-读请求是否均衡？" class="headerlink" title="5. 读请求是否均衡？"></a>5. 读请求是否均衡？</h5><p>优化原理：</p>
<p>​        极端情况下假如所有的读请求都落在一台RegionServer的某几个Region上，这一方面不能发挥整个集群的并发处理能力，另一方面势必造成此台RegionServer资源严重消耗（比如IO耗尽、handler耗尽等），落在该台RegionServer上的其他业务会因此受到很大的波及。可见，读请求不均衡不仅会造成本身业务性能很差，还会严重影响其他业务。当然，写请求不均衡也会造成类似的问题，可见负载不均衡是HBase的大忌。</p>
<p>观察确认：</p>
<p>​        观察所有RegionServer的读请求QPS曲线，确认是否存在读请求不均衡现象</p>
<p>优化建议：</p>
<p>​        RowKey必须进行散列化处理（比如MD5散列），同时建表必须进行预分区处理</p>
<h5 id="6-BlockCache是否设置合理？"><a href="#6-BlockCache是否设置合理？" class="headerlink" title="6. BlockCache是否设置合理？"></a>6. BlockCache是否设置合理？</h5><p>优化原理：</p>
<p>​        BlockCache作为读缓存，对于读性能来说至关重要。默认情况下BlockCache和Memstore的配置相对比较均衡（各占40%），可以根据集群业务进行修正，比如读多写少业务可以将BlockCache占比调大。另一方面，BlockCache的策略选择也很重要，不同策略对读性能来说影响并不是很大，但是对GC的影响却相当显著，尤其BucketCache的offheap模式下GC表现很优越。另外，HBase 2.0对offheap的改造（HBASE-11425）将会使HBase的读性能得到2～4倍的提升，同时GC表现会更好！</p>
<p>观察确认：</p>
<p>​        观察所有RegionServer的缓存未命中率、配置文件相关配置项一级GC日志，确认BlockCache是否可以优化</p>
<p>优化建议：</p>
<p>​        JVM内存配置量 &lt; 20G，BlockCache策略选择LRUBlockCache；否则选择BucketCache策略的offheap模式；</p>
<h5 id="7-HFile文件是否太多？"><a href="#7-HFile文件是否太多？" class="headerlink" title="7. HFile文件是否太多？"></a>7. HFile文件是否太多？</h5><p>优化原理：</p>
<p>​        HBase读取数据通常首先会到Memstore和BlockCache中检索（读取最近写入数据&amp;热点数据），如果查找不到就会到文件中检索。HBase的类LSM结构会导致每个store包含多数HFile文件，文件越多，检索所需的IO次数必然越多，读取延迟也就越高。文件数量通常取决于Compaction的执行策略，一般和两个配置参数有关：hbase.hstore.compaction.min和hbase.hstore.compaction.max.size，前者表示一个store中的文件数超过多少就应该进行合并，后者表示参数合并的文件大小最大是多少，超过此大小的文件不能参与合并。这两个参数不能设置太’松’（前者不能设置太大，后者不能设置太小），导致Compaction合并文件的实际效果不明显，进而很多文件得不到合并。这样就会导致HFile文件数变多。</p>
<p>观察确认：</p>
<p>​        观察RegionServer级别以及Region级别的storefile数，确认HFile文件是否过多</p>
<p>优化建议：</p>
<p>​        hbase.hstore.compaction.min设置不能太大，默认是3个；设置需要根据Region大小确定，通常可以简单的认为hbase.hstore.compaction.max.size = RegionSize / hbase.hstore.compaction.min</p>
<h5 id="8-Compaction是否消耗系统资源过多？"><a href="#8-Compaction是否消耗系统资源过多？" class="headerlink" title="8. Compaction是否消耗系统资源过多？"></a><strong>8. Compaction是否消耗系统资源过多？</strong></h5><p>优化原理：</p>
<p>​        Compaction是将小文件合并为大文件，提高后续业务随机读性能，但是也会带来IO放大以及带宽消耗问题（数据远程读取以及三副本写入都会消耗系统带宽）。正常配置情况下Minor Compaction并不会带来很大的系统资源消耗，除非因为配置不合理导致Minor Compaction太过频繁，或者Region设置太大情况下发生Major Compaction。</p>
<p>观察确认：</p>
<p>​        观察系统IO资源以及带宽资源使用情况，再观察Compaction队列长度，确认是否由于Compaction导致系统资源消耗过多</p>
<p>优化建议：</p>
<p>​        （1）Minor Compaction设置：hbase.hstore.compaction.min设置不能太小，又不能设置太大，因此建议设置为5～6；hbase.hstore.compaction.max.size = RegionSize / hbase.hstore.compaction.min</p>
<p>​        （2）Major Compaction设置：大Region读延迟敏感业务（ 100G以上）通常不建议开启自动Major Compaction，手动低峰期触发。小Region或者延迟不敏感业务可以开启Major Compaction，但建议限制流量；</p>
<h5 id="9、数据本地率是否太低？"><a href="#9、数据本地率是否太低？" class="headerlink" title="9、数据本地率是否太低？"></a>9、数据本地率是否太低？</h5><p>数据本地率：HDFS数据通常存储三份，假如当前RegionA处于Node1上，数据a写入的时候三副本为(Node1,Node2,Node3)，数据b写入三副本是(Node1,Node4,Node5)，数据c写入三副本(Node1,Node3,Node5)，可以看出来所有数据写入本地Node1肯定会写一份，数据都在本地可以读到，因此数据本地率是100%。现在假设RegionA被迁移到了Node2上，只有数据a在该节点上，其他数据（b和c）读取只能远程跨节点读，本地率就为33%（假设a，b和c的数据大小相同）。</p>
<p>优化原理：</p>
<p>​        数据本地率太低很显然会产生大量的跨网络IO请求，必然会导致读请求延迟较高，因此提高数据本地率可以有效优化随机读性能。数据本地率低的原因一般是因为Region迁移（自动balance开启、RegionServer宕机迁移、手动迁移等）,因此一方面可以通过避免Region无故迁移来保持数据本地率，另一方面如果数据本地率很低，也可以通过执行major_compact提升数据本地率到100%。</p>
<p>优化建议：</p>
<p>​        避免Region无故迁移，比如关闭自动balance、RS宕机及时拉起并迁回飘走的Region等；在业务低峰期执行major_compact提升数据本地率</p>
<h3 id="1、hbase数据读取流程简单描述"><a href="#1、hbase数据读取流程简单描述" class="headerlink" title="1、hbase数据读取流程简单描述"></a>1、hbase数据读取流程简单描述</h3><p>​        一般来说，在描述hbase读取流程的时候，简单的描述如下：</p>
<p>​        1、客户端从zookeeper中获取meta表所在的regionserver节点信息</p>
<p>​        2、客户端访问meta表所在的regionserver节点，获取到region所在的regionserver信息</p>
<p>​        3、客户端访问具体的region所在的regionserver，找到对应的region及store</p>
<p>​        4、首先从memstore中读取数据，如果读取到了那么直接将数据返回，如果没有，则去blockcache读取数据</p>
<p>​        5、如果blockcache中读取到数据，则直接返回数据给客户端，如果读取不到，则遍历storefile文件，查找数据</p>
<p>​        6、如果从storefile中读取不到数据，则返回客户端为空，如果读取到数据，那么需要将数据先缓存到blockcache中（方便下一次读取），然后再将数据返回给客户端。</p>
<p>​        7、blockcache是内存空间，如果缓存的数据比较多，满了之后会采用LRU策略，将比较老的数据进行删除。</p>
<p>​        但是为了加深自己的理解，我们需要对hbase的读取流程有一个更深刻的理解。</p>
<h3 id="2、hbase读取数据详细解释"><a href="#2、hbase读取数据详细解释" class="headerlink" title="2、hbase读取数据详细解释"></a>2、hbase读取数据详细解释</h3><p>​        和写流程相比，HBase读数据是一个更加复杂的操作流程，这主要基于两个方面的原因：其一是因为整个HBase存储引擎基于LSM-Like树实现，因此一次范围查询可能会涉及多个分片、多块缓存甚至多个数据存储文件；其二是因为HBase中更新操作以及删除操作实现都很简单，更新操作并没有更新原有数据，而是使用时间戳属性实现了多版本。删除操作也并没有真正删除原有数据，只是插入了一条打上”deleted”标签的数据，而真正的数据删除发生在系统异步执行Major_Compact的时候。很显然，这种实现套路大大简化了数据更新、删除流程，但是对于数据读取来说却意味着套上了层层枷锁，读取过程需要根据版本进行过滤，同时对已经标记删除的数据也要进行过滤。</p>
<p>​        客户端如果需要访问数据，每次必须要找到对应的re·，从客户端发出请求到找到regionserver这个过程比较简单，不需要做多余的赘述，但是当开始读取数据的时候，大家需要注意了，hbase并没有我们想象的那么简单。</p>
<h5 id="1、构建scanner体系–组件施工队"><a href="#1、构建scanner体系–组件施工队" class="headerlink" title="1、构建scanner体系–组件施工队"></a>1、构建scanner体系–组件施工队</h5><p>​        scanner体系的核心在于三层scanner：RegionScanner、StoreScanner以及StoreFileScanner。三者是层级的关系，一个RegionScanner由多个StoreScanner构成，一张表由多个列族组成，就有多少个StoreScanner负责该列族的数据扫描。一个StoreScanner又是由多个StoreFileScanner组成。每个Store的数据由内存中的MemStore和磁盘上的StoreFile文件组成，相对应的，StoreScanner对象会雇佣一个MemStoreScanner和N个StoreFileScanner来进行实际的数据读取，每个StoreFile文件对应一个StoreFileScanner，注意：StoreFileScanner和MemstoreScanner是整个scan的最终执行者。</p>
<p>对应于建楼项目，一栋楼通常由好几个单元楼构成（每个单元楼对应于一个Store），每个单元楼会请一个监工（StoreScanner）负责该单元楼的建造。而监工一般不做具体的事情，他负责招募很多工人（StoreFileScanner），这些工人才是建楼的主体。下图是整个构建流程图：</p>
<p><img src="https://github.com/msbbigdata/hbase/blob/master/image/scan%E6%B5%81%E7%A8%8B.png" alt="scan流程"></p>
<ol>
<li> RegionScanner会根据列族构建StoreScanner，有多少列族就构建多少StoreScanner，用于负责该列族的数据检索</li>
</ol>
<p>​       1.1 构建StoreFileScanner：每个StoreScanner会为当前该Store中每个HFile构造一个StoreFileScanner，用于实际执行对应文件的检索。同时会为对应Memstore构造一个MemstoreScanner，用于执行该Store中Memstore的数据检索。该步骤对应于监工在人才市场招募建楼所需的各种类型工匠。</p>
<p>​       1.2  过滤淘汰StoreFileScanner：根据Time Range以及RowKey Range对StoreFileScanner以及MemstoreScanner进行过滤，淘汰肯定不存在待检索结果的Scanner。上图中StoreFile3因为检查RowKeyRange不存在待检索Rowkey所以被淘汰。该步骤针对具体的建楼方案，裁撤掉部分不需要的工匠，比如这栋楼不需要地暖安装，对应的工匠就可以撤掉。</p>
<p>​       1.3  Seek rowkey：所有StoreFileScanner开始做准备工作，在负责的HFile中定位到满足条件的起始Row。工匠也开始准备自己的建造工具，建造材料，找到自己的工作地点，等待一声命下。就像所有重要项目的准备工作都很核心一样，Seek过程（此处略过Lazy Seek优化）也是一个很核心的步骤，它主要包含下面三步：</p>
<ul>
<li>定位Block Offset：在Blockcache中读取该HFile的索引树结构，根据索引树检索对应RowKey所在的Block Offset和Block Size</li>
<li>Load Block：根据BlockOffset首先在BlockCache中查找Data Block，如果不在缓存，再在HFile中加载</li>
<li>Seek Key：在Data Block内部通过二分查找的方式定位具体的RowKey</li>
</ul>
<p>​       1.4  StoreFileScanner合并构建最小堆：将该Store中所有StoreFileScanner和MemstoreScanner合并形成一个heap（最小堆），所谓heap是一个优先级队列，队列中元素是所有scanner，排序规则按照scanner seek到的keyvalue大小由小到大进行排序。这里需要重点关注三个问题，首先为什么这些Scanner需要由小到大排序，其次keyvalue是什么样的结构，最后，keyvalue谁大谁小是如何确定的：</p>
<ul>
<li><p>为什么这些Scanner需要由小到大排序？</p>
<p>​        最直接的解释是scan的结果需要由小到大输出给用户，当然，这并不全面，最合理的解释是只有由小到大排序才能使得scan效率最高。举个简单的例子，HBase支持数据多版本，假设用户只想获取最新版本，那只需要将这些数据由最新到最旧进行排序，然后取队首元素返回就可以。那么，如果不排序，就只能遍历所有元素，查看符不符合用户查询条件。这就是排队的意义。</p>
<p>​        工匠们也需要排序，先做地板的排前面，做墙体的次之，最后是做门窗户的。做墙体的内部还需要再排序，做内墙的排前面，做外墙的排后面，这样，假如设计师临时决定不做外墙的话，就可以直接跳过外墙部分工作。很显然，如果不排序的话，是没办法临时做决定的，因为这部分工作已经可能做掉了。</p>
</li>
<li><p>HBase中KeyValue是什么样的结构？</p>
<p>​        HBase中KeyValue并不是简单的KV数据对，而是一个具有复杂元素的结构体，其中Key由RowKey，ColumnFamily，Qualifier ，TimeStamp，KeyType等多部分组成，Value是一个简单的二进制数据。Key中元素KeyType表示该KeyValue的类型，取值分别为Put/Delete/Delete Column/Delete Family等。KeyValue可以表示为如下图所示：</p>
</li>
</ul>
<p><img src="https://github.com/msbbigdata/hbase/blob/master/image/keyvalue.png" alt="keyvalue"></p>
<p>​        了解了KeyValue的逻辑结构后，我们不妨再进一步从原理的角度想想HBase的开发者们为什么如此对其设计。这个就得从HBase所支持的数据操作说起了，HBase支持四种主要的数据操作，分别是Get/Scan/Put/Delete，其中Get和Scan代表数据查询，Put操作代表数据插入或更新（如果Put的RowKey不存在则为插入操作、否则为更新操作），特别需要注意的是HBase中更新操作并不是直接覆盖修改原数据，而是生成新的数据，新数据和原数据具有不同的版本（时间戳）；Delete操作执行数据删除，和数据更新操作相同，HBase执行数据删除并不会马上将数据从数据库中永久删除，而只是生成一条删除记录，最后在系统执行文件合并的时候再统一删除。</p>
<p>​        HBase中更新删除操作并不直接操作原数据，而是生成一个新纪录，那问题来了，如何知道一条记录到底是插入操作还是更新操作亦或是删除操作呢？这正是KeyType和Timestamp的用武之地。上文中提到KeyType取值为分别为Put/Delete/Delete Column/Delete Family四种，如果KeyType取值为Put，表示该条记录为插入或者更新操作，而无论是插入或者更新，都可以使用版本号（Timestamp）对记录进行选择；如果KeyType为Delete，表示该条记录为整行删除操作；相应的KeyType为Delete Column和Delete Family分别表示删除某行某列以及某行某列族操作；</p>
<ul>
<li><p>不同KeyValue之间如何进行大小比较？</p>
<p>​        上文提到KeyValue中Key由RowKey，ColumnFamily，Qualifier ，TimeStamp，KeyType等5部分组成，HBase设定Key大小首先比较RowKey，RowKey越小Key就越小；RowKey如果相同就看CF，CF越小Key越小；CF如果相同看Qualifier，Qualifier越小Key越小；Qualifier如果相同再看Timestamp，Timestamp越大表示时间越新，对应的Key越小。如果Timestamp还相同，就看KeyType，KeyType按照DeleteFamily -&gt; DeleteColumn -&gt; Delete -&gt; Put 顺序依次对应的Key越来越大。</p>
</li>
</ul>
<p>2、StoreScanner合并构建最小堆：上文讨论的是一个监工如何构建自己的工匠师团队以及工匠师如何做准备工作、排序工作。实际上，监工也需要进行排序，比如一单元的监工排前面，二单元的监工排之后… StoreScanner一样，列族小的StoreScanner排前面，列族大的StoreScanner排后面。</p>
<h5 id="2、scan查询－层层建楼"><a href="#2、scan查询－层层建楼" class="headerlink" title="2、scan查询－层层建楼"></a>2、scan查询－层层建楼</h5><p>​        构建Scanner体系是为了更好地执行scan查询，就像组建工匠师团队就是为了盖房子一样。scan查询总是一行一行查询的，先查第一行的所有数据，再查第二行的所有数据，但每一行的查询流程却没有什么本质区别。盖房子也一样，无论是盖8层还是盖18层，都需要一层一层往上盖，而且每一层的盖法并没有什么区别。所以实际上我们只需要关注其中一行数据是如何查询的就可以。</p>
<p>​        对于一行数据的查询，又可以分解为多个列族的查询，比如RowKey=row1的一行数据查询，首先查询列族1上该行的数据集合，再查询列族2里该行的数据集合。同样是盖第一层房子，先盖一单元的一层，再改二单元的一层，盖完之后才算一层盖完，接着开始盖第二层。所以我们也只需要关注某一行某个列族的数据是如何查询的就可以。</p>
<p>​        还记得Scanner体系构建的最终结果是一个由StoreFileScanner和MemstoreScanner组成的heap（最小堆）么，这里就派上用场了。下图是一张表的逻辑视图，该表有两个列族cf1和cf2（我们只关注cf1），cf1只有一个列name，表中有5行数据，其中每个cell基本都有多个版本。cf1的数据假如实际存储在三个区域，memstore中有r2和r4的最新数据，hfile1中是最早的数据。现在需要查询RowKey=r2的数据，按照上文的理论对应的Scanner指向就如图所示：</p>
<p><img src="https://github.com/msbbigdata/hbase/blob/master/image/scan%E6%A1%88%E4%BE%8B.png" alt="scan案例"></p>
<pre><code> 这三个Scanner组成的heap为&lt;MemstoreScanner，StoreFileScanner2,   StoreFileScanner1&gt;，Scanner由小到大排列。查询的时候首先pop出heap的堆顶元素，即MemstoreScanner，得到keyvalue  = r2:cf1:name:v3:name23的数据，拿到这个keyvalue之后，需要进行如下判定：  
</code></pre>
<ol>
<li> 检查该KeyValue的KeyType是否是Deleted/DeletedCol等，如果是就直接忽略该列所有其他版本，跳到下列（列族）     </li>
<li> 检查该KeyValue的Timestamp是否在用户设定的Timestamp Range范围，如果不在该范围，忽略     </li>
<li> 检查该KeyValue是否满足用户设置的各种filter过滤器，如果不满足，忽略     </li>
<li> 检查该KeyValue是否满足用户查询中设定的版本数，比如用户只查询最新版本，则忽略该cell的其他版本；反正如果用户查询所有版本，则还需要查询该cell的其他版本。 </li>
</ol>
<p> 现在假设用户查询所有版本而且该keyvalue检查通过，此时当前的堆顶元素需要执行next方法去检索下一个值，并重新组织最小堆。即图中MemstoreScanner将会指向r4，重新组织最小堆之后最小堆将会变为&lt;StoreFileScanner2,  StoreFileScanner1,  MemstoreScanner&gt;，堆顶元素变为StoreFileScanner2，得到keyvalue＝r2:cf1:name:v2:name22，进行一系列判定，再next，再重新组织最小堆…         </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/09/15/hbase%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/15/hbase%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">hbase笔记总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-15 15:43:36" itemprop="dateCreated datePublished" datetime="2020-09-15T15:43:36+08:00">2020-09-15</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-15 16:52:53" itemprop="dateModified" datetime="2020-11-15T16:52:53+08:00">2020-11-15</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="1-先从关系型数据库与非关系型数据讲起"><a href="#1-先从关系型数据库与非关系型数据讲起" class="headerlink" title="1.先从关系型数据库与非关系型数据讲起"></a>1.先从关系型数据库与非关系型数据讲起</h3><p><strong>关系型数据库</strong> 就是我们传统的像mysql,oracle,sql server这样的具有自己的二维固定的数据结构<br/><br>优点:</p>
<ul>
<li><p>易于维护:都是使用表结构,格式一致</p>
</li>
<li><p>使用方便: SQL语言通用,可用于复杂查询</p>
</li>
<li><p>复杂操作:支持SQL,可用于一个表以及多个表之间非常复杂的查询</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/09/15/hbase%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/09/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%BA%95%E5%B1%82%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%9301/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%BA%95%E5%B1%82%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%9301/" class="post-title-link" itemprop="url">计算机底层知识总结01</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-05 16:38:29" itemprop="dateCreated datePublished" datetime="2020-09-05T16:38:29+08:00">2020-09-05</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-10-07 21:20:39" itemprop="dateModified" datetime="2020-10-07T21:20:39+08:00">2020-10-07</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">底层原理</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="硬件基础知识"><a href="#硬件基础知识" class="headerlink" title="硬件基础知识"></a>硬件基础知识</h3><h4 id="CPU的制作"><a href="#CPU的制作" class="headerlink" title="CPU的制作"></a>CPU的制作</h4><h3 id="汇编语言的执行过程"><a href="#汇编语言的执行过程" class="headerlink" title="汇编语言的执行过程"></a>汇编语言的执行过程</h3><h3 id="计算机启动过程"><a href="#计算机启动过程" class="headerlink" title="计算机启动过程"></a>计算机启动过程</h3><h3 id="操作系统的基本知识"><a href="#操作系统的基本知识" class="headerlink" title="操作系统的基本知识"></a>操作系统的基本知识</h3><h3 id="进程线程纤程的基本概念"><a href="#进程线程纤程的基本概念" class="headerlink" title="进程线程纤程的基本概念"></a>进程线程纤程的基本概念</h3><h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><h3 id="进程与线程管理"><a href="#进程与线程管理" class="headerlink" title="进程与线程管理"></a>进程与线程管理</h3><h3 id="中断与系统调用-软中断"><a href="#中断与系统调用-软中断" class="headerlink" title="中断与系统调用(软中断)"></a>中断与系统调用(软中断)</h3><h3 id="内核同步基础知识"><a href="#内核同步基础知识" class="headerlink" title="内核同步基础知识"></a>内核同步基础知识</h3><h3 id="关于硬盘IO-DMA"><a href="#关于硬盘IO-DMA" class="headerlink" title="关于硬盘IO DMA"></a>关于硬盘IO DMA</h3>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/09/04/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/04/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/" class="post-title-link" itemprop="url">hive-笔记总结02</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-04 15:44:41" itemprop="dateCreated datePublished" datetime="2020-09-04T15:44:41+08:00">2020-09-04</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-15 16:52:16" itemprop="dateModified" datetime="2020-11-15T16:52:16+08:00">2020-11-15</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>我们接着上次的hive继续总结</p>
<h3 id="配置补充-hiveserer2的高可用"><a href="#配置补充-hiveserer2的高可用" class="headerlink" title="配置补充,hiveserer2的高可用"></a>配置补充,hiveserer2的高可用</h3><p>node2-hive-site.xml</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/09/04/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/27/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/27/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">hive-笔记总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-27 20:50:49" itemprop="dateCreated datePublished" datetime="2020-08-27T20:50:49+08:00">2020-08-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-15 16:52:12" itemprop="dateModified" datetime="2020-11-15T16:52:12+08:00">2020-11-15</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="who-what-why"><a href="#who-what-why" class="headerlink" title="who,what,why"></a>who,what,why</h3><h4 id="hive的作用"><a href="#hive的作用" class="headerlink" title="hive的作用"></a>hive的作用</h4><p>按照做笔记的习惯来说,说一个新的大数据平台框架,一般先从模型说起,而hive本身是企业级数据仓库工具,基于mapreduce计算引擎的封装(2.x之后逐渐将官方计算引擎指定为spark)所以,就其本身而言并没有模型可以讨论.<br>但是我们可以聊聊他的作用,他是解决什么需求的:</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/08/27/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/21/mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/21/mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/" class="post-title-link" itemprop="url">mapreduce笔记-源码剖析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-21 17:56:07" itemprop="dateCreated datePublished" datetime="2020-08-21T17:56:07+08:00">2020-08-21</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-06-11 13:27:32" itemprop="dateModified" datetime="2021-06-11T13:27:32+08:00">2021-06-11</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="为什么要看源码"><a href="#为什么要看源码" class="headerlink" title="为什么要看源码"></a>为什么要看源码</h4><p>1.为了更好的使用框架的Api解决问题,比如说我们遇到一个问题,需要修改mapreduce分片的大小,如果没看过源码,可能会写很多代码,甚至重新调整文件block的大小上传,但是看过源码的都懂,只要简单的修改minSplite和maxSplite这两个配置属性就可以.<br>2.为了学习框架本身的设计方法,应用到日常开发中.<br>(此次源码分析的hadoop版本为2.7.2)</p>
<h4 id="怎么看源码"><a href="#怎么看源码" class="headerlink" title="怎么看源码"></a>怎么看源码</h4><p>要有目的性的的看源码,如果不带目的直接看的话,会很晕,源码一般信息量很大,而且很多部分是没有必要的,我们要取其精髓,忽略与当前目标无关的部分.并将重要的部分记录下来,最好是自己可以用伪代码实现,并且能够讲出其中的逻辑点和技术应用点</p>
<h4 id="mapreduce源码梗概"><a href="#mapreduce源码梗概" class="headerlink" title="mapreduce源码梗概"></a>mapreduce源码梗概</h4><p>mapreduce笔者目前了解的主要有三部分,client端,map计算端的输入输出,reduce计算端的输入输出.<br>client端主要验证client端的任务,以及关键切片部分的逻辑<br>map端和reduce端输入输出,一是看数据格式相关的转换<br>二是看shuffle的主要流程,看有哪些可以在开发过程中可以微调的地方</p>
<h4 id="client端"><a href="#client端" class="headerlink" title="client端"></a>client端</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.waitForCompletion(true);</span><br></pre></td></tr></table></figure>
<p>我们编写mapreduce程序的时候到最后执行这个方法的时候,任务才会真正提交,<br>那我们提交任务之后客户端都是如何做得呢?<br>点进去查看源码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public boolean waitForCompletion(boolean verbose</span><br><span class="line">                                   ) throws IOException, InterruptedException,</span><br><span class="line">                                            ClassNotFoundException &#123;</span><br><span class="line">    if (state == JobState.DEFINE) &#123;  //检查任务状态,是否允许提交</span><br><span class="line">      submit(); //提交任务方法</span><br><span class="line">    &#125;</span><br><span class="line">    if (verbose) &#123;</span><br><span class="line">      monitorAndPrintJob(); //监控并且获取任务的详细运行信息</span><br><span class="line">    &#125; else ...</span><br><span class="line">    return isSuccessful();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>提交之后,再调用方法获取任务的详细信息,可见这个任务是异步任务.<br>我们关系心的任务如何提交的,那么就进入submit方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void submit()</span><br><span class="line">       throws IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  setUseNewAPI();  //hadoop1.x和hadoop2.x的mapreduce架构不同,所以这里是新版API</span><br><span class="line">  connect(); //连接yarn resourceManager</span><br><span class="line">  final JobSubmitter submitter =</span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient()); //获取集群HDFS操作对象和Client对象,为以后把分片信息,配置文件,jar包,通过FileSystem上传到hdfs上</span><br><span class="line">  status = ugi.doAs(new PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    public JobStatus run() throws IOException, InterruptedException,</span><br><span class="line">    ClassNotFoundException &#123;</span><br><span class="line">      return submitter.submitJobInternal(Job.this, cluster); //这里就是提交Job的地方</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(&quot;The url to track the job: &quot; + getTrackingURL());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>再进入submitJobInternal方法,然后发现注释这部分很有东西</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Internal method for submitting jobs to the system.</span><br><span class="line">The job submission process involves:</span><br><span class="line">1.Checking the input and output specifications of the job.</span><br><span class="line">2.Computing the InputSplits for the job.</span><br><span class="line">3.Setup the requisite accounting information for the DistributedCache of the job, if necessary.</span><br><span class="line">4.Copying the job&#x27;s jar and configuration to the map-reduce system directory on the distributed file-system.</span><br><span class="line">5.Submitting the job to the JobTracker and optionally monitoring it&#x27;s status.</span><br></pre></td></tr></table></figure>
<p>简单的翻译以下就懂了,里面会</p>
<ol>
<li>检查job输入输出路径</li>
<li>计算分片的大小</li>
<li>有必要的话,为作业的缓存设置账户信息</li>
<li>把job的jar包,配置文件拷贝到hdfs</li>
<li>提交job到JobTracker</li>
</ol>
<p>然后我们再看代码,因为源代码比较多,这里只挑出重要的伪代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">JobStatus submitJobInternal(Job job, Cluster cluster)</span><br><span class="line">  throws ClassNotFoundException, InterruptedException, IOException &#123;</span><br><span class="line">    //validate the jobs output specs</span><br><span class="line">    checkSpecs(job);  //这个就是检查文件路径的方法</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    int maps = writeSplits(job, submitJobDir);//写入切片信息,返回切片数量</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    String queue = conf.get(MRJobConfig.QUEUE_NAME,</span><br><span class="line">        JobConf.DEFAULT_QUEUE_NAME); // 获取任务队列名,源码中有很多这样的获取配置信息的代码,这里只挑出一个说明一下</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    // Write job file to submit dir</span><br><span class="line">     writeConf(conf, submitJobFile); //写入conf文件到hdfs上</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">     //真正提交客户端的方法</span><br><span class="line">     status = submitClient.submitJob(</span><br><span class="line">           jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>client的用户任务,在这里如何调用的基本了解清除了,我们重点看切片是如何写入的,毕竟这是hadoop生态的一个要点:如何通过分片实现计算向数据移动的,点开writeSplite方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,</span><br><span class="line">    Path jobSubmitDir) throws IOException,</span><br><span class="line">    InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">  int maps;</span><br><span class="line">  if (jConf.getUseNewMapper()) &#123;</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir); //hadoop2.x使用newApi,所以进入这个方法</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    maps = writeOldSplits(jConf, jobSubmitDir);</span><br><span class="line">  &#125;</span><br><span class="line">  return maps;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private &lt;T extends InputSplit&gt;</span><br><span class="line"> int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException,</span><br><span class="line">     InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">       List&lt;InputSplit&gt; splits = input.getSplits(job);//不解释,再进入getSpiltes方法</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>进入之后,发现是InputFormat是个接口,有多个子类,那怎么办?查看子类,有 DB数据库的,有Line管每行记录的,切片当然是以文件系统为依托,所以选FileInputFormat<br>点进去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException &#123;</span><br><span class="line">  long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">  long maxSize = getMaxSplitSize(job);</span><br><span class="line">  ...</span><br><span class="line">  long blockSize = file.getBlockSize();</span><br><span class="line">  long splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">  ...</span><br><span class="line">  long bytesRemaining = length;</span><br><span class="line">  while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                        blkLocations[blkIndex].getHosts(),</span><br><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">            bytesRemaining -= splitSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return splites;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>最后终于找到了关于切片的代码,首先开头有两个值minSize和maxSize,分别进入这些方法,发现默认的是0,和long型的最大值,并且受SPLIT_MINSIZE(mapreduce.input.fileinputformat.split.minsize)和SPLIT_MAXSIZE(mapreduce.input.fileinputformat.split.maxsize)这两个配置变量控制,然后继续往下走,有个cpmputeSpliteSize方法,用到了minSize和maxSize还有BlockSize,进入之后我们总算知道了切片如何计算大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Math.max(minSize, Math.min(maxSize, blockSize))</span><br></pre></td></tr></table></figure>
<p><font color="#00FF00">它的语义就是以minSize为最小边界,maxSize为最大边界<br>如果blockSize没有超过最大最小边界,则SpliteSize取BlockSize的值<br>如果超过边界则取边界值.</font><br/><br>继续追getSplites的代码<br>有一个getBlockIndex方法,获取块索引,并且块索引最后会放到切片信息中,<br>我们进入这个方法发现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">protected int getBlockIndex(BlockLocation[] blkLocations,</span><br><span class="line">                              long offset) &#123;</span><br><span class="line">    for (int i = 0 ; i &lt; blkLocations.length; i++) &#123;</span><br><span class="line">      // is the offset inside this block?</span><br><span class="line">      if ((blkLocations[i].getOffset() &lt;= offset) &amp;&amp;</span><br><span class="line">          (offset &lt; blkLocations[i].getOffset() + blkLocations[i].getLength()))&#123;</span><br><span class="line">        return i;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>注意if判断方法,语义就是如何取得切片的block索引,就是对切片在文件中偏移量,做一次”向下取整”,比如说第二block块的偏移量是50,而第二个切片的偏移量是75,位于第二个和第三个块(100)偏移量之间,也就是说,在真正进行计算的时候,会从块的第50的偏移量读取,<br/><br><font color="#00ff00">这就是为什么我们一般把分片大小设置为块大小的倍数,因为这样可以避免交叉读写.</font><br/><br>最后就是写入分片信息包括分片的hosts,size,filepath,offset<br/><br>有了这些信息,就可以支持日后的计算能够保证计算程序找到分片的位置,也就是支持计算向数据移动<br/></p>
<h4 id="map端源码"><a href="#map端源码" class="headerlink" title="map端源码"></a>map端源码</h4><p>首先明确目的,我们Map端的源码是对输入输出进行分析,主要分析map两端的输入输出,</p>
<h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>已知我们Map是通过MapTask类运行的,那么我们就先进入MapTask类,先找run方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line"> public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)</span><br><span class="line">   throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     if (conf.getNumReduceTasks() == 0) &#123;</span><br><span class="line">       mapPhase = getProgress().addPhase(&quot;map&quot;, 1.0f);</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       // If there are reducers then the entire attempt&#x27;s progress will be</span><br><span class="line">       // split between the map phase (67%) and the sort phase (33%).</span><br><span class="line">       mapPhase = getProgress().addPhase(&quot;map&quot;, 0.667f);</span><br><span class="line">       sortPhase  = getProgress().addPhase(&quot;sort&quot;, 0.333f);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line">   if (useNewApi) &#123;</span><br><span class="line">     runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     runOldMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">   &#125;</span><br><span class="line">   done(umbilical, reporter);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>首先映入眼帘的是getNumReduceTasks(),获取Reduce的数量,如果数量为零则不进行排序计算,不为排序任务分配全中<br>然后到下面的runNewMapper点进去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">  private &lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;</span><br><span class="line">  void runNewMapper(final JobConf job,</span><br><span class="line">                    final TaskSplitIndex splitIndex,</span><br><span class="line">                    final TaskUmbilicalProtocol umbilical,</span><br><span class="line">                    TaskReporter reporter</span><br><span class="line">                    ) throws IOException, ClassNotFoundException,</span><br><span class="line">                             InterruptedException &#123;</span><br><span class="line">    // make a task context so we can get the classes</span><br><span class="line">    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =</span><br><span class="line">      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,</span><br><span class="line">                                                                  getTaskID(),</span><br><span class="line">                                                                  reporter);</span><br><span class="line">    // make a mapper</span><br><span class="line">    org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt; mapper =</span><br><span class="line">      (org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);</span><br><span class="line">    // make the input format</span><br><span class="line">    org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt; inputFormat =</span><br><span class="line">      (org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);//初始化InputFormat</span><br><span class="line">    // rebuild the input split</span><br><span class="line">    org.apache.hadoop.mapreduce.InputSplit split = null;</span><br><span class="line">    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),</span><br><span class="line">        splitIndex.getStartOffset());//获取切片信息,保证自己拿到最近的切片数据.</span><br><span class="line">    LOG.info(&quot;Processing split: &quot; + split);</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.RecordReader&lt;INKEY,INVALUE&gt; input =</span><br><span class="line">      new NewTrackingRecordReader&lt;INKEY,INVALUE&gt;</span><br><span class="line">        (split, inputFormat, reporter, taskContext);</span><br><span class="line"></span><br><span class="line">    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());</span><br><span class="line">    org.apache.hadoop.mapreduce.RecordWriter output = null;</span><br><span class="line"></span><br><span class="line">    // get an output object</span><br><span class="line">    if (job.getNumReduceTasks() == 0) &#123;</span><br><span class="line">      output =</span><br><span class="line">        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      output = new NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.MapContext&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;</span><br><span class="line">    mapContext =</span><br><span class="line">      new MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(job, getTaskID(),</span><br><span class="line">          input, output,</span><br><span class="line">          committer,</span><br><span class="line">          reporter, split);</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;.Context</span><br><span class="line">        mapperContext =</span><br><span class="line">          new WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;().getMapContext(</span><br><span class="line">              mapContext);</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">      input.initialize(split, mapperContext);</span><br><span class="line">      mapper.run(mapperContext);</span><br><span class="line">      mapPhase.complete();</span><br><span class="line">      setPhase(TaskStatus.Phase.SORT);</span><br><span class="line">      statusUpdate(umbilical);</span><br><span class="line">      input.close();</span><br><span class="line">      input = null;</span><br><span class="line">      output.close(mapperContext);</span><br><span class="line">      output = null;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      closeQuietly(input);</span><br><span class="line">      closeQuietly(output, mapperContext);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>我们先看下面try cacth里面的东西,一般看源码,try语句块里面的东西是比较重要的,<br>在try语句块当中,我们看到了,mapper对象通过run方法运行我们开发编写的map方法,<br>并且且有自己的输入输出.<br>然后从头开始捋,首先通过反射将我们编写的Map对象赋值给Mapper,中间注释的跳过,直接开门见山,我们看一下Mapper的输入类,进入到NewTrackingRecordReader</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NewTrackingRecordReader(org.apache.hadoop.mapreduce.InputSplit split,</span><br><span class="line">       org.apache.hadoop.mapreduce.InputFormat&lt;K, V&gt; inputFormat,</span><br><span class="line">       TaskReporter reporter,</span><br><span class="line">       org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)</span><br><span class="line">       ...</span><br><span class="line">       throws InterruptedException, IOException &#123;</span><br><span class="line">     this.real = inputFormat.createRecordReader(split, taskContext);</span><br><span class="line">     ...</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>进入后发现这是个包装类,有nextKeyalue方法获取我们Map中所需要的键值对,而他调用的是real对象的nextKeyValue,<br>而real对象就是我们的LineRecordReader类型.进入有一个初始化类initialize</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public void initialize(InputSplit genericSplit,</span><br><span class="line">                       TaskAttemptContext context) throws IOException &#123;</span><br><span class="line">  FileSplit split = (FileSplit) genericSplit;</span><br><span class="line">  Configuration job = context.getConfiguration();</span><br><span class="line">  this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);</span><br><span class="line">  start = split.getStart();</span><br><span class="line">  end = start + split.getLength();</span><br><span class="line">  final Path file = split.getPath();</span><br><span class="line"></span><br><span class="line">  // open the file and seek to the start of the split</span><br><span class="line">  final FileSystem fs = file.getFileSystem(job);</span><br><span class="line">  fileIn = fs.open(file);</span><br><span class="line">  ...</span><br><span class="line">    fileIn.seek(start);</span><br><span class="line">  ...</span><br><span class="line">  if (start != 0) &#123;</span><br><span class="line">     start += in.readLine(new Text(), 0,maxBytesToConsume(start));</span><br><span class="line">   &#125;</span><br><span class="line">   this.pos = start;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过seek方法获从自己相应的切片偏移量开始读取信息,<br>最后一个判断是,默认跳过第一条数据的读取,因为切块的原因很有可能第一条信息不完整.然后我们知道,NewTrackingRecordReader在Context对象里而我们的LineRecordReader在NewTrackingRecordReader当中,所以其实最后context对象调用的nextKeyValue其实调用的是LineRecordReader的方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public boolean nextKeyValue() throws IOException &#123;</span><br></pre></td></tr></table></figure>
<pre><code>key.set(pos);
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</code></pre>
<p>而在nextKeyValue里面有一个key.set(pos)其实就是文件的行号赋值给key</p>
<h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><p>好了,输入看完了我们再看一下输出,重新回到MapTask类,我们点开输出类,NewOutputCollector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                      JobConf job,</span><br><span class="line">                      TaskUmbilicalProtocol umbilical,</span><br><span class="line">                      TaskReporter reporter</span><br><span class="line">                      ) throws IOException, ClassNotFoundException &#123;</span><br><span class="line">     collector = createSortingCollector(job, reporter);//获取排序的数据收集器</span><br><span class="line">     partitions = jobContext.getNumReduceTasks();//根据reduce数量进行分区,如果分区数量等于1使用默认的分区器将数据分区为一</span><br><span class="line">     if (partitions &gt; 1) &#123;</span><br><span class="line">       partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">         ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       partitioner = new org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public int getPartition(K key, V value, int numPartitions) &#123;</span><br><span class="line">           return partitions - 1;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>代码的中文注释已经比较详细了,我们继续走,看看排序收集器里面都是什么东西<br/><br>打开createSortingCollector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private &lt;KEY, VALUE&gt; MapOutputCollector&lt;KEY, VALUE&gt;</span><br><span class="line">         createSortingCollector(JobConf job, TaskReporter reporter) &#123;</span><br><span class="line">     Class&lt;?&gt;[] collectorClasses = job.getClasses(</span><br><span class="line">      JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);</span><br><span class="line">    int remainingCollectors = collectorClasses.length;</span><br><span class="line">    Exception lastException = null;</span><br><span class="line">    for (Class clazz : collectorClasses) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        if (!MapOutputCollector.class.isAssignableFrom(clazz)) &#123;</span><br><span class="line">          throw new IOException(&quot;Invalid output collector class: &quot; + clazz.getName() +</span><br><span class="line">            &quot; (does not implement MapOutputCollector)&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        Class&lt;? extends MapOutputCollector&gt; subclazz =</span><br><span class="line">          clazz.asSubclass(MapOutputCollector.class);</span><br><span class="line">        LOG.debug(&quot;Trying map output collector class: &quot; + subclazz.getName());</span><br><span class="line">        MapOutputCollector&lt;KEY, VALUE&gt; collector =</span><br><span class="line">          ReflectionUtils.newInstance(subclazz, job);</span><br><span class="line">        collector.init(context);</span><br><span class="line">        LOG.info(&quot;Map output collector class = &quot; + collector.getClass().getName());</span><br><span class="line">        return collector;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>最后知道了,输出数据的排序收集器就是唤醒缓冲区MapOutputBuffer的子类,打开他的init方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">public void init(MapOutputCollector.Context context</span><br><span class="line">                    ) throws IOException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      ...</span><br><span class="line">      //sanity checks</span><br><span class="line">      final float spillper =</span><br><span class="line">        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);</span><br><span class="line">      final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);     </span><br><span class="line">      ...</span><br><span class="line">      sorter = ReflectionUtils.newInstance(job.getClass(&quot;map.sort.class&quot;,</span><br><span class="line">            QuickSort.class, IndexedSorter.class), job);</span><br><span class="line">    </span><br><span class="line">      ...</span><br><span class="line">      // k/v serialization</span><br><span class="line">      comparator = job.getOutputKeyComparator();</span><br><span class="line">      keyClass = (Class&lt;K&gt;)job.getMapOutputKeyClass();</span><br><span class="line">      valClass = (Class&lt;V&gt;)job.getMapOutputValueClass();</span><br><span class="line">      serializationFactory = new SerializationFactory(job);</span><br><span class="line">      keySerializer = serializationFactory.getSerializer(keyClass);</span><br><span class="line">      keySerializer.open(bb);</span><br><span class="line">      valSerializer = serializationFactory.getSerializer(valClass);</span><br><span class="line">      valSerializer.open(bb);</span><br><span class="line">    </span><br><span class="line">      ...</span><br><span class="line">      // compression</span><br><span class="line">      if (job.getCompressMapOutput()) &#123;</span><br><span class="line">        Class&lt;? extends CompressionCodec&gt; codecClass =</span><br><span class="line">          job.getMapOutputCompressorClass(DefaultCodec.class);</span><br><span class="line">        codec = ReflectionUtils.newInstance(codecClass, job);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        codec = null;</span><br><span class="line">      &#125;</span><br><span class="line">      ...</span><br><span class="line">      if (combinerRunner != null) &#123;</span><br><span class="line">        final Counters.Counter combineOutputCounter =</span><br><span class="line">          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);</span><br><span class="line">        combineCollector= new CombineOutputCollector&lt;K,V&gt;(combineOutputCounter, reporter, job);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        combineCollector = null;</span><br><span class="line">      &#125;</span><br><span class="line">      ...</span><br><span class="line">      spillThread.setDaemon(true);</span><br><span class="line">      spillThread.setName(&quot;SpillThread&quot;);</span><br><span class="line">      spillLock.lock();</span><br><span class="line">      try &#123;</span><br><span class="line">        spillThread.start();</span><br><span class="line">        while (!spillThreadRunning) &#123;</span><br><span class="line">          spillDone.await();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (InterruptedException e) &#123;</span><br><span class="line">        throw new IOException(&quot;Spill thread failed to initialize&quot;, e);</span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        spillLock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">      if (sortSpillException != null) &#123;</span><br><span class="line">        throw new IOException(&quot;Spill thread failed to initialize&quot;,</span><br><span class="line">            sortSpillException);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面的粗略的的分组,代码分别是</p>
<ul>
<li>设置缓冲取大小和溢写百分比(默认100M和0.8)</li>
<li>设置缓冲区数据的排序类(默认快速排序)</li>
<li>获取排序比较器(优先获取设置的比较类,没有的话取Writable类型默认的比较器)</li>
<li>将keyvalue键值对序列化</li>
<li>判断是否启用combiner,如果溢写的小文件数量超过3,则启用combiner合并</li>
<li>获取压缩对象</li>
<li>开启溢写线程</li>
</ul>
<p>这里多嘴几句,因为篇幅有限,先写下buffer的一些特性,以后可以在这个类的源码中验证:</p>
<ul>
<li>buffer本质还是字节数组</li>
<li>buffer有赤道的概念,即分界点,一边输入数据,一边输入索引</li>
<li>索引:固定宽度:16字节,4个int(partition,keystart,valuestart,valuelenth)</li>
<li>combiner默认发生在溢写之前,排序之后</li>
</ul>
<h4 id="reduce端源码"><a href="#reduce端源码" class="headerlink" title="reduce端源码"></a>reduce端源码</h4><p>我们先看Reducer类的注释,头注释翻译过来大概意思就是<br/><br>reducer主要做两件事,一件是拉取shuffle的数据,一件是对数据进行sort,这里的排序不是对数据进行在排序,因为map已经对数据进行过排序了,这里是对map排序过的数据文件进行归并.<br>好了要点说完了,我们直接看ReudceTask的run方法<br>其中有一句代码是这样的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">    RawKeyValueIterator rIter = null;</span><br><span class="line">    ShuffleConsumerPlugin shuffleConsumerPlugin = null;</span><br><span class="line">    shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line">    ShuffleConsumerPlugin.Context shuffleContext =</span><br><span class="line">      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical,</span><br><span class="line">                  super.lDirAlloc, reporter, codec,</span><br><span class="line">                  combinerClass, combineCollector,</span><br><span class="line">                  spilledRecordsCounter, reduceCombineInputCounter,</span><br><span class="line">                  shuffledMapsCounter,</span><br><span class="line">                  reduceShuffleBytes, failedShuffleCounter,</span><br><span class="line">                  mergedMapOutputsCounter,</span><br><span class="line">                  taskStatus, copyPhase, sortPhase, this,</span><br><span class="line">                  mapOutputFile, localMapFiles);</span><br><span class="line">    shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line"></span><br><span class="line">rIter = shuffleConsumerPlugin.run();</span><br></pre></td></tr></table></figure>
<p>最后一句,通过shfulle插件获取迭代器,我们知道基本reduce的数据都是通过迭代器获取的<br/></p>
<h4 id="迭代器的使用"><a href="#迭代器的使用" class="headerlink" title="迭代器的使用"></a>迭代器的使用</h4><p>为什么使用迭代器呢?因为我不可能将数据一次性的装进内存里,最好是通过迭代器维护一个对文件的指针,这样不仅遍历和实现分离,而且谁想要读取这个文件只要生成一个迭代器,维护自己的指针就可以,不会出现强指针或者同一个数据文件占多份内存的情况.<br>然后我们追踪Reducer的的迭代器类,追踪路径如下<br>context.getValues().iterator();  -&gt; ReudceContext -&gt; ReduceContextImpl<br>进入reduce实现类之后最后有一个getValues方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public</span><br><span class="line">Iterable&lt;VALUEIN&gt; getValues() throws IOException, InterruptedException &#123;</span><br><span class="line">  return iterable;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>他返回一个Iterable对象,而Iterable只有一个方法,返回iterator对象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private ValueIterator iterator = new ValueIterator();</span><br><span class="line"> @Override</span><br><span class="line">    public Iterator&lt;VALUEIN&gt; iterator() &#123;</span><br><span class="line">      return iterator;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>而iterator绝对有hasNext对象和next方法(迭代器模式常识)<br>我们看一下他的hasNext方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">   public boolean hasNext() &#123;</span><br><span class="line">     try &#123;</span><br><span class="line">       if (inReset &amp;&amp; backupStore.hasNext()) &#123;</span><br><span class="line">         return true;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125; catch (Exception e) &#123;</span><br><span class="line">       e.printStackTrace();</span><br><span class="line">       throw new RuntimeException(&quot;hasNext failed&quot;, e);</span><br><span class="line">     &#125;</span><br><span class="line">     return firstValue || nextKeyIsSame;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后有一个boolean值nextKeyIsSame,我们先记住它然后我们看ReducerContextImpl的另一个方法nextkey方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public boolean nextKey() throws IOException,InterruptedException &#123;</span><br><span class="line">    while (hasMore &amp;&amp; nextKeyIsSame) &#123;</span><br><span class="line">      nextKeyValue();</span><br><span class="line">    &#125;</span><br><span class="line">    if (hasMore) &#123;</span><br><span class="line">      if (inputKeyCounter != null) &#123;</span><br><span class="line">        inputKeyCounter.increment(1);</span><br><span class="line">      &#125;</span><br><span class="line">      return nextKeyValue();</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return false;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>最后nextKey方法会调用nextkeyValue,这里只给出nextKeyValue的最后一句代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0,</span><br><span class="line">                                     currentRawKey.getLength(),</span><br><span class="line">                                     nextKey.getData(),</span><br><span class="line">                                     nextKey.getPosition(),</span><br><span class="line">                                     nextKey.getLength() - nextKey.getPosition()</span><br><span class="line">                                         ) == 0;</span><br></pre></td></tr></table></figure>
<p>他会他通过判断器进行判断,下一个key是否和现在的key相等,把结果值赋值给nextKeyIsSame,对就是刚刚记住的nextKeyIsSame.<br>也就是说,我们调用的values.hasNext方法,会判断nextKeyIsSame,下一个key是否相同,不同则返回false,触发reducor再次重新调用reduce方法.</p>
<h4 id="比较器的使用"><a href="#比较器的使用" class="headerlink" title="比较器的使用"></a>比较器的使用</h4><p>我们再看一下,ReducerTask的run方法,这里给出关键点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">RawComparator comparator = job.getOutputValueGroupingComparator();</span><br><span class="line"></span><br><span class="line">public RawComparator getOutputValueGroupingComparator() &#123;</span><br><span class="line">    Class&lt;? extends RawComparator&gt; theClass = getClass(</span><br><span class="line">      JobContext.GROUP_COMPARATOR_CLASS, null, RawComparator.class);</span><br><span class="line">    if (theClass == null) &#123;</span><br><span class="line">      return getOutputKeyComparator();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return ReflectionUtils.newInstance(theClass, this);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这个是获取分组比较器的方法,优先使用用户的分组比较器,如果用户的分组比较器为null,则使用默认的key的Writable类型包含的比较器.<br>而reduce也有排序比较器通过getOutputKeyComparator()获取,<br>再加上map的排序比较器,我们有三个比较器可以自定义也可取默认的比较器,mapreduce给了我们很灵活的选择取加工数据.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/19/mapreduce%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/19/mapreduce%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">mapreduce&yarn笔记总结 01</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2020-08-19 21:14:32 / Modified: 16:22:18" itemprop="dateCreated datePublished" datetime="2020-08-19T21:14:32+08:00">2020-08-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="背景"><a href="#背景" class="headerlink" title="背景,"></a>背景,</h4><p>为了体现分布式计算的优点,将数据分而治之再进行相应方面的计算.hadoop提出了mapreduce计算模型</p>
<h4 id="计算模型"><a href="#计算模型" class="headerlink" title="计算模型"></a>计算模型</h4><p><img src="http://picture.lemcoden.xyz/mapreduce/mr_mod.png" alt="计算模型"></p>
<ul>
<li>map 端负责将拆分出来的数据进行映射,变换,过滤.体现在一进N出</li>
<li>reduce 端负责将数据整合归纳,缩小,分解,一般是一组数据进N出</li>
<li>不管是map还是reduce处理的数据结构基本都是&lt;key,value&gt;的形式划分的</li>
<li>最基本的数据格式确定后,会有数据迁移更加细致的流程</li>
<li>首先分布式计算是基于分布式文件系统的,而分布式文件系统的存储模型以块为单位,所以分布式的物理模型以split(分片)为的单位</li>
<li>默认每个split对应一个map进程</li>
<li>split的数据对应map计算之后并不会直接写入磁盘而是先写入环形缓冲区 || 因为每一次IO都会调用linux内核,所以不是一条记录IO一次,而是缓冲区写满后进行一次性IO</li>
<li>跳过中间阶段,看reduce,reduce会根据之前数据的partion数量对应开启reduce进程.</li>
<li>默认一个reduce进程对应一个partition,再次体现分而治之的理念</li>
<li>map段的数据经过buffer之后会为reduce分区作准备,所以会先进行分区,对key进行取模操作划分出partition,会将数据结构转换成&lt;key,value,partition&gt;的形式.</li>
<li>进行partition之后,为减少reduce的拉取IO操作(总不能一条数据拉取查找一次吧),会将partition按照进行再次分片(split).</li>
<li>数据进行分片之后,再按照partition进行小文件排序(sort),同时还会进行key的第二次排序,关于为什么还会进行key的排序,到reduce端会解释<h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h4><h4 id="遥远的hadoop1-x"><a href="#遥远的hadoop1-x" class="headerlink" title="遥远的hadoop1.x"></a>遥远的hadoop1.x</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(ps:因为找不到合适的图,就分开为client端和job端的架构)</span><br></pre></td></tr></table></figure>
计算模型出现后,就需要搭建整体的框架,首先我们说我们的主要角色有client,JobTracker,TaskTracker<br><img src="http://picture.lemcoden.xyz/mapreduce/mr_arch_client.jpg" alt="clinet架构"></li>
</ul>
<p>我们client端主要做四件事:</p>
<ul>
<li>会根据每次的计算数据,咨询NN元数据(block) =&gt; 算出spilt切片清单</li>
<li>生成计算程序未来运行相关的配置文件</li>
<li>将jar包,split的切片清单,配置文件上传到HDFS目录当中</li>
<li>cli调用jobTracker,通知启动一个计算程序,并且告知文件都放在了hdfs的哪些地方</li>
</ul>
<p><img src="http://picture.lemcoden.xyz/mapreduce/mr_arch_job.jpg" alt="job架构"></p>
<ul>
<li>jobTracker会根据cli提交的信息,去HDFS上寻找Jar包程序,split清单,以及配置文件</li>
<li>根据拿到的切片清单和配置文件,以及收到的TaskTracker汇报的资源,最终确定每一个spilt应该去往哪个个节点</li>
<li>TaskTracker会在汇报心跳的时候拿到分配给自己的人物信息</li>
<li>TaskTrakcer取回任务后会从hdfs下载jar包,xml配置文件到本机</li>
<li>TaskTraker会根据xml配置文件以及JobTrakcer的任务描述,从jar包中抽取出mapreduce任务运行</li>
</ul>
<p>这个是hadoop1.x的mapreduce的任务调度,到了hadoop2.x的时候这种架构被重新修改,why?</p>
<ol>
<li>任务调度框架jobTracker和TaskTracker使用的是主从架构,那必将出现两个问题,一个是单点故障问题</li>
<li>另一个则是主节点压力过大的问题</li>
<li>JobTracker同时负责资源的调度以及计算任务管理,两者耦合,如果引入新的计算框架则不能复用资源管理<h4 id="改进后的Hadoop2-x"><a href="#改进后的Hadoop2-x" class="headerlink" title="改进后的Hadoop2.x"></a>改进后的Hadoop2.x</h4>hadoop2.x后将JobTraker的资源调度功能抽出,封装为Yarn资源管理框架,并配置了高可用.<br>hadoop2.x的计算与资源管理架构如下<br><img src="http://picture.lemcoden.xyz/mapreduce/mr_arch_yarn.png" alt="job架构"><br>主要角色有client,ResourceManager,NodeManager,ApplicatioMaster以及Container</li>
</ol>
<ul>
<li>client与之前的流程一致</li>
<li>client会将job提交到ResourceManager</li>
<li>ResourceManger接收到job请求后,会在集群当中挑一台不忙的节点,在NodeManager中启动一个ApplicationMaster进程</li>
<li>ApplicationMaster进程启动之后,会去HDFS下载Splite清单以及配置文件,并将配置清单发送ResouceManager,申请Container</li>
<li>ResouceManager会根据清单计算出使用多少资源,并将根据现有资源通知NodeManager启动相应的Container容器</li>
<li>Container向App Mstr(Application Master)反向注册,此时App Mstr才知道有多少Container可以执行任务</li>
<li>App Mstr会向Container发送Map Task消息.</li>
<li>Container受到消息后,会从hdfs下载jar包,并通过反射取出对象执行MapReduce任务<br>相较于Hadoop1.x,2.x的框架很好的解决的1.x框架出现的问题:</li>
</ul>
<ol>
<li>单点故障<br/></li>
</ol>
<ul>
<li>App Mstr由ResouceManager监控管理,所以当App Mstr没有心跳时,RM(Resource Manager)会触发失败重试机制,ResourceManager会在其他节点重新启动个App Mstr</li>
<li>ResourceManager本身可以配置高可用</li>
<li>Container 也会有失败重试</li>
</ul>
<ol start="2">
<li>压力过大问题</li>
</ol>
<ul>
<li>yarn中每个计算程序自有一个AppMaster,每个AppMaster之负责自己计算程序的任务调度.</li>
<li>AppMasters是在不同的节点中启动的,默认有了负载的光环</li>
</ul>
<ol start="3">
<li>资源管理与任务调度耦合</li>
</ol>
<ul>
<li>yarn只负责资源管理,不负责具体的任务调度</li>
<li>yarn作为资源管理框架可以被其他计算程序复用(只需要继承AppMaster类就可以)</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/19/hadoop%E9%9B%86%E7%BE%A4HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/19/hadoop%E9%9B%86%E7%BE%A4HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">hadoop集群HA高可用配置总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-19 16:27:16" itemprop="dateCreated datePublished" datetime="2020-08-19T16:27:16+08:00">2020-08-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-09 10:58:23" itemprop="dateModified" datetime="2020-11-09T10:58:23+08:00">2020-11-09</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="基础设施"><a href="#基础设施" class="headerlink" title="基础设施"></a>基础设施</h3><ul>
<li>网卡静态IP<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ifconfig 查看网卡信息</span><br><span class="line">vim /etc/udev/rules.d/70-persistent-ipoib.rules</span><br><span class="line">              ACTION==&quot;add&quot;, SUBSYSTEM==&quot;net&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;type&#125;==&quot;32&quot;, ATTR&#123;address&#125;==&quot;?*00:02:c9:03:00:31:78:f2&quot;, NAME=&quot;网卡名&quot;</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-网卡名</span><br><span class="line">POXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static  //设置静态IP</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">NAME=enp0s3</span><br><span class="line">UUID=290c55a8-1b88-4d99-b741-dcfe455f5c2c</span><br><span class="line">DEVICE=enp0s3</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.0.101  //一般本地IP最后依次增加</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATWAY=192.168.0.1 //同一集群必须同一网关</span><br></pre></td></tr></table></figure></li>
<li>设置hosts<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line">192.168.0.101 hadoop01</span><br><span class="line">192.168.0.101 hadoop02</span><br></pre></td></tr></table></figure></li>
<li>关闭防火墙<br/><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Centos6.x</span><br><span class="line">service iptables stop</span><br><span class="line">service iptables status</span><br><span class="line">chkconfig iptables off</span><br><span class="line">Centos7.x</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl status firewalld.service</span><br><span class="line">systemctl disable firewalld.service</span><br></pre></td></tr></table></figure></li>
<li>关闭 selinux<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure></li>
<li>作时间同步<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">yum install ntp  -y</span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line">主节点</span><br><span class="line">          注释掉其他server</span><br><span class="line">          server ntp1.aliyun.com</span><br><span class="line">          server 127.127.1.0</span><br><span class="line">          fudge 127.127.1.0 stratum 10</span><br><span class="line"> 从节点</span><br><span class="line">          server 192.168.0.101</span><br><span class="line">从节点设置crontab同步</span><br><span class="line">vim /etc/crontab</span><br><span class="line">10 20 * * * /usr/sbin/ntpdate -u 192.168.0.101</span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl enable ntpd</span><br></pre></td></tr></table></figure></li>
<li>安装JDK<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rpm -i   jdk-8u181-linux-x64.rpm</span><br><span class="line">		*有一些软件只认：/usr/java/default</span><br><span class="line">vi /etc/profile     </span><br><span class="line">	  export  JAVA_HOME=/usr/java/default</span><br><span class="line">		export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">source /etc/profile   |  .    /etc/profile</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>免密登陆<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa  //一路回车</span><br><span class="line">ssh-copy-id 其他节点</span><br></pre></td></tr></table></figure>
<h3 id="集群安装配置"><a href="#集群安装配置" class="headerlink" title="集群安装配置"></a>集群安装配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf tar包 -C 目录</span><br></pre></td></tr></table></figure>
<h4 id="java配置"><a href="#java配置" class="headerlink" title="java配置"></a>java配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rpm -i   jdk-8u181-linux-x64.rpm</span><br><span class="line">		*有一些软件只认：/usr/java/default</span><br><span class="line">vi /etc/profile     </span><br><span class="line">		export  JAVA_HOME=/usr/java/default</span><br><span class="line">		export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="hadooop-配置"><a href="#hadooop-配置" class="headerlink" title="hadooop 配置"></a>hadooop 配置</h4><table>
<thead>
<tr>
<th></th>
<th>NN</th>
<th>NN</th>
<th>JN</th>
<th>ZKFC</th>
<th>ZK</th>
<th>DN</th>
<th>RM</th>
<th>NM</th>
</tr>
</thead>
<tbody><tr>
<td>node01</td>
<td>*</td>
<td></td>
<td>*</td>
<td>*</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>node02</td>
<td></td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td></td>
<td>*</td>
</tr>
<tr>
<td>node03</td>
<td></td>
<td></td>
<td>*</td>
<td></td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
</tr>
<tr>
<td>node04</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
</tr>
</tbody></table>
<p>hadoop配置的七个文件:<br/><br>core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml <br/><br>hadoop-env.sh mapred-env.sh slaves <br/><br>core-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;hdfs://mycluster&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		 &lt;property&gt;</span><br><span class="line">		   &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">		   &lt;value&gt;node02:2181,node03:2181,node04:2181&lt;/value&gt;</span><br><span class="line">		 &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>hdfs-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># 以下是  一对多，逻辑到物理节点的映射</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">				&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">				&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">			&lt;/property&gt;</span><br><span class="line">			&lt;property&gt;</span><br><span class="line">				&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">				&lt;value&gt;/var/bigdata/hadoop/local/dfs/name&lt;/value&gt;</span><br><span class="line">			&lt;/property&gt;</span><br><span class="line">			&lt;property&gt;</span><br><span class="line">				&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">				&lt;value&gt;/var/bigdata/hadoop/local/dfs/data&lt;/value&gt;</span><br><span class="line">			&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;mycluster&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node01:8020&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node02:8020&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node01:50070&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node02:50070&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		#以下是JN在哪里启动，数据存那个磁盘</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;qjournal://node01:8485;node02:8485;node03:8485/mycluster&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;/var/bigdata/hadoop/ha/dfs/jn&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		#HA角色切换的代理类和实现方法，我们用的ssh免密</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		#开启自动化： 启动zkfc</span><br><span class="line">		 &lt;property&gt;</span><br><span class="line">		   &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">		   &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">		 &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;node02:2181,node03:2181,node04:2181&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;lemcoden_yarn_cluster&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;node03&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;node04&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>hadoop-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/default</span><br></pre></td></tr></table></figure>
<p>mapred-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/default</span><br></pre></td></tr></table></figure>
<p>slaves</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node02</span><br><span class="line">node03</span><br><span class="line">node04</span><br></pre></td></tr></table></figure>
<p>设置环境变量 <br/></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">		export  JAVA_HOME=/usr/java/default</span><br><span class="line">		export HADOOP_HOME=/opt/bigdata/hadoop-2.6.5</span><br><span class="line">		export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>软件包分发:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">			scp -r ./bigdata/  node02:`pwd`</span><br><span class="line">			scp -r ./bigdata/  node03:`pwd`</span><br><span class="line">			scp -r ./bigdata/  node04:`pwd`</span><br></pre></td></tr></table></figure>

<h4 id="zookeeper配置"><a href="#zookeeper配置" class="headerlink" title="zookeeper配置:"></a>zookeeper配置:</h4><p>cp zoo_sanmple.cfg zoo.cfg <br/><br>zoo.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   datadir=/var/bigdata/hadoop/zk</span><br><span class="line">server.1=node02:2888:3888</span><br><span class="line">server.2=node03:2888:3888</span><br><span class="line">server.3=node04:2888:3888</span><br></pre></td></tr></table></figure>
<p>mkdir /var/bigdata/hadoop/zk<br>配置环境变量:<br>node02</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">				export ZOOKEEPER_HOME=/opt/bigdata/zookeeper-3.4.6</span><br><span class="line">				export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin</span><br><span class="line">. /etc/profile</span><br></pre></td></tr></table></figure>
<p>分发软件包:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r ./zookeeper-3.4.6  node03:`pwd`</span><br><span class="line">scp -r ./zookeeper-3.4.6  node04:`pwd`</span><br></pre></td></tr></table></figure>
<p>设置myid:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">node03:</span><br><span class="line">			mkdir /var/bigdata/hadoop/zk</span><br><span class="line">			echo 2 &gt;  /var/bigdata/hadoop/zk/myid</span><br><span class="line">			*环境变量</span><br><span class="line">			. /etc/profile</span><br><span class="line">node04:</span><br><span class="line">			mkdir /var/bigdata/hadoop/zk</span><br><span class="line">			echo 3 &gt;  /var/bigdata/hadoop/zk/myid</span><br><span class="line">			*环境变量</span><br><span class="line">			. /etc/profile</span><br></pre></td></tr></table></figure>
<h3 id="集群初始化启动"><a href="#集群初始化启动" class="headerlink" title="集群初始化启动"></a>集群初始化启动</h3><p>先启动JN (node1~node3)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>
<p>选择一个NN格式化(只在初始化时做一次)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
<p>在另一台机器里同步元数据(先启动node1的namenode)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>
<p>格式化zk(也是只做一次)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc  -formatZK</span><br></pre></td></tr></table></figure>
<p>启动集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br><span class="line">node03-04:</span><br><span class="line">  yarn-daemon.sh start recourcemanager</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lemcoden</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="0" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;forest&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script class="next-config" data-name="nprogress" type="application/json">{&quot;enable&quot;:true,&quot;spinner&quot;:true}</script>
  <script src="/js/third-party/nprogress.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
