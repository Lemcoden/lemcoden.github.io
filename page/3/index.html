<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;lemcoden.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="Lemcoden">
<meta property="og:url" content="https://lemcoden.github.io/page/3/index.html">
<meta property="og:site_name" content="Lemcoden">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Lemcoden">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://lemcoden.github.io/page/3/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;page&#x2F;3&#x2F;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Lemcoden</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Lemcoden</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">来自于大数据攻城狮的分享</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Lemcoden</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/09/04/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/04/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/" class="post-title-link" itemprop="url">hive-笔记总结02</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-09-04 15:44:41" itemprop="dateCreated datePublished" datetime="2020-09-04T15:44:41+08:00">2020-09-04</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-15 16:52:16" itemprop="dateModified" datetime="2020-11-15T16:52:16+08:00">2020-11-15</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>我们接着上次的hive继续总结</p>
<h3 id="配置补充-hiveserer2的高可用"><a href="#配置补充-hiveserer2的高可用" class="headerlink" title="配置补充,hiveserer2的高可用"></a>配置补充,hiveserer2的高可用</h3><p>node2-hive-site.xml</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/09/04/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%9302/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/27/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/27/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">hive-笔记总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-27 20:50:49" itemprop="dateCreated datePublished" datetime="2020-08-27T20:50:49+08:00">2020-08-27</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-15 16:52:12" itemprop="dateModified" datetime="2020-11-15T16:52:12+08:00">2020-11-15</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="who-what-why"><a href="#who-what-why" class="headerlink" title="who,what,why"></a>who,what,why</h3><h4 id="hive的作用"><a href="#hive的作用" class="headerlink" title="hive的作用"></a>hive的作用</h4><p>按照做笔记的习惯来说,说一个新的大数据平台框架,一般先从模型说起,而hive本身是企业级数据仓库工具,基于mapreduce计算引擎的封装(2.x之后逐渐将官方计算引擎指定为spark)所以,就其本身而言并没有模型可以讨论.<br>但是我们可以聊聊他的作用,他是解决什么需求的:</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/08/27/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/21/mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/21/mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/" class="post-title-link" itemprop="url">mapreduce笔记-源码剖析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-21 17:56:07" itemprop="dateCreated datePublished" datetime="2020-08-21T17:56:07+08:00">2020-08-21</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-08-24 23:13:20" itemprop="dateModified" datetime="2020-08-24T23:13:20+08:00">2020-08-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="为什么要看源码"><a href="#为什么要看源码" class="headerlink" title="为什么要看源码"></a>为什么要看源码</h4><p>1.为了更好的使用框架的Api解决问题,比如说我们遇到一个问题,需要修改mapreduce分片的大小,如果没看过源码,可能会写很多代码,甚至重新调整文件block的大小上传,但是看过源码的都懂,只要简单的修改minSplite和maxSplite这两个配置属性就可以.<br>2.为了学习框架本身的设计方法,应用到日常开发中.<br>(此次源码分析的hadoop版本为2.7.2)</p>
<h4 id="怎么看源码"><a href="#怎么看源码" class="headerlink" title="怎么看源码"></a>怎么看源码</h4><p>要有目的性的的看源码,如果不带目的直接看的话,会很晕,源码一般信息量很大,而且很多部分是没有必要的,我们要取其精髓,忽略与当前目标无关的部分.并将重要的部分记录下来,最好是自己可以用伪代码实现,并且能够讲出其中的逻辑点和技术应用点</p>
<h4 id="mapreduce源码梗概"><a href="#mapreduce源码梗概" class="headerlink" title="mapreduce源码梗概"></a>mapreduce源码梗概</h4><p>mapreduce笔者目前了解的主要有三部分,client端,map计算端的输入输出,reduce计算端的输入输出.<br>client端主要验证client端的任务,以及关键切片部分的逻辑<br>map端和reduce端输入输出,一是看数据格式相关的转换<br>二是看shuffle的主要流程,看有哪些可以在开发过程中可以微调的地方</p>
<h4 id="client端"><a href="#client端" class="headerlink" title="client端"></a>client端</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.waitForCompletion(true);</span><br></pre></td></tr></table></figure>
<p>我们编写mapreduce程序的时候到最后执行这个方法的时候,任务才会真正提交,<br>那我们提交任务之后客户端都是如何做得呢?<br>点进去查看源码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public boolean waitForCompletion(boolean verbose</span><br><span class="line">                                   ) throws IOException, InterruptedException,</span><br><span class="line">                                            ClassNotFoundException &#123;</span><br><span class="line">    if (state == JobState.DEFINE) &#123;  //检查任务状态,是否允许提交</span><br><span class="line">      submit(); //提交任务方法</span><br><span class="line">    &#125;</span><br><span class="line">    if (verbose) &#123;</span><br><span class="line">      monitorAndPrintJob(); //监控并且获取任务的详细运行信息</span><br><span class="line">    &#125; else ...</span><br><span class="line">    return isSuccessful();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>提交之后,再调用方法获取任务的详细信息,可见这个任务是异步任务.<br>我们关系心的任务如何提交的,那么就进入submit方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void submit()</span><br><span class="line">       throws IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  setUseNewAPI();  //hadoop1.x和hadoop2.x的mapreduce架构不同,所以这里是新版API</span><br><span class="line">  connect(); //连接yarn resourceManager</span><br><span class="line">  final JobSubmitter submitter =</span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient()); //获取集群HDFS操作对象和Client对象,为以后把分片信息,配置文件,jar包,通过FileSystem上传到hdfs上</span><br><span class="line">  status = ugi.doAs(new PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    public JobStatus run() throws IOException, InterruptedException,</span><br><span class="line">    ClassNotFoundException &#123;</span><br><span class="line">      return submitter.submitJobInternal(Job.this, cluster); //这里就是提交Job的地方</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(&quot;The url to track the job: &quot; + getTrackingURL());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>再进入submitJobInternal方法,然后发现注释这部分很有东西</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Internal method for submitting jobs to the system.</span><br><span class="line">The job submission process involves:</span><br><span class="line">1.Checking the input and output specifications of the job.</span><br><span class="line">2.Computing the InputSplits for the job.</span><br><span class="line">3.Setup the requisite accounting information for the DistributedCache of the job, if necessary.</span><br><span class="line">4.Copying the job&#x27;s jar and configuration to the map-reduce system directory on the distributed file-system.</span><br><span class="line">5.Submitting the job to the JobTracker and optionally monitoring it&#x27;s status.</span><br></pre></td></tr></table></figure>
<p>简单的翻译以下就懂了,里面会</p>
<ol>
<li>检查job输入输出路径</li>
<li>计算分片的大小</li>
<li>有必要的话,为作业的缓存设置账户信息</li>
<li>把job的jar包,配置文件拷贝到hdfs</li>
<li>提交job到JobTracker</li>
</ol>
<p>然后我们再看代码,因为源代码比较多,这里只挑出重要的伪代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">JobStatus submitJobInternal(Job job, Cluster cluster)</span><br><span class="line">  throws ClassNotFoundException, InterruptedException, IOException &#123;</span><br><span class="line">    //validate the jobs output specs</span><br><span class="line">    checkSpecs(job);  //这个就是检查文件路径的方法</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    int maps = writeSplits(job, submitJobDir);//写入切片信息,返回切片数量</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    String queue = conf.get(MRJobConfig.QUEUE_NAME,</span><br><span class="line">        JobConf.DEFAULT_QUEUE_NAME); // 获取任务队列名,源码中有很多这样的获取配置信息的代码,这里只挑出一个说明一下</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    // Write job file to submit dir</span><br><span class="line">     writeConf(conf, submitJobFile); //写入conf文件到hdfs上</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">     //真正提交客户端的方法</span><br><span class="line">     status = submitClient.submitJob(</span><br><span class="line">           jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>client的用户任务,在这里如何调用的基本了解清除了,我们重点看切片是如何写入的,毕竟这是hadoop生态的一个要点:如何通过分片实现计算向数据移动的,点开writeSplite方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,</span><br><span class="line">    Path jobSubmitDir) throws IOException,</span><br><span class="line">    InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">  int maps;</span><br><span class="line">  if (jConf.getUseNewMapper()) &#123;</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir); //hadoop2.x使用newApi,所以进入这个方法</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    maps = writeOldSplits(jConf, jobSubmitDir);</span><br><span class="line">  &#125;</span><br><span class="line">  return maps;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private &lt;T extends InputSplit&gt;</span><br><span class="line"> int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException,</span><br><span class="line">     InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">       List&lt;InputSplit&gt; splits = input.getSplits(job);//不解释,再进入getSpiltes方法</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>进入之后,发现是InputFormat是个接口,有多个子类,那怎么办?查看子类,有 DB数据库的,有Line管每行记录的,切片当然是以文件系统为依托,所以选FileInputFormat<br>点进去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException &#123;</span><br><span class="line">  long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">  long maxSize = getMaxSplitSize(job);</span><br><span class="line">  ...</span><br><span class="line">  long blockSize = file.getBlockSize();</span><br><span class="line">  long splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">  ...</span><br><span class="line">  long bytesRemaining = length;</span><br><span class="line">  while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                        blkLocations[blkIndex].getHosts(),</span><br><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">            bytesRemaining -= splitSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return splites;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>最后终于找到了关于切片的代码,首先开头有两个值minSize和maxSize,分别进入这些方法,发现默认的是0,和long型的最大值,并且受SPLIT_MINSIZE(mapreduce.input.fileinputformat.split.minsize)和SPLIT_MAXSIZE(mapreduce.input.fileinputformat.split.maxsize)这两个配置变量控制,然后继续往下走,有个cpmputeSpliteSize方法,用到了minSize和maxSize还有BlockSize,进入之后我们总算知道了切片如何计算大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Math.max(minSize, Math.min(maxSize, blockSize))</span><br></pre></td></tr></table></figure>
<p><font color="#00FF00">它的语义就是以minSize为最小边界,maxSize为最大边界<br>如果blockSize没有超过最大最小边界,则SpliteSize取BlockSize的值<br>如果超过边界则取边界值.</font><br/><br>继续追getSplites的代码<br>有一个getBlockIndex方法,获取块索引,并且块索引最后会放到切片信息中,<br>我们进入这个方法发现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">protected int getBlockIndex(BlockLocation[] blkLocations,</span><br><span class="line">                              long offset) &#123;</span><br><span class="line">    for (int i = 0 ; i &lt; blkLocations.length; i++) &#123;</span><br><span class="line">      // is the offset inside this block?</span><br><span class="line">      if ((blkLocations[i].getOffset() &lt;= offset) &amp;&amp;</span><br><span class="line">          (offset &lt; blkLocations[i].getOffset() + blkLocations[i].getLength()))&#123;</span><br><span class="line">        return i;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>注意if判断方法,语义就是如何取得切片的block索引,就是对切片在文件中偏移量,做一次”向下取整”,比如说第二block块的偏移量是50,而第二个切片的偏移量是75,位于第二个和第三个块(100)偏移量之间,也就是说,在真正进行计算的时候,会从块的第50的偏移量读取,<br/><br><font color="#00ff00">这就是为什么我们一般把分片大小设置为块大小的倍数,因为这样可以避免交叉读写.</font><br/><br>最后就是写入分片信息包括分片的hosts,size,filepath,offset<br/><br>有了这些信息,就可以支持日后的计算能够保证计算程序找到分片的位置,也就是支持计算向数据移动<br/></p>
<h4 id="map端源码"><a href="#map端源码" class="headerlink" title="map端源码"></a>map端源码</h4><p>首先明确目的,我们Map端的源码是对输入输出进行分析,主要分析map两端的输入输出,</p>
<h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>已知我们Map是通过MapTask类运行的,那么我们就先进入MapTask类,先找run方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line"> public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)</span><br><span class="line">   throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     if (conf.getNumReduceTasks() == 0) &#123;</span><br><span class="line">       mapPhase = getProgress().addPhase(&quot;map&quot;, 1.0f);</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       // If there are reducers then the entire attempt&#x27;s progress will be</span><br><span class="line">       // split between the map phase (67%) and the sort phase (33%).</span><br><span class="line">       mapPhase = getProgress().addPhase(&quot;map&quot;, 0.667f);</span><br><span class="line">       sortPhase  = getProgress().addPhase(&quot;sort&quot;, 0.333f);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line">   if (useNewApi) &#123;</span><br><span class="line">     runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     runOldMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">   &#125;</span><br><span class="line">   done(umbilical, reporter);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>首先映入眼帘的是getNumReduceTasks(),获取Reduce的数量,如果数量为零则不进行排序计算,不为排序任务分配全中<br>然后到下面的runNewMapper点进去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">  private &lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;</span><br><span class="line">  void runNewMapper(final JobConf job,</span><br><span class="line">                    final TaskSplitIndex splitIndex,</span><br><span class="line">                    final TaskUmbilicalProtocol umbilical,</span><br><span class="line">                    TaskReporter reporter</span><br><span class="line">                    ) throws IOException, ClassNotFoundException,</span><br><span class="line">                             InterruptedException &#123;</span><br><span class="line">    // make a task context so we can get the classes</span><br><span class="line">    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =</span><br><span class="line">      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,</span><br><span class="line">                                                                  getTaskID(),</span><br><span class="line">                                                                  reporter);</span><br><span class="line">    // make a mapper</span><br><span class="line">    org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt; mapper =</span><br><span class="line">      (org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);</span><br><span class="line">    // make the input format</span><br><span class="line">    org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt; inputFormat =</span><br><span class="line">      (org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);//初始化InputFormat</span><br><span class="line">    // rebuild the input split</span><br><span class="line">    org.apache.hadoop.mapreduce.InputSplit split = null;</span><br><span class="line">    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),</span><br><span class="line">        splitIndex.getStartOffset());//获取切片信息,保证自己拿到最近的切片数据.</span><br><span class="line">    LOG.info(&quot;Processing split: &quot; + split);</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.RecordReader&lt;INKEY,INVALUE&gt; input =</span><br><span class="line">      new NewTrackingRecordReader&lt;INKEY,INVALUE&gt;</span><br><span class="line">        (split, inputFormat, reporter, taskContext);</span><br><span class="line"></span><br><span class="line">    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());</span><br><span class="line">    org.apache.hadoop.mapreduce.RecordWriter output = null;</span><br><span class="line"></span><br><span class="line">    // get an output object</span><br><span class="line">    if (job.getNumReduceTasks() == 0) &#123;</span><br><span class="line">      output =</span><br><span class="line">        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      output = new NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.MapContext&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;</span><br><span class="line">    mapContext =</span><br><span class="line">      new MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(job, getTaskID(),</span><br><span class="line">          input, output,</span><br><span class="line">          committer,</span><br><span class="line">          reporter, split);</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;.Context</span><br><span class="line">        mapperContext =</span><br><span class="line">          new WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;().getMapContext(</span><br><span class="line">              mapContext);</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">      input.initialize(split, mapperContext);</span><br><span class="line">      mapper.run(mapperContext);</span><br><span class="line">      mapPhase.complete();</span><br><span class="line">      setPhase(TaskStatus.Phase.SORT);</span><br><span class="line">      statusUpdate(umbilical);</span><br><span class="line">      input.close();</span><br><span class="line">      input = null;</span><br><span class="line">      output.close(mapperContext);</span><br><span class="line">      output = null;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      closeQuietly(input);</span><br><span class="line">      closeQuietly(output, mapperContext);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>我们先看下面try cacth里面的东西,一般看源码,try语句块里面的东西是比较重要的,<br>在try语句块当中,我们看到了,mapper对象通过run方法运行我们开发编写的map方法,<br>并且且有自己的输入输出.<br>然后从头开始捋,首先通过反射将我们编写的Map对象赋值给Mapper,中间注释的跳过,直接开门见山,我们看一下Mapper的输入类,进入到NewTrackingRecordReader</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NewTrackingRecordReader(org.apache.hadoop.mapreduce.InputSplit split,</span><br><span class="line">       org.apache.hadoop.mapreduce.InputFormat&lt;K, V&gt; inputFormat,</span><br><span class="line">       TaskReporter reporter,</span><br><span class="line">       org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)</span><br><span class="line">       ...</span><br><span class="line">       throws InterruptedException, IOException &#123;</span><br><span class="line">     this.real = inputFormat.createRecordReader(split, taskContext);</span><br><span class="line">     ...</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>进入后发现这是个包装类,有nextKeyalue方法获取我们Map中所需要的键值对,而他调用的是real对象的nextKeyValue,<br>而real对象就是我们的LineRecordReader类型.进入有一个初始化类initialize</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public void initialize(InputSplit genericSplit,</span><br><span class="line">                       TaskAttemptContext context) throws IOException &#123;</span><br><span class="line">  FileSplit split = (FileSplit) genericSplit;</span><br><span class="line">  Configuration job = context.getConfiguration();</span><br><span class="line">  this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);</span><br><span class="line">  start = split.getStart();</span><br><span class="line">  end = start + split.getLength();</span><br><span class="line">  final Path file = split.getPath();</span><br><span class="line"></span><br><span class="line">  // open the file and seek to the start of the split</span><br><span class="line">  final FileSystem fs = file.getFileSystem(job);</span><br><span class="line">  fileIn = fs.open(file);</span><br><span class="line">  ...</span><br><span class="line">    fileIn.seek(start);</span><br><span class="line">  ...</span><br><span class="line">  if (start != 0) &#123;</span><br><span class="line">     start += in.readLine(new Text(), 0,maxBytesToConsume(start));</span><br><span class="line">   &#125;</span><br><span class="line">   this.pos = start;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过seek方法获从自己相应的切片偏移量开始读取信息,<br>最后一个判断是,默认跳过第一条数据的读取,因为切块的原因很有可能第一条信息不完整.然后我们知道,NewTrackingRecordReader在Context对象里而我们的LineRecordReader在NewTrackingRecordReader当中,所以其实最后context对象调用的nextKeyValue其实调用的是LineRecordReader的方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public boolean nextKeyValue() throws IOException &#123;</span><br></pre></td></tr></table></figure>
<pre><code>key.set(pos);
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</code></pre>
<p>而在nextKeyValue里面有一个key.set(pos)其实就是文件的行号赋值给key</p>
<h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><p>好了,输入看完了我们再看一下输出,重新回到MapTask类,我们点开输出类,NewOutputCollector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                      JobConf job,</span><br><span class="line">                      TaskUmbilicalProtocol umbilical,</span><br><span class="line">                      TaskReporter reporter</span><br><span class="line">                      ) throws IOException, ClassNotFoundException &#123;</span><br><span class="line">     collector = createSortingCollector(job, reporter);//获取排序的数据收集器</span><br><span class="line">     partitions = jobContext.getNumReduceTasks();//根据reduce数量进行分区,如果分区数量等于1使用默认的分区器将数据分区为一</span><br><span class="line">     if (partitions &gt; 1) &#123;</span><br><span class="line">       partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">         ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       partitioner = new org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public int getPartition(K key, V value, int numPartitions) &#123;</span><br><span class="line">           return partitions - 1;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>代码的中文注释已经比较详细了,我们继续走,看看排序收集器里面都是什么东西<br/><br>打开createSortingCollector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private &lt;KEY, VALUE&gt; MapOutputCollector&lt;KEY, VALUE&gt;</span><br><span class="line">         createSortingCollector(JobConf job, TaskReporter reporter) &#123;</span><br><span class="line">     Class&lt;?&gt;[] collectorClasses = job.getClasses(</span><br><span class="line">      JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);</span><br><span class="line">    int remainingCollectors = collectorClasses.length;</span><br><span class="line">    Exception lastException = null;</span><br><span class="line">    for (Class clazz : collectorClasses) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        if (!MapOutputCollector.class.isAssignableFrom(clazz)) &#123;</span><br><span class="line">          throw new IOException(&quot;Invalid output collector class: &quot; + clazz.getName() +</span><br><span class="line">            &quot; (does not implement MapOutputCollector)&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        Class&lt;? extends MapOutputCollector&gt; subclazz =</span><br><span class="line">          clazz.asSubclass(MapOutputCollector.class);</span><br><span class="line">        LOG.debug(&quot;Trying map output collector class: &quot; + subclazz.getName());</span><br><span class="line">        MapOutputCollector&lt;KEY, VALUE&gt; collector =</span><br><span class="line">          ReflectionUtils.newInstance(subclazz, job);</span><br><span class="line">        collector.init(context);</span><br><span class="line">        LOG.info(&quot;Map output collector class = &quot; + collector.getClass().getName());</span><br><span class="line">        return collector;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>最后知道了,输出数据的排序收集器就是唤醒缓冲区MapOutputBuffer的子类,打开他的init方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">public void init(MapOutputCollector.Context context</span><br><span class="line">                    ) throws IOException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      ...</span><br><span class="line">      //sanity checks</span><br><span class="line">      final float spillper =</span><br><span class="line">        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);</span><br><span class="line">      final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);     </span><br><span class="line">      ...</span><br><span class="line">      sorter = ReflectionUtils.newInstance(job.getClass(&quot;map.sort.class&quot;,</span><br><span class="line">            QuickSort.class, IndexedSorter.class), job);</span><br><span class="line"></span><br><span class="line">      ...</span><br><span class="line">      // k/v serialization</span><br><span class="line">      comparator = job.getOutputKeyComparator();</span><br><span class="line">      keyClass = (Class&lt;K&gt;)job.getMapOutputKeyClass();</span><br><span class="line">      valClass = (Class&lt;V&gt;)job.getMapOutputValueClass();</span><br><span class="line">      serializationFactory = new SerializationFactory(job);</span><br><span class="line">      keySerializer = serializationFactory.getSerializer(keyClass);</span><br><span class="line">      keySerializer.open(bb);</span><br><span class="line">      valSerializer = serializationFactory.getSerializer(valClass);</span><br><span class="line">      valSerializer.open(bb);</span><br><span class="line"></span><br><span class="line">      ...</span><br><span class="line">      // compression</span><br><span class="line">      if (job.getCompressMapOutput()) &#123;</span><br><span class="line">        Class&lt;? extends CompressionCodec&gt; codecClass =</span><br><span class="line">          job.getMapOutputCompressorClass(DefaultCodec.class);</span><br><span class="line">        codec = ReflectionUtils.newInstance(codecClass, job);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        codec = null;</span><br><span class="line">      &#125;</span><br><span class="line">      ...</span><br><span class="line">      if (combinerRunner != null) &#123;</span><br><span class="line">        final Counters.Counter combineOutputCounter =</span><br><span class="line">          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);</span><br><span class="line">        combineCollector= new CombineOutputCollector&lt;K,V&gt;(combineOutputCounter, reporter, job);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        combineCollector = null;</span><br><span class="line">      &#125;</span><br><span class="line">      ...</span><br><span class="line">      spillThread.setDaemon(true);</span><br><span class="line">      spillThread.setName(&quot;SpillThread&quot;);</span><br><span class="line">      spillLock.lock();</span><br><span class="line">      try &#123;</span><br><span class="line">        spillThread.start();</span><br><span class="line">        while (!spillThreadRunning) &#123;</span><br><span class="line">          spillDone.await();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (InterruptedException e) &#123;</span><br><span class="line">        throw new IOException(&quot;Spill thread failed to initialize&quot;, e);</span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        spillLock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">      if (sortSpillException != null) &#123;</span><br><span class="line">        throw new IOException(&quot;Spill thread failed to initialize&quot;,</span><br><span class="line">            sortSpillException);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面的粗略的的分组,代码分别是</p>
<ul>
<li>设置缓冲取大小和溢写百分比(默认100M和0.8)</li>
<li>设置缓冲区数据的排序类(默认快速排序)</li>
<li>获取排序比较器(优先获取设置的比较类,没有的话取Writable类型默认的比较器)</li>
<li>将keyvalue键值对序列化</li>
<li>判断是否启用combiner,如果溢写的小文件数量超过3,则启用combiner合并</li>
<li>获取压缩对象</li>
<li>开启溢写线程</li>
</ul>
<p>这里多嘴几句,因为篇幅有限,先写下buffer的一些特性,以后可以在这个类的源码中验证:</p>
<ul>
<li>buffer本质还是字节数组</li>
<li>buffer有赤道的概念,即分界点,一边输入数据,一边输入索引</li>
<li>索引:固定宽度:16字节,4个int(partition,keystart,valuestart,valuelenth)</li>
<li>combiner默认发生在溢写之前,排序之后</li>
</ul>
<h4 id="reduce端源码"><a href="#reduce端源码" class="headerlink" title="reduce端源码"></a>reduce端源码</h4><p>我们先看Reducer类的注释,头注释翻译过来大概意思就是<br/><br>reducer主要做两件事,一件是拉取shuffle的数据,一件是对数据进行sort,这里的排序不是对数据进行在排序,因为map已经对数据进行过排序了,这里是对map排序过的数据文件进行归并.<br>好了要点说完了,我们直接看ReudceTask的run方法<br>其中有一句代码是这样的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">    RawKeyValueIterator rIter = null;</span><br><span class="line">    ShuffleConsumerPlugin shuffleConsumerPlugin = null;</span><br><span class="line">    shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line">    ShuffleConsumerPlugin.Context shuffleContext =</span><br><span class="line">      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical,</span><br><span class="line">                  super.lDirAlloc, reporter, codec,</span><br><span class="line">                  combinerClass, combineCollector,</span><br><span class="line">                  spilledRecordsCounter, reduceCombineInputCounter,</span><br><span class="line">                  shuffledMapsCounter,</span><br><span class="line">                  reduceShuffleBytes, failedShuffleCounter,</span><br><span class="line">                  mergedMapOutputsCounter,</span><br><span class="line">                  taskStatus, copyPhase, sortPhase, this,</span><br><span class="line">                  mapOutputFile, localMapFiles);</span><br><span class="line">    shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line"></span><br><span class="line">rIter = shuffleConsumerPlugin.run();</span><br></pre></td></tr></table></figure>
<p>最后一句,通过shfulle插件获取迭代器,我们知道基本reduce的数据都是通过迭代器获取的<br/></p>
<h4 id="迭代器的使用"><a href="#迭代器的使用" class="headerlink" title="迭代器的使用"></a>迭代器的使用</h4><p>为什么使用迭代器呢?因为我不可能将数据一次性的装进内存里,最好是通过迭代器维护一个对文件的指针,这样不仅遍历和实现分离,而且谁想要读取这个文件只要生成一个迭代器,维护自己的指针就可以,不会出现强指针或者同一个数据文件占多份内存的情况.<br>然后我们追踪Reducer的的迭代器类,追踪路径如下<br>context.getValues().iterator();  -&gt; ReudceContext -&gt; ReduceContextImpl<br>进入reduce实现类之后最后有一个getValues方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public</span><br><span class="line">Iterable&lt;VALUEIN&gt; getValues() throws IOException, InterruptedException &#123;</span><br><span class="line">  return iterable;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>他返回一个Iterable对象,而Iterable只有一个方法,返回iterator对象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private ValueIterator iterator = new ValueIterator();</span><br><span class="line"> @Override</span><br><span class="line">    public Iterator&lt;VALUEIN&gt; iterator() &#123;</span><br><span class="line">      return iterator;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>而iterator绝对有hasNext对象和next方法(迭代器模式常识)<br>我们看一下他的hasNext方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">   public boolean hasNext() &#123;</span><br><span class="line">     try &#123;</span><br><span class="line">       if (inReset &amp;&amp; backupStore.hasNext()) &#123;</span><br><span class="line">         return true;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125; catch (Exception e) &#123;</span><br><span class="line">       e.printStackTrace();</span><br><span class="line">       throw new RuntimeException(&quot;hasNext failed&quot;, e);</span><br><span class="line">     &#125;</span><br><span class="line">     return firstValue || nextKeyIsSame;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后有一个boolean值nextKeyIsSame,我们先记住它然后我们看ReducerContextImpl的另一个方法nextkey方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public boolean nextKey() throws IOException,InterruptedException &#123;</span><br><span class="line">    while (hasMore &amp;&amp; nextKeyIsSame) &#123;</span><br><span class="line">      nextKeyValue();</span><br><span class="line">    &#125;</span><br><span class="line">    if (hasMore) &#123;</span><br><span class="line">      if (inputKeyCounter != null) &#123;</span><br><span class="line">        inputKeyCounter.increment(1);</span><br><span class="line">      &#125;</span><br><span class="line">      return nextKeyValue();</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return false;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>最后nextKey方法会调用nextkeyValue,这里只给出nextKeyValue的最后一句代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0,</span><br><span class="line">                                     currentRawKey.getLength(),</span><br><span class="line">                                     nextKey.getData(),</span><br><span class="line">                                     nextKey.getPosition(),</span><br><span class="line">                                     nextKey.getLength() - nextKey.getPosition()</span><br><span class="line">                                         ) == 0;</span><br></pre></td></tr></table></figure>
<p>他会他通过判断器进行判断,下一个key是否和现在的key相等,把结果值赋值给nextKeyIsSame,对就是刚刚记住的nextKeyIsSame.<br>也就是说,我们调用的values.hasNext方法,会判断nextKeyIsSame,下一个key是否相同,不同则返回false,触发reducor再次重新调用reduce方法.</p>
<h4 id="比较器的使用"><a href="#比较器的使用" class="headerlink" title="比较器的使用"></a>比较器的使用</h4><p>我们再看一下,ReducerTask的run方法,这里给出关键点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">RawComparator comparator = job.getOutputValueGroupingComparator();</span><br><span class="line"></span><br><span class="line">public RawComparator getOutputValueGroupingComparator() &#123;</span><br><span class="line">    Class&lt;? extends RawComparator&gt; theClass = getClass(</span><br><span class="line">      JobContext.GROUP_COMPARATOR_CLASS, null, RawComparator.class);</span><br><span class="line">    if (theClass == null) &#123;</span><br><span class="line">      return getOutputKeyComparator();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return ReflectionUtils.newInstance(theClass, this);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这个是获取分组比较器的方法,优先使用用户的分组比较器,如果用户的分组比较器为null,则使用默认的key的Writable类型包含的比较器.<br>而reduce也有排序比较器通过getOutputKeyComparator()获取,<br>再加上map的排序比较器,我们有三个比较器可以自定义也可取默认的比较器,mapreduce给了我们很灵活的选择取加工数据.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/19/mapreduce%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/19/mapreduce%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">mapreduce&yarn笔记总结 01</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2020-08-19 21:14:32 / Modified: 16:22:18" itemprop="dateCreated datePublished" datetime="2020-08-19T21:14:32+08:00">2020-08-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="背景"><a href="#背景" class="headerlink" title="背景,"></a>背景,</h4><p>为了体现分布式计算的优点,将数据分而治之再进行相应方面的计算.hadoop提出了mapreduce计算模型</p>
<h4 id="计算模型"><a href="#计算模型" class="headerlink" title="计算模型"></a>计算模型</h4><p><img src="http://picture.lemcoden.xyz/mapreduce/mr_mod.png" alt="计算模型"></p>
<ul>
<li>map 端负责将拆分出来的数据进行映射,变换,过滤.体现在一进N出</li>
<li>reduce 端负责将数据整合归纳,缩小,分解,一般是一组数据进N出</li>
<li>不管是map还是reduce处理的数据结构基本都是&lt;key,value&gt;的形式划分的</li>
<li>最基本的数据格式确定后,会有数据迁移更加细致的流程</li>
<li>首先分布式计算是基于分布式文件系统的,而分布式文件系统的存储模型以块为单位,所以分布式的物理模型以split(分片)为的单位</li>
<li>默认每个split对应一个map进程</li>
<li>split的数据对应map计算之后并不会直接写入磁盘而是先写入环形缓冲区 || 因为每一次IO都会调用linux内核,所以不是一条记录IO一次,而是缓冲区写满后进行一次性IO</li>
<li>跳过中间阶段,看reduce,reduce会根据之前数据的partion数量对应开启reduce进程.</li>
<li>默认一个reduce进程对应一个partition,再次体现分而治之的理念</li>
<li>map段的数据经过buffer之后会为reduce分区作准备,所以会先进行分区,对key进行取模操作划分出partition,会将数据结构转换成&lt;key,value,partition&gt;的形式.</li>
<li>进行partition之后,为减少reduce的拉取IO操作(总不能一条数据拉取查找一次吧),会将partition按照进行再次分片(split).</li>
<li>数据进行分片之后,再按照partition进行小文件排序(sort),同时还会进行key的第二次排序,关于为什么还会进行key的排序,到reduce端会解释<h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h4><h4 id="遥远的hadoop1-x"><a href="#遥远的hadoop1-x" class="headerlink" title="遥远的hadoop1.x"></a>遥远的hadoop1.x</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(ps:因为找不到合适的图,就分开为client端和job端的架构)</span><br></pre></td></tr></table></figure>
计算模型出现后,就需要搭建整体的框架,首先我们说我们的主要角色有client,JobTracker,TaskTracker<br><img src="http://picture.lemcoden.xyz/mapreduce/mr_arch_client.jpg" alt="clinet架构"></li>
</ul>
<p>我们client端主要做四件事:</p>
<ul>
<li>会根据每次的计算数据,咨询NN元数据(block) =&gt; 算出spilt切片清单</li>
<li>生成计算程序未来运行相关的配置文件</li>
<li>将jar包,split的切片清单,配置文件上传到HDFS目录当中</li>
<li>cli调用jobTracker,通知启动一个计算程序,并且告知文件都放在了hdfs的哪些地方</li>
</ul>
<p><img src="http://picture.lemcoden.xyz/mapreduce/mr_arch_job.jpg" alt="job架构"></p>
<ul>
<li>jobTracker会根据cli提交的信息,去HDFS上寻找Jar包程序,split清单,以及配置文件</li>
<li>根据拿到的切片清单和配置文件,以及收到的TaskTracker汇报的资源,最终确定每一个spilt应该去往哪个个节点</li>
<li>TaskTracker会在汇报心跳的时候拿到分配给自己的人物信息</li>
<li>TaskTrakcer取回任务后会从hdfs下载jar包,xml配置文件到本机</li>
<li>TaskTraker会根据xml配置文件以及JobTrakcer的任务描述,从jar包中抽取出mapreduce任务运行</li>
</ul>
<p>这个是hadoop1.x的mapreduce的任务调度,到了hadoop2.x的时候这种架构被重新修改,why?</p>
<ol>
<li>任务调度框架jobTracker和TaskTracker使用的是主从架构,那必将出现两个问题,一个是单点故障问题</li>
<li>另一个则是主节点压力过大的问题</li>
<li>JobTracker同时负责资源的调度以及计算任务管理,两者耦合,如果引入新的计算框架则不能复用资源管理<h4 id="改进后的Hadoop2-x"><a href="#改进后的Hadoop2-x" class="headerlink" title="改进后的Hadoop2.x"></a>改进后的Hadoop2.x</h4>hadoop2.x后将JobTraker的资源调度功能抽出,封装为Yarn资源管理框架,并配置了高可用.<br>hadoop2.x的计算与资源管理架构如下<br><img src="http://picture.lemcoden.xyz/mapreduce/mr_arch_yarn.png" alt="job架构"><br>主要角色有client,ResourceManager,NodeManager,ApplicatioMaster以及Container</li>
</ol>
<ul>
<li>client与之前的流程一致</li>
<li>client会将job提交到ResourceManager</li>
<li>ResourceManger接收到job请求后,会在集群当中挑一台不忙的节点,在NodeManager中启动一个ApplicationMaster进程</li>
<li>ApplicationMaster进程启动之后,会去HDFS下载Splite清单以及配置文件,并将配置清单发送ResouceManager,申请Container</li>
<li>ResouceManager会根据清单计算出使用多少资源,并将根据现有资源通知NodeManager启动相应的Container容器</li>
<li>Container向App Mstr(Application Master)反向注册,此时App Mstr才知道有多少Container可以执行任务</li>
<li>App Mstr会向Container发送Map Task消息.</li>
<li>Container受到消息后,会从hdfs下载jar包,并通过反射取出对象执行MapReduce任务<br>相较于Hadoop1.x,2.x的框架很好的解决的1.x框架出现的问题:</li>
</ul>
<ol>
<li>单点故障<br/></li>
</ol>
<ul>
<li>App Mstr由ResouceManager监控管理,所以当App Mstr没有心跳时,RM(Resource Manager)会触发失败重试机制,ResourceManager会在其他节点重新启动个App Mstr</li>
<li>ResourceManager本身可以配置高可用</li>
<li>Container 也会有失败重试</li>
</ul>
<ol start="2">
<li>压力过大问题</li>
</ol>
<ul>
<li>yarn中每个计算程序自有一个AppMaster,每个AppMaster之负责自己计算程序的任务调度.</li>
<li>AppMasters是在不同的节点中启动的,默认有了负载的光环</li>
</ul>
<ol start="3">
<li>资源管理与任务调度耦合</li>
</ol>
<ul>
<li>yarn只负责资源管理,不负责具体的任务调度</li>
<li>yarn作为资源管理框架可以被其他计算程序复用(只需要继承AppMaster类就可以)</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/19/hadoop%E9%9B%86%E7%BE%A4HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/19/hadoop%E9%9B%86%E7%BE%A4HA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">hadoop集群HA高可用配置总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-19 16:27:16" itemprop="dateCreated datePublished" datetime="2020-08-19T16:27:16+08:00">2020-08-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-09 10:58:23" itemprop="dateModified" datetime="2020-11-09T10:58:23+08:00">2020-11-09</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="基础设施"><a href="#基础设施" class="headerlink" title="基础设施"></a>基础设施</h3><ul>
<li>网卡静态IP<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ifconfig 查看网卡信息</span><br><span class="line">vim /etc/udev/rules.d/70-persistent-ipoib.rules</span><br><span class="line">              ACTION==&quot;add&quot;, SUBSYSTEM==&quot;net&quot;, DRIVERS==&quot;?*&quot;, ATTR&#123;type&#125;==&quot;32&quot;, ATTR&#123;address&#125;==&quot;?*00:02:c9:03:00:31:78:f2&quot;, NAME=&quot;网卡名&quot;</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-网卡名</span><br><span class="line">POXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static  //设置静态IP</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">NAME=enp0s3</span><br><span class="line">UUID=290c55a8-1b88-4d99-b741-dcfe455f5c2c</span><br><span class="line">DEVICE=enp0s3</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.0.101  //一般本地IP最后依次增加</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATWAY=192.168.0.1 //同一集群必须同一网关</span><br></pre></td></tr></table></figure></li>
<li>设置hosts<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line">192.168.0.101 hadoop01</span><br><span class="line">192.168.0.101 hadoop02</span><br></pre></td></tr></table></figure></li>
<li>关闭防火墙<br/><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Centos6.x</span><br><span class="line">service iptables stop</span><br><span class="line">service iptables status</span><br><span class="line">chkconfig iptables off</span><br><span class="line">Centos7.x</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl status firewalld.service</span><br><span class="line">systemctl disable firewalld.service</span><br></pre></td></tr></table></figure></li>
<li>关闭 selinux<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure></li>
<li>作时间同步<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">yum install ntp  -y</span><br><span class="line">vi /etc/ntp.conf</span><br><span class="line">主节点</span><br><span class="line">          注释掉其他server</span><br><span class="line">          server ntp1.aliyun.com</span><br><span class="line">          server 127.127.1.0</span><br><span class="line">          fudge 127.127.1.0 stratum 10</span><br><span class="line"> 从节点</span><br><span class="line">          server 192.168.0.101</span><br><span class="line">从节点设置crontab同步</span><br><span class="line">vim /etc/crontab</span><br><span class="line">10 20 * * * /usr/sbin/ntpdate -u 192.168.0.101</span><br><span class="line">systemctl start ntpd</span><br><span class="line">systemctl enable ntpd</span><br></pre></td></tr></table></figure></li>
<li>安装JDK<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rpm -i   jdk-8u181-linux-x64.rpm</span><br><span class="line">		*有一些软件只认：/usr/java/default</span><br><span class="line">vi /etc/profile     </span><br><span class="line">	  export  JAVA_HOME=/usr/java/default</span><br><span class="line">		export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">source /etc/profile   |  .    /etc/profile</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>免密登陆<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa  //一路回车</span><br><span class="line">ssh-copy-id 其他节点</span><br></pre></td></tr></table></figure>
<h3 id="集群安装配置"><a href="#集群安装配置" class="headerlink" title="集群安装配置"></a>集群安装配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf tar包 -C 目录</span><br></pre></td></tr></table></figure>
<h4 id="java配置"><a href="#java配置" class="headerlink" title="java配置"></a>java配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rpm -i   jdk-8u181-linux-x64.rpm</span><br><span class="line">		*有一些软件只认：/usr/java/default</span><br><span class="line">vi /etc/profile     </span><br><span class="line">		export  JAVA_HOME=/usr/java/default</span><br><span class="line">		export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="hadooop-配置"><a href="#hadooop-配置" class="headerlink" title="hadooop 配置"></a>hadooop 配置</h4><table>
<thead>
<tr>
<th></th>
<th>NN</th>
<th>NN</th>
<th>JN</th>
<th>ZKFC</th>
<th>ZK</th>
<th>DN</th>
<th>RM</th>
<th>NM</th>
</tr>
</thead>
<tbody><tr>
<td>node01</td>
<td>*</td>
<td></td>
<td>*</td>
<td>*</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>node02</td>
<td></td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
<td></td>
<td>*</td>
</tr>
<tr>
<td>node03</td>
<td></td>
<td></td>
<td>*</td>
<td></td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
</tr>
<tr>
<td>node04</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>*</td>
<td>*</td>
<td>*</td>
<td>*</td>
</tr>
</tbody></table>
<p>hadoop配置的七个文件:<br/><br>core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml <br/><br>hadoop-env.sh mapred-env.sh slaves <br/><br>core-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;hdfs://mycluster&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		 &lt;property&gt;</span><br><span class="line">		   &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">		   &lt;value&gt;node02:2181,node03:2181,node04:2181&lt;/value&gt;</span><br><span class="line">		 &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>hdfs-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># 以下是  一对多，逻辑到物理节点的映射</span><br><span class="line">      &lt;property&gt;</span><br><span class="line">				&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">				&lt;value&gt;1&lt;/value&gt;</span><br><span class="line">			&lt;/property&gt;</span><br><span class="line">			&lt;property&gt;</span><br><span class="line">				&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">				&lt;value&gt;/var/bigdata/hadoop/local/dfs/name&lt;/value&gt;</span><br><span class="line">			&lt;/property&gt;</span><br><span class="line">			&lt;property&gt;</span><br><span class="line">				&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">				&lt;value&gt;/var/bigdata/hadoop/local/dfs/data&lt;/value&gt;</span><br><span class="line">			&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;mycluster&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node01:8020&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node02:8020&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node01:50070&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;node02:50070&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		#以下是JN在哪里启动，数据存那个磁盘</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;qjournal://node01:8485;node02:8485;node03:8485/mycluster&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;/var/bigdata/hadoop/ha/dfs/jn&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		#HA角色切换的代理类和实现方法，我们用的ssh免密</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line">		&lt;property&gt;</span><br><span class="line">		  &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">		  &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt;</span><br><span class="line">		&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">		#开启自动化： 启动zkfc</span><br><span class="line">		 &lt;property&gt;</span><br><span class="line">		   &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">		   &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">		 &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">      &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;node02:2181,node03:2181,node04:2181&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;lemcoden_yarn_cluster&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;node03&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;node04&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>hadoop-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/default</span><br></pre></td></tr></table></figure>
<p>mapred-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/default</span><br></pre></td></tr></table></figure>
<p>slaves</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node02</span><br><span class="line">node03</span><br><span class="line">node04</span><br></pre></td></tr></table></figure>
<p>设置环境变量 <br/></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">		export  JAVA_HOME=/usr/java/default</span><br><span class="line">		export HADOOP_HOME=/opt/bigdata/hadoop-2.6.5</span><br><span class="line">		export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>软件包分发:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">			scp -r ./bigdata/  node02:`pwd`</span><br><span class="line">			scp -r ./bigdata/  node03:`pwd`</span><br><span class="line">			scp -r ./bigdata/  node04:`pwd`</span><br></pre></td></tr></table></figure>

<h4 id="zookeeper配置"><a href="#zookeeper配置" class="headerlink" title="zookeeper配置:"></a>zookeeper配置:</h4><p>cp zoo_sanmple.cfg zoo.cfg <br/><br>zoo.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   datadir=/var/bigdata/hadoop/zk</span><br><span class="line">server.1=node02:2888:3888</span><br><span class="line">server.2=node03:2888:3888</span><br><span class="line">server.3=node04:2888:3888</span><br></pre></td></tr></table></figure>
<p>mkdir /var/bigdata/hadoop/zk<br>配置环境变量:<br>node02</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">				export ZOOKEEPER_HOME=/opt/bigdata/zookeeper-3.4.6</span><br><span class="line">				export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin</span><br><span class="line">. /etc/profile</span><br></pre></td></tr></table></figure>
<p>分发软件包:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r ./zookeeper-3.4.6  node03:`pwd`</span><br><span class="line">scp -r ./zookeeper-3.4.6  node04:`pwd`</span><br></pre></td></tr></table></figure>
<p>设置myid:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">node03:</span><br><span class="line">			mkdir /var/bigdata/hadoop/zk</span><br><span class="line">			echo 2 &gt;  /var/bigdata/hadoop/zk/myid</span><br><span class="line">			*环境变量</span><br><span class="line">			. /etc/profile</span><br><span class="line">node04:</span><br><span class="line">			mkdir /var/bigdata/hadoop/zk</span><br><span class="line">			echo 3 &gt;  /var/bigdata/hadoop/zk/myid</span><br><span class="line">			*环境变量</span><br><span class="line">			. /etc/profile</span><br></pre></td></tr></table></figure>
<h3 id="集群初始化启动"><a href="#集群初始化启动" class="headerlink" title="集群初始化启动"></a>集群初始化启动</h3><p>先启动JN (node1~node3)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure>
<p>选择一个NN格式化(只在初始化时做一次)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
<p>在另一台机器里同步元数据(先启动node1的namenode)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>
<p>格式化zk(也是只做一次)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc  -formatZK</span><br></pre></td></tr></table></figure>
<p>启动集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br><span class="line">node03-04:</span><br><span class="line">  yarn-daemon.sh start recourcemanager</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/16/hdfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/16/hdfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">hdfs文件系统笔记总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-16 21:03:36" itemprop="dateCreated datePublished" datetime="2020-08-16T21:03:36+08:00">2020-08-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-08-17 13:31:32" itemprop="dateModified" datetime="2020-08-17T13:31:32+08:00">2020-08-17</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>关于HDFS原理在此写个总结<br>前三点主要围绕分布式文件系统那么多,为什么apache还要开发自己的文件系统<br>后两点主要围绕hdfs的高可用问题</p>
<h3 id="1-HDFS存储模型"><a href="#1-HDFS存储模型" class="headerlink" title="1.HDFS存储模型"></a>1.HDFS存储模型</h3><ul>
<li><p>1.hdfs的存储模型第一个核心为block(块),hdfs的所有存储文件都是按照块来进行划分的,每个文件可以有不同的块,但是文件中除了最后一个块,每个块的大小必须相同,这个为了保证可以和hadoop计算框架,相适应能够有一个统一的计算单位,这个统一的计算单位block不是固定的,需要根据具体的I/O特性进行调整.</p>
</li>
<li><p>2.除了围绕块之外存储模型还有一个核心是存储副本(replication),副本可以冗余数据保证系统的可靠性.并多个副本存储在不同主机当中可以增加计算程序与数据在同一集群的概率,提升计算的性能.</p>
<h3 id="2-HDFS的角色"><a href="#2-HDFS的角色" class="headerlink" title="2.HDFS的角色"></a>2.HDFS的角色</h3><h4 id="HDFS主要角色"><a href="#HDFS主要角色" class="headerlink" title="HDFS主要角色"></a>HDFS主要角色</h4><p>主要角色有两个namenode和datanode,主要功能包括如下:</p>
</li>
<li><p>1.namenode主要维护文件的元数据</p>
</li>
<li><p>2.datanode主要维护负责block的读和写</p>
</li>
<li><p>3.datanode会与namenode维持心跳,并汇报自己持有的block信息和列表</p>
</li>
<li><p>4.clinet向Namenode交互文件元数据.和datanode交互文件blocks数据<br/></p>
</li>
</ul>
<h4 id="次要角色-SecodaryNamenode"><a href="#次要角色-SecodaryNamenode" class="headerlink" title="次要角色:SecodaryNamenode"></a>次要角色:SecodaryNamenode</h4><p>要聊SecondaryNamenode就需要先知道Namenode是如何持久化元数据文件的<br>首先,NameNode维护元数据是在内存中,如果机器突然宕掉,如果不把元数据写在磁盘上,那时没有办法恢复的,元数据会永久丢失<br/><br>  目前持久化有两种方式:</p>
<ul>
<li>1.通过操作日志文件(EditLog)恢复,也就是每当NameNode有一条对元数据的操作,就会增加一条日志,但是这样的方式有两个缺点,第一随着运行时间的增长,NN的log会变得及大,很浪费磁盘空间,第二点也是运行时间长的话,要恢复需要很长时间,比如这台机器运行了十年,宕掉之后可能机器需要五年的时间恢复.</li>
<li>2.通过内存的快照(fsImage)恢复,就是将某个时间点的内存状态溢写到磁盘上,但这种方式没办法实时的保存,磁盘IO是有瓶颈的,我们不可能隔一两分钟就保存内存的镜像<br>而NameNode保存选择了一个折衷的方式来规避两者明显的缺点:<br>每相隔一个时间点,将Editlog里面的操作写入到fsImage当中,合并成为一个新的fsImage,然后将在之前产生旧EditLog,只保留最近fsImage时间点之后产生EditLog删掉,这样让fsImage滚动更新的方法,使得占少量的磁盘的情况下,能让NN恢复到机器宕掉那一刻的状态.<br>但是如果把这个保存合并快照的工作全都的交给Namenode的话,此节点的压力会很大,所以关于fsImage的滚动合并工作便交给另一个工作节点SecondaryNamenode来执行<h3 id="3-角色交互产生的机制"><a href="#3-角色交互产生的机制" class="headerlink" title="3.角色交互产生的机制"></a>3.角色交互产生的机制</h3><h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4>每次NN启动的时候,都会将最近时间点的fsImage加载进来,然后将EditLog操作合并到系统内存当中,最后将新的fsImage写入,并删除EditLog<br>NN会文件的属性作为元数据,但不包括datanode列表,<br>主要是为了防止<br/><br>NN重启后,DN(DataNode)列表中,有启动不起来DN,此时正好有客户端请求,NN返回了不可靠的DN列表<br/><br>这样的情况发生<br>所以NN,在重启后,会接受所有DN心跳信号和状态报告.<br>当确定NameNode检测某个数据块的副本达到最小值,那么他会被认为副本安全的,当有一定百分比的数据被确认是安全后,NameNode将会退出安全模式.<h4 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h4>第一个副本放置在本机,第二个副本放在不同机架,第三副本放置在同一机架,之后的副本随机.<br>原因:</li>
<li>为了能在程序计算的时候找到最近的block数据</li>
<li>为了能在机架损毁的时候仍然能保留其完整的<h4 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h4><img src="http://picture.lemcoden.xyz/hdfs/hdfs_write.png" alt="读流程"></li>
<li>Client与NN创建连接</li>
<li>NN建立元数据</li>
<li>NN验证元数据是否合理</li>
<li>NN触发副本放置策略,返回DN列表</li>
<li>Clinet将数据块分割为64K数据,并使用chunk(512B)和sumchunk进行填充</li>
<li>Clinet向DataNode发送数据块,第一个DN受到packet后本地保存发送到第二个DN</li>
<li>第二个DN发送到第三个DN</li>
<li>当block传输完成,DN分别向各NN汇报,同时Client继续传输下一个block</li>
<li>所以client和block汇报也是并行的</li>
<li>数据分割是为了保证DN可以在第一个数据包发送完成之后,可以立刻发送给第二个DN,保证传输效率以及传输的一致性,并且这样传输,对客户端来说是透明,客户端只要保证给第一个DataNode传输完整数据就可以.<h4 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h4><img src="http://picture.lemcoden.xyz/hdfs/hdfs_read.png" alt="写流程"></li>
<li>为了降低整体的带宽消耗和读延迟，HDFS会尽量让读取程序读取离他最近的副本</li>
<li>如果再读取程序的同一个机架上有一个副本，那么就读该副本</li>
<li>如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本</li>
<li>语义：<br>　　 download a file<br>　　 Client和NN交互文件元数据获取fileBlockLocation<br>　　 NN按距离策略排序返回<br>　　 Client尝试下载Block并且校验数据完整性（校验盒校验）</li>
<li>语义：下载一个文件其实是获取文件的所有的Block元数据，那么子集获取block应该成立<br>　　Hdfs支持Client输出文件的offset自定义连接哪些Block的DN，自定义获取数据<br>　　这个是支持计算层的分治，并行计算的核心（牢记）</li>
</ul>
<h3 id="5-HDFS设置高可用"><a href="#5-HDFS设置高可用" class="headerlink" title="5.HDFS设置高可用"></a>5.HDFS设置高可用</h3><h4 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h4><p>为了提升NN的可靠性,如果集群当中只有一个NN,那么在某些情况下Namenode宕机,那么整个集群就不可用了,所以为了提升整个集群的可用性,我们设置两个NN,一主一备,确保主NN宕掉之后,备用NN能启动起来.</p>
<h4 id="开始实现"><a href="#开始实现" class="headerlink" title="开始实现"></a>开始实现</h4><p>要想让两个NN能够无缝切换,我们必须先实现两个进程的内存同步,有两种方法进行两个机器的进程同步:</p>
<ul>
<li>阻塞同步,就是需要同步数据的时候,主NN进入阻塞状态,等待备用NN同步完毕,然后继续运行,但是在现实生产生活中,这种方式显然是行不通的,我们保证了两个NN的强一致性,但是主NameNode的可用性却大大降低了,也就是Namenode同步数据的时候,我们无法使用这个NN.</li>
<li>非阻塞异步,就是主NN需要同步数据时直接发给备用NN,同时NN保持运行接受客户端请求,等待备用NN同步完毕回调通知主NN.但是这样NN的一致性就无法保证</li>
</ul>
<h4 id="CAP定理"><a href="#CAP定理" class="headerlink" title="CAP定理"></a>CAP定理</h4><p>说到这么不得不谈谈CAP定理,即一致性(Consistency),可用性(Avalible),分区容错性(Partition tolerance)三者只可满足其二</p>
<ul>
<li>分区容错性,当分布式系统中遇到任何网络分区故障时,仍然需要能够保证对外提供满足一致性和可用性的服务,除非整个网络发生故障,通俗一点说,我将数据副本设置到多个节点上,其中一个节点故障了,因为其他节点持有数据副本,仍能对外保持可用一致的服务,我称这个分布式系统具有分区容忍性.</li>
<li>可用性,是指系统提供的服务必须一直处于可用的状态,对于每一个从操作请求总是能够在有限的时间内返回结果.</li>
<li>一致性,指的是数据在多个副本之间能够保持一致的特性,在一致性的需求下,当一个系统在数据一致的状态执行更新操作后,应该保证系统为数据仍然处于一致的状态<h4 id="CAP套用分析"><a href="#CAP套用分析" class="headerlink" title="CAP套用分析"></a>CAP套用分析</h4>我们套用CAP定理再回顾一下之前NN的同步的两种方法,</li>
<li>两个方法都满足分区容忍性,</li>
<li>第一个阻塞同步方法,会影响可用性,导致系统没办法在系统规定时间内返回正确的响应结果,</li>
<li>第二个异步非阻塞方法,没有办法保证一致性,异步回调没有办法保证备用的NN能完全将数据保持同步一致.<h4 id="折衷办法-最终一致性"><a href="#折衷办法-最终一致性" class="headerlink" title="折衷办法:最终一致性"></a>折衷办法:最终一致性</h4>既然CAP没办法全部满足,那么我们能不能选择一个折衷的方法呢?<br>当然有,NN就是使用这种方法,即通过Poxas选举算法保持数据的最终一致性<br>首先,添加一个角色较jounralNode,当NN的命名空间有任何修改时,会告知大部分的JounralNodes进程,standby状态的NN会读取JNs中的信息,并监控EditLog的变化,把变化与应用于自己的命名空间.<br/><br>那如何保证数据的一致性呢?<br>JounralNode一般会大于等于3的基数个,首先JounralNode根据少数服从多数的原则,选择出其中的Leader,Leader只有一个,它负责记录自己和其他的JounralNode是否接受到NameNode的变更信息,超过半数的JounralNode接收到NN的变更信息时,才承认作数据同步是可靠的.通知备用的NN读取JounralNode的信息.这样我们即保证了可用性,也保证了数据同步的最终一致性(弱一致性)<h4 id="zookeeper分布式协调系统自动切换"><a href="#zookeeper分布式协调系统自动切换" class="headerlink" title="zookeeper分布式协调系统自动切换"></a>zookeeper分布式协调系统自动切换</h4>上面我们解决的NN之间的数据同步问题,但是现实是active的NN出现故障的时候,我们只能手动其切换NN,所以我们还需要zookeeper的帮助.<h4 id="新增角色ZK-zookeeper-和ZKFC-ZookeeperFailOver"><a href="#新增角色ZK-zookeeper-和ZKFC-ZookeeperFailOver" class="headerlink" title="新增角色ZK(zookeeper)和ZKFC(ZookeeperFailOver)"></a>新增角色ZK(zookeeper)和ZKFC(ZookeeperFailOver)</h4>当我们启动zookeeper进程的时候,会有两个进程监控我们的NN,一个是zookeeper本身的进程,和JounralNode有着相似的选举算法,也是进程数必须超过zookeeper的奇数倍,zkfc要与zk进程保持心跳,而zkfc主要负责监控和切换主备的Namenode<h4 id="自动主备切换流程详述"><a href="#自动主备切换流程详述" class="headerlink" title="自动主备切换流程详述"></a>自动主备切换流程详述</h4><img src="http://picture.lemcoden.xyz/hdfs/HA" alt="主备切换"></li>
<li>启动两个NN进程,此时两个NN都处于备用的状态</li>
<li>启动zk进程和zkfc进程,当zk进程启动之后,zkfc进程争相在最近的zk进程上建立节点(抢锁),第一个成功建立节点的zkfc进程会将它操控的NN设置为Active,另一个就会被定义为standby</li>
<li>假如zkfc挂掉了,zk进程监控不到zkfc的心跳,会将在zkfc建立的节点删掉,而监控standby的zkfc进程看到zk上的节点消失了,会主动建立节点,并先将active的NN节点降级为standby,自己监控的NN节点升级为active</li>
<li>假如active的NN挂掉了,zkfc进程检测到会删除在zk上建立的节点,而监控standby的zkfc会在zk上建立节点,并测试连接另一个NN是否宕掉,最后将自己监控的NN升级为Active</li>
<li>有一种特殊情况是,Active的NN可以运行可以和DataNode连接,但就是连接不上其他人主机的zkfc,当这个NameNode挂掉的时候,其他zkfc会一直处于阻塞状态,不断的尝试连接挂掉的NN,此时需要检查一下网卡硬件的问题</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/13/%E7%8E%A9%E8%BD%ACunbuntu-%E3%80%87%E5%A3%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/%E7%8E%A9%E8%BD%ACunbuntu-%E3%80%87%E5%A3%B9/" class="post-title-link" itemprop="url">玩转unbuntu 〇壹</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 20:00:37" itemprop="dateCreated datePublished" datetime="2020-08-13T20:00:37+08:00">2020-08-13</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-19 11:42:36" itemprop="dateModified" datetime="2020-11-19T11:42:36+08:00">2020-11-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="终端-gt-文件管理器"><a href="#终端-gt-文件管理器" class="headerlink" title="终端 =&gt; 文件管理器"></a>终端 =&gt; 文件管理器</h4><p>在终端输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nautilus ./</span><br></pre></td></tr></table></figure>

<h4 id="pc与手机链接-GSConnect方式"><a href="#pc与手机链接-GSConnect方式" class="headerlink" title="pc与手机链接(GSConnect方式)"></a>pc与手机链接(GSConnect方式)</h4><p>主要是因为linux版QQ都是bug,linux也没有微信所以只能通过GSConnect链接手机</p>
<p>来相互传送文件.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/08/13/%E7%8E%A9%E8%BD%ACunbuntu-%E3%80%87%E5%A3%B9/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/06/28/blog%E4%BC%98%E5%8C%96-%E5%9B%BE%E5%BA%8A%E9%80%89%E6%8B%A9-%E5%9B%BE%E7%89%87%E5%8A%A0%E6%B0%B4%E5%8D%B0-%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/28/blog%E4%BC%98%E5%8C%96-%E5%9B%BE%E5%BA%8A%E9%80%89%E6%8B%A9-%E5%9B%BE%E7%89%87%E5%8A%A0%E6%B0%B4%E5%8D%B0-%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/" class="post-title-link" itemprop="url">blog优化:图床选择&图片加水印&一些问题的解决</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-06-28 09:53:16" itemprop="dateCreated datePublished" datetime="2020-06-28T09:53:16+08:00">2020-06-28</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-11-09 22:43:50" itemprop="dateModified" datetime="2020-11-09T22:43:50+08:00">2020-11-09</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BB%BA%E7%AB%99/" itemprop="url" rel="index"><span itemprop="name">建站</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>前情提要:<br><a target="_blank" rel="noopener" href="https://lemcoden.xyz/2020/05/20/hexo优化-github-coding双搭建，域名CNAME别称指向/">hexo优化:github+coding双搭建，域名CNAME别称指向</a></p>
<h3 id="关于域名备案"><a href="#关于域名备案" class="headerlink" title="关于域名备案"></a>关于域名备案</h3><p>首先向各位读者道歉,之前向大家推荐了Godaddy的域名注册,笔者发现注册完成之后并不是非常好用,官网难以打开,客服反映慢,并且也不提供备案服务</p>
<p>如果大家像笔者之前的那样注册了Godady的域名,请直接去阿里&amp;腾讯云社区,搜索域名转入,进行相关操作,<font color=00ff00>域名转入需要多交一年的域名租赁费用</font></p>
<p>如果申请国内阿里,腾讯云的,可以直接去备案,<font color=00ff00>备案需要有域名提供商的云服务器,并且需要填写身份信息,备案申请,快的话一个星期才能申请下来</font></p>
<h3 id="关于图床"><a href="#关于图床" class="headerlink" title="关于图床"></a>关于图床</h3><h4 id="怎么加入图片"><a href="#怎么加入图片" class="headerlink" title="怎么加入图片?"></a>怎么加入图片?</h4><p>关于怎么在hexo框架中加入图片,百度肯定有很多方法,笔者也尝试过,比如</p>
<ul>
<li>直接加到github的库当中,然后通过链接引用,但是这样速度很慢,多次打开都是图片坏掉的小图标</li>
<li>使用<em>hexo</em>-asset-<em>image</em>,作为非nodejs的程序员,安装之后,引用图片并没有显示,并且中间还报了各种依赖异常,本人不会解决,所以直接跳过</li>
<li>踩坑之后笔者决定选择用云存储做图床,考虑到成本问题选择七牛云,完全免费</li>
</ul>
<h4 id="七牛云"><a href="#七牛云" class="headerlink" title="七牛云"></a>七牛云</h4><p>关于七牛云的注册,登录,以及申请对象存储,这里笔者不再多赘述,看着官方文档说明都可以做到,这里主要聊一聊申请过程中需要注意的问题</p>
<ul>
<li>千万不要使用顶级域名绑定，可以在DNSpod域名解析那里添加一个子域名，比如这个</li>
</ul>
<p><img src="http://picture.lemcoden.xyz/blog_optimize/picture_sub_domain.png" alt="picture_sub_domain"></p>
<ul>
<li>上传图片之前记得添加前缀，多个路径用/隔开</li>
</ul>
<p><img src="http://picture.lemcoden.xyz/blog_optimize/pic_path.png" alt="pic_path"></p>
<ul>
<li>多图上传推荐用上传工具PicGo</li>
</ul>
<h4 id="PicGo的linux安装方法"><a href="#PicGo的linux安装方法" class="headerlink" title="PicGo的linux安装方法"></a>PicGo的linux安装方法</h4><p><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/PicGo">PicGo的github链接</a></p>
<p>windows和mac直接在release里面下载相应的exe和dmg就可以</p>
<p>linux推荐下载appImage后缀的安装包</p>
<p>下载完成后,操作如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp PicGo-2.3.0-beta.3.AppImage /usr/bin/PicGo</span><br><span class="line">chmod +x /usr/bin/PicGo</span><br><span class="line">PicGo</span><br></pre></td></tr></table></figure>

<p>然后直接命令输入PicGo运行就可以了</p>
<p>如果出现如下异常</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xclip not found</span><br></pre></td></tr></table></figure>

<p>控制台输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install xclip -y</span><br></pre></td></tr></table></figure>

<p>安装xclip即可</p>
<p>这里放一个PicGo的配置参考</p>
<p><img src="http://picture.lemcoden.xyz/blog_optimize/conf_pic_go.png" alt="conf_pic_go"></p>
<p>存储区域华南,华北那些在官方文档有对应的参考映射值</p>
<p><a target="_blank" rel="noopener" href="https://picgo.github.io/PicGo-Doc/zh/guide/config.html#%E4%B8%83%E7%89%9B%E5%9B%BE%E5%BA%8A">官方配置指南</a></p>
<h4 id="关于图片加水印"><a href="#关于图片加水印" class="headerlink" title="关于图片加水印"></a>关于图片加水印</h4><p>为了防止别人盗图,笔者特意写了一个加水印的脚本,逻辑很简单的一个shell脚本,基于image magic库对图片进行编辑,下面放出地址</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Lemcoden/blog.tools">https://github.com/Lemcoden/blog.tools</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/06/25/jvm%E7%9A%84%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%88%BD%E5%8F%A3%E8%AE%B2%E8%A7%A3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%AD%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BF%97%E7%A7%B0%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%89%E3%80%87%E8%82%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/25/jvm%E7%9A%84%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%88%BD%E5%8F%A3%E8%AE%B2%E8%A7%A3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%AD%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BF%97%E7%A7%B0%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%89%E3%80%87%E8%82%86/" class="post-title-link" itemprop="url"> jvm的轻量级爽口讲解--内存管理子系统（俗称垃圾回收）〇肆</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-06-25 19:06:25" itemprop="dateCreated datePublished" datetime="2020-06-25T19:06:25+08:00">2020-06-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-06-26 22:17:08" itemprop="dateModified" datetime="2020-06-26T22:17:08+08:00">2020-06-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/jvm%E8%99%9A%E6%8B%9F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">jvm虚拟机</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h4><p><a target="_blank" rel="noopener" href="https://lemcoden.xyz/2020/06/18/jvm的轻量级爽口讲解-内存管理子系统（俗称垃圾回收）〇叁/">jvm的轻量级爽口讲解–内存管理子系统（俗称垃圾回收）〇贰</a><br/></p>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">表示博客已经优化到博主比较满意的程度，图片加载问题已解决，jvm系列文章的封面也使用了自己设计的封面，首页菜单添加jvm和blog建站的专栏．虽然说的有点像枯燥的开发日志，但是能看到自己的东西越做越好真是由衷的发自内心的高兴，在这里博主祝大家端午节快乐，来口粽子，来口ｊｖｍ小菜（恩，没毛病）</span><br></pre></td></tr></table></figure>
<h4 id="上次没吃完的一口粽子之安全点不够用？"><a href="#上次没吃完的一口粽子之安全点不够用？" class="headerlink" title="上次没吃完的一口粽子之安全点不够用？"></a><font color=#3.5169E1>上次没吃完的一口粽子之安全点不够用？</font></h4><p>上次我们讲了，ｊｖｍ虚拟机一般是在并发标记回收时，通过设置安全点来实现用户线程的停顿，确切的说是主动式中断的安全点（设置一个轮询问标志，一旦发现论询标志为真时，就跑到附近的安全点去挂起．），但是遇到程序暂停执行的情况就不够用了，比如说线程的sleep和blocked状态，这个时候，他是无法响应程序的中断请求的．<br/></p>
<p>因此我们还需要另一种方式，来让线程中断，这就是<font color=#00ff00>安全区域</font>，其实完全就可以理解为被拉成线的安全点，当线程跑到安全区域时，会标识自己已进入，这样回收线程启动，就不会再管安全区域的对象，而在回收线程运行期间，安全区域的线程没有收到回收完毕的信号，是不会离开安全区域的，这就保证了安全区域的安全性（有点套娃）</p>
<h4 id="刚蒸好的新粽子之新老年代如何同时收集？"><a href="#刚蒸好的新粽子之新老年代如何同时收集？" class="headerlink" title="刚蒸好的新粽子之新老年代如何同时收集？"></a><font color=#3.5169E1>刚蒸好的新粽子之新老年代如何同时收集？</font></h4><p>之前我们介绍了＂经典＂（这里是新老年代分代，如果把到目前为止可以稳定使用定义为经典的话，需要再加上Ｇ１回收器）的回收器，都是新老年代分管，并进行组合使用的．<font color=#3.5169E1>但是那之后的新型回收器都是新老年代同时进行收集的，他们是如何做到呢？</font><br>让我们重新再回到理论阶段来解读<br/><br>我们已知的分代理论出自分带收集理论，当时有两个假说做支撑：<br/><br><font color=#00ff00>强分代假说</font>：绝大多数对象都是朝生夕灭的<br/><br><font color=#00ff00>弱分代价说</font>：熬过多次垃圾回收的对象，就越难以消亡．<br/><br>这和我们现在的<font color=#00ff00>垃圾回收流程</font>相对应:<br/></p>
<ol>
<li>开始生成的对象会放到eden区当中，等eden区满，触发一次<font color=#00ff00>minor GC（有时候也叫young GC）</font>，熬过第一次回收的对象会放到survivor区．</li>
<li>等suvivor区满，再进行一次minor GC，并计算幸存对象熬过ＧＣ的次数，将熬过多次（默认１５次，可用参数调节）幸存对象放到old区中．</li>
<li>老年代区域满了收集器触发Full GC，回收整个堆以及方法区内存．</li>
</ol>
<p>讲到这里大家就要问了，<font color=#3.5169E1>老年代区域满了为什么不直接回收老年代区域的内存？</font><br/><br>问的好!<del>话说完全是你自己自问自答啊！！！</del>这是因为可能会出现老年代引用新生代的对象，即出现跨代引用问题．<br>如果出现跨代引用，我们回收老年代的引用同时，势必要查询到引用到年轻代的对象，因此会连带年轻代的对象同时回收<br>，所以老一代的回收器都是old区域满了，进行一次<font color=#00ff00>FullGC</font>.<br/>（在这里多嘴一句，<font color=#00ff00>FullGC</font>很容易和<font  color=#00ff00> MajorGC</font>，<font color=#00ff00>OldGC</font>搞混，原因是CMS回收器之前是没有Old GC这个说法的，Old区满直接Full GC，目前只有CMS收集器能进行所谓的old GC,即只回收老年代的内存，所以CMS收集器出来之后，大家就old区满这个原因，混淆了old GC 和Full GC的说法，而Ｍajor GC更加说不清楚，各个资料各有个的说法）<br>所以就跨代引用问题，在这之后又出现了一个新的假说，叫做<br/></p>
<ul>
<li><font color=#00ff00>跨代引用假说</font>：  存在相互引用关系的两个对象，是应该倾向于同时生存和同时消亡．</li>
</ul>
<p>举个栗子，如果新生代存在跨代引用，那么回收的时候，老年代势必会使新生代对象得以存活，那么我们就不必在为了少量的跨带引用去回收老年代，我们只需要在新生代上建立一个全局的数据结构（叫做记忆集），当我们触发minor GC 的时候，只有包含跨带引用的小块内存里的对象才会被加入到GC Roots进行扫描回收．<br/><br>基于<font color=#00ff00>记忆集</font>我们也可以实现老年代的独立GC(CMS收集器).也可以实现新老年代的同时收集(G1收集器)<br/></p>
<h4 id="最后一口肉粽子之经典收集器的控制参数"><a href="#最后一口肉粽子之经典收集器的控制参数" class="headerlink" title="最后一口肉粽子之经典收集器的控制参数"></a><font color=#3.5169E1 >最后一口肉粽子之经典收集器的控制参数</font></h4><p>好的，新的理论假说已经聊清楚了，我们再回到之前讲的经典收集器．之前所介绍的经典收集器分为串行和并行？<del>不！你没讲过</del>（其实就是单核和多核并行）我们按收集器的顺序，再把收集器的调节参数讲一讲</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
<th>所属收集器</th>
</tr>
</thead>
<tbody><tr>
<td>-XX:SuvivorRatio</td>
<td>新生代中Eden区和Suvivor区域的容量比,默认数值为８，Eden:Survivor=8:1</td>
<td>Serial&amp; ParNew&amp;Parallel Scavenge</td>
</tr>
<tr>
<td>-XX:PretenureSizeThreshold</td>
<td>设置直接晋升到老年代的老年代的对象大小，设置这个参数后，大于这个参数的对象将在老年代分配，单位字节</td>
<td>所有经典收集器</td>
</tr>
<tr>
<td>-XX:MaxTenuringThreshold</td>
<td>设置晋升到老年代的年龄，每躲过一次Minor GC对象的年龄＋１，默认此项不设置</td>
<td>所有经典收集器</td>
</tr>
<tr>
<td>-XX:+UseAdaptiveSizePolicy</td>
<td>动态调整java堆区域的大小以及进入老年代的年龄还有停顿时间和吞吐量，特别适合新手</td>
<td>Parallel Scavenge</td>
</tr>
<tr>
<td>-XX:GCTimeRatio</td>
<td>GC时间站总时间的比率，默认为99, 即允许1％的ＧＣ时间</td>
<td>Parallel Scavenge</td>
</tr>
<tr>
<td>-XX:MaxGCPauseMillis</td>
<td>设置ＧＣ最大的停顿时间</td>
<td>Parallel Scavenge</td>
</tr>
<tr>
<td>-XX:CMSInitiatingOccupancyFraction</td>
<td>设置CMS老年代的空间被使用多少时进行收集，默认值为68％</td>
<td>CMS</td>
</tr>
<tr>
<td>-XX:+UseCMSCompactAtFullCollection</td>
<td>完成垃圾回收后是否进行一次内存碎片整理</td>
<td>CMS</td>
</tr>
<tr>
<td>-XX:CMSFullGCsBeforeCompact</td>
<td>设置进行多少次垃圾回收之后再启动一次内存碎片整理</td>
<td>CMS</td>
</tr>
</tbody></table>
<p>列完参数我们发现，大部分的调节参数集中在Paraellel Scavenge 收集器和ＣＭＳ收集器当中，那我们再聊聊这俩个收集器<br></p>
<h4 id="Parallel-Scavenge-收集器"><a href="#Parallel-Scavenge-收集器" class="headerlink" title="Parallel Scavenge 收集器"></a><font color=#3.5169E1>Parallel Scavenge 收集器</font></h4><p>Parallel Scavenge 收集器，恩命名方式突然多了一个Scavenge一定有特殊之处，的确如此，Scavenge 收集器提供了两个重要的可控指标给我们，那就是<font color=#00ff00>吞吐量和停顿时间</font>，这是之前提到过的，拿第一个来说,</p>
<ul>
<li><font color=#00ff00>程序的吞吐量=运行用户代码的时间/(运行用户代码的时间+运行垃圾收集的时间)</font></li>
</ul>
<p>而在Scavenge收集器当中的参数就是<font color=#00ff00>GCTimeRatio</font> 表示GC回收的时间比率，正好和吞吐量取倒，比如说这个参数取19,那么他所占的用户时间比率为１/(1+19),即百分之五<br/><br>而Scavenge的另一个参数便是，<font color=#00ff00>ＭaxGCPauseMills</font> ＧＣ最大的停顿时间，这个没有什么可讲的，单位为毫秒<br/><br>不过这个参数在使用的时候需要注意，停顿时间和吞吐量的参数设置是矛盾的，如果停顿时间设置比较小，虚拟机为了保持回收效率，会增加回收次数，这样回收次数＊回收时间＝总的回收时间，如果停顿时间设太短，回收次数将增加很多，其最后的结果势必会使吞吐量也降下来．<br/><br><font color=#00ff00>UseAdaptiveSizePolicy</font>，这个参数说明的已经很详细，不再多赘述，如果你对收集器不了解，请务必把这项参数打开．</p>
<h4 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a><font color=#3.5169E1>CMS收集器</font></h4><p>CMS收集器，是一个跨时代意义的收集器，因为自它开始，之后的收集器都实现了并发标记并发收集的功能，它自己也如此，但需要注意的是即使并发收集也有一些停顿避免不了(初始标记GC Root对象时，也会产生停顿)．好了，让我们来看看它的参数<br><font color=#00ff00>CMSInitiatingOccupancyFraction</font>　既然是并发执行，那么我们不能等老年代区满了再去执行，因为在回收过程中用户线程还在执行，我们需要预留一部分空间给用户线程，所以才有这个参数，jdk5 默认值为６８，jdk6默认值为９２，jdk6设置有些风险，因为在程序运行过程中，如果回收之后的老年代空间不足以用户使用，会出现＂并发失败＂，这时会临时启用Serial Old收集器，势必会拖慢用户线程的运行.</p>
<p><font color=#00ff00>+UseCMSCompactAtFullCollection　和　CMSFullGCsBeforeCompact</font> 这两个参数同时讲，我们知道ＣＭＳ收集器是基于标记－清除算法的，这可能会导致老年代没有连续的空间来存储大对象而导致full GC.所以就需要通过这两个参数来设置和整理（两个参数在jdk９开始废弃）</p>
<h3 id="最后简单总结一下问题"><a href="#最后简单总结一下问题" class="headerlink" title="最后简单总结一下问题"></a><font color=#3.5169E1>最后简单总结一下问题</font></h3><ul>
<li>安全点有了，为什么需要安全区域？</li>
<li>新生代老年代如何一同收集？</li>
<li>经典收集器的参数？</li>
<li>简单说一下Parallel Scavenge 收集器和ＣＭＳ收集器的特征．</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/06/18/jvm%E7%9A%84%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%88%BD%E5%8F%A3%E8%AE%B2%E8%A7%A3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%AD%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BF%97%E7%A7%B0%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%89%E3%80%87%E5%8F%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/18/jvm%E7%9A%84%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%88%BD%E5%8F%A3%E8%AE%B2%E8%A7%A3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%AD%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BF%97%E7%A7%B0%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%89%E3%80%87%E5%8F%81/" class="post-title-link" itemprop="url">jvm的轻量级爽口讲解--内存管理子系统（俗称垃圾回收）〇叁</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-06-18 20:22:33" itemprop="dateCreated datePublished" datetime="2020-06-18T20:22:33+08:00">2020-06-18</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-06-25 19:05:52" itemprop="dateModified" datetime="2020-06-25T19:05:52+08:00">2020-06-25</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/jvm%E8%99%9A%E6%8B%9F%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">jvm虚拟机</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h4><p><a target="_blank" rel="noopener" href="https://lemcoden.xyz/2020/05/27/jvm的轻量级爽口讲解-内存管理子系统（俗称垃圾回收）〇贰/">jvm的轻量级爽口讲解–内存管理子系统（俗称垃圾回收）〇贰</a><br/><br>其中对象的引用链路描述有误，现已经更改（应该是查找根节点引用的对象，而不是查找引用根节点的对象）</p>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hey！guys，I&#x27;m back,关于之前两篇的blog，博主尽可能进行debug，毕竟一篇好的blog是需要经过不断修改打磨的，</span><br><span class="line">就像我们敲过的代码，如果不去不断的重构，之后必将&quot;积重难返&quot;，然后还有一个，关于图片显示的问题，这个博主也</span><br><span class="line">正在全力解决当中，准备把图床转到国内(此行为需要备案)，域名正在备案中.......请大家耐心等待。好了我们继续上一</span><br><span class="line">篇的问题，上一篇我们留下一个关键的问题，如何进行垃圾内存的并发标记，这次，我们就从这个问题开始。</span><br></pre></td></tr></table></figure>

<h3 id="再聊并发可达性分析"><a href="#再聊并发可达性分析" class="headerlink" title=" 再聊并发可达性分析"></a><font color=#3.5169E1> 再聊并发可达性分析</font></h3><p>上一篇关于并发可达性问题，我们只是简单的一笔带过，这次我们利用对象图，帮我们把问题描述的更加清晰一些，以便于更加清晰的理解和解决。首先我们先对对象在对象图中的颜色进行区分定义：</p>
<ul>
<li>白色：表示对象还没有被垃圾回收器访问过，也没有被标记过。</li>
<li>黑色：表示对象已经被访问且标记为安全存活对象（即不是垃圾对象），不会再进行扫描。</li>
<li>灰色：表示对象已经被访问，但是此对象所引用的其他对象至少有一个还没有被扫描过。只有灰色对象所引用的白色对象扫描完毕，其才能被标记为黑色对象，所以说在标记过程中， <font color=#00ff00>黑色对象不能直接指向白色对象</font>。<br/></li>
</ul>
<p>好的，我们看一下标记过程的抽象图：<br>初始过程，黑色的方框对象为根节点，开始向下查找。<br><br><img src="http://picture.lemcoden.xyz/jvm_memory_manage/memory_sign1.png" alt="memory_sign1"><br>中间标记过程，就像是波纹<del>（dio：纳尼？）</del>一样，以灰色对象为波峰持续推进</br><br><img src="http://picture.lemcoden.xyz/jvm_memory_manage/memory_sign2.png" alt="memory_sign2"><br>最后直到所有的灰色对象都查找不到引用，将最后这批灰色对象标记为黑色。</br><br><img src="http://picture.lemcoden.xyz/jvm_memory_manage/memory_sign3.png" alt="memory_sign3"><br>好了，<font color=#00ff00 >正常的标记过程</font>就是如此，但如果有用户线程来捣乱，那就不一样了<br/><br>那用户线程如何捣乱呢？把正在查找的灰色对象对白色对象切断？我们先试一下，以下虚线为切断的引用<br/><br><img src="http://picture.lemcoden.xyz/jvm_memory_manage/memory_sign4.png" alt="memory_sign4"><br>好像结果并没有什么阻碍，最终查找的结果是正确的，因为用户已经切断了引用，而我们的标记结果也实时作出了改变。<br/><br>那么添加黑色对象对已经查找的白色对象的引用呢？<br/><br><img src="http://picture.lemcoden.xyz/jvm_memory_manage/memory_sign5.png" alt="memory_sign5"><br>也同样的不会出现什么问题，最后的结果和原来一致，那么，如果两个同时叠加呢？<br/><br><img src="http://picture.lemcoden.xyz/jvm_memory_manage/memory_sign6.png" alt="memory_sign6"><br>唉？问题就出来了，如果我们切断灰色对象对白色对象的引用，然后用一个黑色对象引用此白色对象，会让这个白色对象到扫描的最后都不会标记为黑色，但他有黑色对象的引用，照这样的标记回收会使我们的回收掉本应该存活的对象。<br/><br>现在，我们知道，要是并发收集出现错误，必须满足以下两个条件：</p>
<ul>
<li>切断从灰色对象到黑色对象的引用</li>
<li>添加已经扫描过的黑色对象对白色对象的引用</li>
</ul>
<p>只要我们破坏其中一个条件，并发标记便可以实现。<br/><br>破坏第一个条件，当探测到切断灰色对象对白色对象的引用，把这个引用记录下来，然后再以记录过的灰色对象为根节点，再扫描一遍，这种方法叫做<font color=#00ff00 >原始快照</font><br/><br>破坏第二个条件，当添加黑色对象到白色对象的引用时，我们将这个黑色对象重置为灰色对象，再进行查找。这种方法叫做<font color=#00ff00>增量更新</font><br>CMS收集器是基于增量更新来做并发标记的，G1、shennandoah则是用原始快照来实现的，这些收集器，会在之后进行专门的一一讲解。</p>
<h3 id="再聊谁实现"><a href="#再聊谁实现" class="headerlink" title=" 再聊谁实现"></a><font color=#3.5169E1> 再聊谁实现</font></h3><p>好了讲了这么多的理论，我们该讲讲实现了，其实还有一些理论还没有涉及，笔者会在下面以及后几篇补上，现在感觉理论比较枯燥，我们还是聊聊实际的吧。<br/><br>接下来我们会聊到一些垃圾收集器，以及垃圾收集器所用到的之前的理论部分。<br/><br>首先，我们先为垃圾收集器进行分类，Serial、ParNew，Parallel  Scavenge，Serial Old，Parallel Old,CMS收集器，笔者称他们为经典的收集器，因为这些收集器都是专门管理新生代，或者老年代的。<br/><br>而之后出现的G1,shennandoah，ZGC收集器，都是新生代，老年代并用的。<br/><br>而 jdk11出现的Epsilon收集器比较特殊，它不做任何回收动作<del>(我要你有何用？)</del>，至于其作用笔者会在之后描述。<br/><br>目前我们只聊了聊经典收集器的理论，所以我们从经典的收集器开始聊起，<br/></p>
<ul>
<li><font color=#00ff00>  Serial </font>收集器（直译是电视连续剧？重在连续这个词）最早出现的垃圾回收器，它适用于单核的CPU，因此特别适合运行客户端方面的应用，虽然在之后的回收器层出不穷，但是在客户端应用方面，Serial收集器一直最低内存消耗发挥着作用。</li>
<li><font color=#00ff00>  ParNew   </font>收集器，专管新生代的多线程收集器<font color=#00FFFF>JDK1.3</font></li>
<li><font color=#00ff00> Parallel Scavenge  </font>ParNew 收集器的进阶版一般Par开头的，都是多线程收集器，这款收集器主打可控的吞吐量，吞吐量？好像是一个新概念，没错，之前我们讲过，停顿时间是衡量垃圾回收性能的重要指标之一，另一个指标便是吞吐量，他是指用户运行时间在整个程序总运行时间（用户运行时间和垃圾回收时间）的占比。他有自己的参数<font color=#00FFFF>JDK1.4</font></li>
<li><font color=#00ff00> Serial Old</font> 收集器 Serial收集器的老年代版本</li>
<li><font color=#00ff00> Parallel Old </font>收集器专门管理老年代的收集器，在此收集器出现之前ParNew 收集器一直初一比较尴尬的境地，因为作为多线程的新生代收集器，他却只能与单线程的Serial 配合使用<font color=#00FFFF>JDK6</font></li>
<li><font color=#00ff00> CMS 划时代意义的收集器</font>，我们之前所说的并发回收就是从这个收集器开始的，<font color=#00ff00>之前的所有收集器，都只能通过停顿其他用户线程进行标记和回收</font>，但是他是老年代的收集器，其次，作为老年代的收集器，它却无法配合Parallel Scavenge使用。</li>
</ul>
<p>以上就是对我们所讲的理论支撑的垃圾回收器的简单介绍，那我们怎么使用他们呢？<br>很简单，只要启动java的时候加入配置参数就好</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java  -XX:+回收器参数   运行的Main类</span><br></pre></td></tr></table></figure>
<p>那么下面给出的回收器参数</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>UseSerialGC</td>
<td>使用Serial + Serial Old 收集器</td>
</tr>
<tr>
<td>UseParNewGC</td>
<td>使用ParNew + Serial Old 收集器</td>
</tr>
<tr>
<td>UseConcMarkSweepGC</td>
<td>使用ParNew + CMS+Serial Old 的收集器组合进行回收 ,如果CMS收集器并发收集失败，会切换到Serial Old 收集器</td>
</tr>
<tr>
<td>UseParallelGC</td>
<td>使用Parallel Scavenge + Serial Old 收集器</td>
</tr>
<tr>
<td>UseParallelOldGC</td>
<td>使用 Parallel Scavenge + Parallel Old 收集器</td>
</tr>
</tbody></table>
<p>这是我们目前可以通过参数切换到的收集器参数，我们会在下一篇blog，将参数写的更详尽一些，包括一些收集器的配置参数，和JDK9之后的从参数改变。</p>
<h3 id="再聊一下停顿"><a href="#再聊一下停顿" class="headerlink" title="  再聊一下停顿"></a><font color=#3.5169E1>  再聊一下停顿</font></h3><p>我们前面聊了聊并发标记是如何实现的，现在我们聊一下如何让用户线程停顿，为什么要讲这个？因为可能会出现停顿时间异常的现象，如果我们不懂其中的原理将没有办法定位问题所在。<br/></p>
<p>首先我们先退一步，上一篇博客，我们讲到GC ROOT 其中有很多类型的引用都会被作为GC ROOT，那么随着应用的体量的增大，引用数量必定猛增不减，这种时候我们如果在垃圾回收的时候，再去查询GC ROOT的引用，<br/></p>
<p>那必然是不行的，因此，我们需要先整一个数据结构，在类型加载的时候，就把相关的引用写入数据结构中，这样我们不必在海量的引用中查找，只取数据结构里面的数据便可，<font color=#00ff00>JVM最经典的HotSpot虚拟机就是利用OopMap数据结构这样进行工作的</font>。<br/></p>
<p>好了，GC Root的收集问题得到解决了，但是运行过程当中，引用关系肯定会改变，而JVM指令当中，大部分的指令都会造成引用关系的改变，我们不可能在每条指令后面都加一个OopMap结构，那对回收来说，空间成本将会变得额外的高昂。<br/></p>
<p>因此，存储了OopMap数据结构外，在运行时，JVM会选择某些“特殊的位置”来记录引用信息，这些特殊的位置就叫做<font color=#00ff00>安全点</font>,那么安全点应该如何设置呢？<br/></p>
<p>安全点设置既不能太少而使回收程序等待时间过长，又不能太多增大程序运行负荷，因此安全点一般选在可长时间执行的地方，一般在<font color=#00ff00>方法调用、循环跳转、异常跳转</font>等这些可以指令序列复用的地方。<br/></p>
<h3 id="简单总结一下问题"><a href="#简单总结一下问题" class="headerlink" title="简单总结一下问题"></a><font color=#3.5169E1>简单总结一下问题</font></h3><ul>
<li>如何做到并发可达性分析，方法分为哪几种？</li>
<li>前期新生代，老年代分开管理的回收器都有那些？</li>
<li>安全点是什么？一般在哪里设置</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lemcoden</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
