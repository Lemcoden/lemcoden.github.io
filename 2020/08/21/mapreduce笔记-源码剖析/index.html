<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32*32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16*16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.css" integrity="sha256-no0c5ccDODBwp+9hSmV5VvPpKwHCpbVzXHexIkupM6U=" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js" integrity="sha256-a5YRB27CcBwBFcT5EF/f3E4vzIqyHrSR878nseNYw64=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;lemcoden.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Pisces&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;}}</script>
<meta name="description" content="为什么要看源码 1.为了更好的使用框架的Api解决问题,比如说我们遇到一个问题,需要修改mapreduce分片的大小,如果没看过源码,可能会写很多代码,甚至重新调整文件block的大小上传,但是看过源码的都懂,只要简单的修改minSplite和maxSplite这两个配置属性就可以. 2.为了学习框架本身的设计方法,应用到日常开发中. (此次源码分析的hadoop版本为2.7.2)">
<meta property="og:type" content="article">
<meta property="og:title" content="mapreduce笔记-源码剖析">
<meta property="og:url" content="https://lemcoden.github.io/2020/08/21/mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/index.html">
<meta property="og:site_name" content="Lemcoden">
<meta property="og:description" content="为什么要看源码 1.为了更好的使用框架的Api解决问题,比如说我们遇到一个问题,需要修改mapreduce分片的大小,如果没看过源码,可能会写很多代码,甚至重新调整文件block的大小上传,但是看过源码的都懂,只要简单的修改minSplite和maxSplite这两个配置属性就可以. 2.为了学习框架本身的设计方法,应用到日常开发中. (此次源码分析的hadoop版本为2.7.2)">
<meta property="og:locale">
<meta property="article:published_time" content="2020-08-21T09:56:07.000Z">
<meta property="article:modified_time" content="2021-08-08T10:56:55.615Z">
<meta property="article:author" content="Lemcoden">
<meta property="article:tag" content="hadoop生态">
<meta property="article:tag" content="分布式">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://lemcoden.github.io/2020/08/21/mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-Hans&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;lemcoden.github.io&#x2F;2020&#x2F;08&#x2F;21&#x2F;mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90&#x2F;&quot;,&quot;path&quot;:&quot;2020&#x2F;08&#x2F;21&#x2F;mapreduce笔记-源码剖析&#x2F;&quot;,&quot;title&quot;:&quot;mapreduce笔记-源码剖析&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>mapreduce笔记-源码剖析 | Lemcoden</title><script src="/js/config.js"></script>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?5559683623f16c6f10fe5b4f1cb2f062"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Lemcoden</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">来自于大数据攻城狮的分享</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%9C%8B%E6%BA%90%E7%A0%81"><span class="nav-number">1.</span> <span class="nav-text"> 为什么要看源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E7%9C%8B%E6%BA%90%E7%A0%81"><span class="nav-number">2.</span> <span class="nav-text"> 怎么看源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mapreduce%E6%BA%90%E7%A0%81%E6%A2%97%E6%A6%82"><span class="nav-number">3.</span> <span class="nav-text"> mapreduce源码梗概</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#client%E7%AB%AF"><span class="nav-number">4.</span> <span class="nav-text"> client端</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#map%E7%AB%AF%E6%BA%90%E7%A0%81"><span class="nav-number">5.</span> <span class="nav-text"> map端源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%85%A5"><span class="nav-number">6.</span> <span class="nav-text"> 输入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BE%93%E5%87%BA"><span class="nav-number">7.</span> <span class="nav-text"> 输出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reduce%E7%AB%AF%E6%BA%90%E7%A0%81"><span class="nav-number">8.</span> <span class="nav-text"> reduce端源码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">9.</span> <span class="nav-text"> 迭代器的使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AF%94%E8%BE%83%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">10.</span> <span class="nav-text"> 比较器的使用</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Lemcoden"
      src="http://picture.lemcoden.xyz/avater.jpeg">
  <p class="site-author-name" itemprop="name">Lemcoden</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://lemcoden.github.io/2020/08/21/mapreduce%E7%AC%94%E8%AE%B0-%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://picture.lemcoden.xyz/avater.jpeg">
      <meta itemprop="name" content="Lemcoden">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lemcoden">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          mapreduce笔记-源码剖析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-21 17:56:07" itemprop="dateCreated datePublished" datetime="2020-08-21T17:56:07+08:00">2020-08-21</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-08 18:56:55" itemprop="dateModified" datetime="2021-08-08T18:56:55+08:00">2021-08-08</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/hadoop%E7%94%9F%E6%80%81/" itemprop="url" rel="index"><span itemprop="name">hadoop生态</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h4 id="为什么要看源码"><a class="markdownIt-Anchor" href="#为什么要看源码"></a> 为什么要看源码</h4>
<p>1.为了更好的使用框架的Api解决问题,比如说我们遇到一个问题,需要修改mapreduce分片的大小,如果没看过源码,可能会写很多代码,甚至重新调整文件block的大小上传,但是看过源码的都懂,只要简单的修改minSplite和maxSplite这两个配置属性就可以.<br />
2.为了学习框架本身的设计方法,应用到日常开发中.<br />
(此次源码分析的hadoop版本为2.7.2)</p>
<span id="more"></span>
<h4 id="怎么看源码"><a class="markdownIt-Anchor" href="#怎么看源码"></a> 怎么看源码</h4>
<p>要有目的性的的看源码,如果不带目的直接看的话,会很晕,源码一般信息量很大,而且很多部分是没有必要的,我们要取其精髓,忽略与当前目标无关的部分.并将重要的部分记录下来,最好是自己可以用伪代码实现,并且能够讲出其中的逻辑点和技术应用点</p>
<h4 id="mapreduce源码梗概"><a class="markdownIt-Anchor" href="#mapreduce源码梗概"></a> mapreduce源码梗概</h4>
<p>mapreduce笔者目前了解的主要有三部分,client端,map计算端的输入输出,reduce计算端的输入输出.<br />
client端主要验证client端的任务,以及关键切片部分的逻辑<br />
map端和reduce端输入输出,一是看数据格式相关的转换<br />
二是看shuffle的主要流程,看有哪些可以在开发过程中可以微调的地方</p>
<h4 id="client端"><a class="markdownIt-Anchor" href="#client端"></a> client端</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.waitForCompletion(true);</span><br></pre></td></tr></table></figure>
<p>我们编写mapreduce程序的时候到最后执行这个方法的时候,任务才会真正提交,<br />
那我们提交任务之后客户端都是如何做得呢?<br />
点进去查看源码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public boolean waitForCompletion(boolean verbose</span><br><span class="line">                                   ) throws IOException, InterruptedException,</span><br><span class="line">                                            ClassNotFoundException &#123;</span><br><span class="line">    if (state == JobState.DEFINE) &#123;  //检查任务状态,是否允许提交</span><br><span class="line">      submit(); //提交任务方法</span><br><span class="line">    &#125;</span><br><span class="line">    if (verbose) &#123;</span><br><span class="line">      monitorAndPrintJob(); //监控并且获取任务的详细运行信息</span><br><span class="line">    &#125; else ...</span><br><span class="line">    return isSuccessful();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>提交之后,再调用方法获取任务的详细信息,可见这个任务是异步任务.<br />
我们关系心的任务如何提交的,那么就进入submit方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void submit()</span><br><span class="line">       throws IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  setUseNewAPI();  //hadoop1.x和hadoop2.x的mapreduce架构不同,所以这里是新版API</span><br><span class="line">  connect(); //连接yarn resourceManager</span><br><span class="line">  final JobSubmitter submitter =</span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient()); //获取集群HDFS操作对象和Client对象,为以后把分片信息,配置文件,jar包,通过FileSystem上传到hdfs上</span><br><span class="line">  status = ugi.doAs(new PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    public JobStatus run() throws IOException, InterruptedException,</span><br><span class="line">    ClassNotFoundException &#123;</span><br><span class="line">      return submitter.submitJobInternal(Job.this, cluster); //这里就是提交Job的地方</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(&quot;The url to track the job: &quot; + getTrackingURL());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>再进入submitJobInternal方法,然后发现注释这部分很有东西</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Internal method for submitting jobs to the system.</span><br><span class="line">The job submission process involves:</span><br><span class="line">1.Checking the input and output specifications of the job.</span><br><span class="line">2.Computing the InputSplits for the job.</span><br><span class="line">3.Setup the requisite accounting information for the DistributedCache of the job, if necessary.</span><br><span class="line">4.Copying the job&#x27;s jar and configuration to the map-reduce system directory on the distributed file-system.</span><br><span class="line">5.Submitting the job to the JobTracker and optionally monitoring it&#x27;s status.</span><br></pre></td></tr></table></figure>
<p>简单的翻译以下就懂了,里面会</p>
<ol>
<li>检查job输入输出路径</li>
<li>计算分片的大小</li>
<li>有必要的话,为作业的缓存设置账户信息</li>
<li>把job的jar包,配置文件拷贝到hdfs</li>
<li>提交job到JobTracker</li>
</ol>
<p>然后我们再看代码,因为源代码比较多,这里只挑出重要的伪代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">JobStatus submitJobInternal(Job job, Cluster cluster)</span><br><span class="line">  throws ClassNotFoundException, InterruptedException, IOException &#123;</span><br><span class="line">    //validate the jobs output specs</span><br><span class="line">    checkSpecs(job);  //这个就是检查文件路径的方法</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    int maps = writeSplits(job, submitJobDir);//写入切片信息,返回切片数量</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    String queue = conf.get(MRJobConfig.QUEUE_NAME,</span><br><span class="line">        JobConf.DEFAULT_QUEUE_NAME); // 获取任务队列名,源码中有很多这样的获取配置信息的代码,这里只挑出一个说明一下</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    // Write job file to submit dir</span><br><span class="line">     writeConf(conf, submitJobFile); //写入conf文件到hdfs上</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">     //真正提交客户端的方法</span><br><span class="line">     status = submitClient.submitJob(</span><br><span class="line">           jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>client的用户任务,在这里如何调用的基本了解清除了,我们重点看切片是如何写入的,毕竟这是hadoop生态的一个要点:如何通过分片实现计算向数据移动的,点开writeSplite方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,</span><br><span class="line">    Path jobSubmitDir) throws IOException,</span><br><span class="line">    InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">  JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">  int maps;</span><br><span class="line">  if (jConf.getUseNewMapper()) &#123;</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir); //hadoop2.x使用newApi,所以进入这个方法</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    maps = writeOldSplits(jConf, jobSubmitDir);</span><br><span class="line">  &#125;</span><br><span class="line">  return maps;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private &lt;T extends InputSplit&gt;</span><br><span class="line"> int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException,</span><br><span class="line">     InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">       List&lt;InputSplit&gt; splits = input.getSplits(job);//不解释,再进入getSpiltes方法</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>进入之后,发现是InputFormat是个接口,有多个子类,那怎么办?查看子类,有 DB数据库的,有Line管每行记录的,切片当然是以文件系统为依托,所以选FileInputFormat<br />
点进去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException &#123;</span><br><span class="line">  long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">  long maxSize = getMaxSplitSize(job);</span><br><span class="line">  ...</span><br><span class="line">  long blockSize = file.getBlockSize();</span><br><span class="line">  long splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">  ...</span><br><span class="line">  long bytesRemaining = length;</span><br><span class="line">  while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                        blkLocations[blkIndex].getHosts(),</span><br><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">            bytesRemaining -= splitSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return splites;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>最后终于找到了关于切片的代码,首先开头有两个值minSize和maxSize,分别进入这些方法,发现默认的是0,和long型的最大值,并且受SPLIT_MINSIZE(mapreduce.input.fileinputformat.split.minsize)和SPLIT_MAXSIZE(mapreduce.input.fileinputformat.split.maxsize)这两个配置变量控制,然后继续往下走,有个cpmputeSpliteSize方法,用到了minSize和maxSize还有BlockSize,进入之后我们总算知道了切片如何计算大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Math.max(minSize, Math.min(maxSize, blockSize))</span><br></pre></td></tr></table></figure>
<p><font color="#00FF00">它的语义就是以minSize为最小边界,maxSize为最大边界<br />
如果blockSize没有超过最大最小边界,则SpliteSize取BlockSize的值<br />
如果超过边界则取边界值.</font><br/><br />
继续追getSplites的代码<br />
有一个getBlockIndex方法,获取块索引,并且块索引最后会放到切片信息中,<br />
我们进入这个方法发现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">protected int getBlockIndex(BlockLocation[] blkLocations,</span><br><span class="line">                              long offset) &#123;</span><br><span class="line">    for (int i = 0 ; i &lt; blkLocations.length; i++) &#123;</span><br><span class="line">      // is the offset inside this block?</span><br><span class="line">      if ((blkLocations[i].getOffset() &lt;= offset) &amp;&amp;</span><br><span class="line">          (offset &lt; blkLocations[i].getOffset() + blkLocations[i].getLength()))&#123;</span><br><span class="line">        return i;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>注意if判断方法,语义就是如何取得切片的block索引,就是对切片在文件中偏移量,做一次&quot;向下取整&quot;,比如说第二block块的偏移量是50,而第二个切片的偏移量是75,位于第二个和第三个块(100)偏移量之间,也就是说,在真正进行计算的时候,会从块的第50的偏移量读取,<br/><br />
<font color="#00ff00">这就是为什么我们一般把分片大小设置为块大小的倍数,因为这样可以避免交叉读写.</font><br/><br />
最后就是写入分片信息包括分片的hosts,size,filepath,offset<br/><br />
有了这些信息,就可以支持日后的计算能够保证计算程序找到分片的位置,也就是支持计算向数据移动<br/></p>
<h4 id="map端源码"><a class="markdownIt-Anchor" href="#map端源码"></a> map端源码</h4>
<p>首先明确目的,我们Map端的源码是对输入输出进行分析,主要分析map两端的输入输出,</p>
<h4 id="输入"><a class="markdownIt-Anchor" href="#输入"></a> 输入</h4>
<p>已知我们Map是通过MapTask类运行的,那么我们就先进入MapTask类,先找run方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line"> public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)</span><br><span class="line">   throws IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     if (conf.getNumReduceTasks() == 0) &#123;</span><br><span class="line">       mapPhase = getProgress().addPhase(&quot;map&quot;, 1.0f);</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       // If there are reducers then the entire attempt&#x27;s progress will be</span><br><span class="line">       // split between the map phase (67%) and the sort phase (33%).</span><br><span class="line">       mapPhase = getProgress().addPhase(&quot;map&quot;, 0.667f);</span><br><span class="line">       sortPhase  = getProgress().addPhase(&quot;sort&quot;, 0.333f);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line">   if (useNewApi) &#123;</span><br><span class="line">     runNewMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     runOldMapper(job, splitMetaInfo, umbilical, reporter);</span><br><span class="line">   &#125;</span><br><span class="line">   done(umbilical, reporter);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>首先映入眼帘的是getNumReduceTasks(),获取Reduce的数量,如果数量为零则不进行排序计算,不为排序任务分配全中<br />
然后到下面的runNewMapper点进去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">  private &lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;</span><br><span class="line">  void runNewMapper(final JobConf job,</span><br><span class="line">                    final TaskSplitIndex splitIndex,</span><br><span class="line">                    final TaskUmbilicalProtocol umbilical,</span><br><span class="line">                    TaskReporter reporter</span><br><span class="line">                    ) throws IOException, ClassNotFoundException,</span><br><span class="line">                             InterruptedException &#123;</span><br><span class="line">    // make a task context so we can get the classes</span><br><span class="line">    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =</span><br><span class="line">      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,</span><br><span class="line">                                                                  getTaskID(),</span><br><span class="line">                                                                  reporter);</span><br><span class="line">    // make a mapper</span><br><span class="line">    org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt; mapper =</span><br><span class="line">      (org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);</span><br><span class="line">    // make the input format</span><br><span class="line">    org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt; inputFormat =</span><br><span class="line">      (org.apache.hadoop.mapreduce.InputFormat&lt;INKEY,INVALUE&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);//初始化InputFormat</span><br><span class="line">    // rebuild the input split</span><br><span class="line">    org.apache.hadoop.mapreduce.InputSplit split = null;</span><br><span class="line">    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),</span><br><span class="line">        splitIndex.getStartOffset());//获取切片信息,保证自己拿到最近的切片数据.</span><br><span class="line">    LOG.info(&quot;Processing split: &quot; + split);</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.RecordReader&lt;INKEY,INVALUE&gt; input =</span><br><span class="line">      new NewTrackingRecordReader&lt;INKEY,INVALUE&gt;</span><br><span class="line">        (split, inputFormat, reporter, taskContext);</span><br><span class="line"></span><br><span class="line">    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());</span><br><span class="line">    org.apache.hadoop.mapreduce.RecordWriter output = null;</span><br><span class="line"></span><br><span class="line">    // get an output object</span><br><span class="line">    if (job.getNumReduceTasks() == 0) &#123;</span><br><span class="line">      output =</span><br><span class="line">        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      output = new NewOutputCollector(taskContext, job, umbilical, reporter);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.MapContext&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;</span><br><span class="line">    mapContext =</span><br><span class="line">      new MapContextImpl&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;(job, getTaskID(),</span><br><span class="line">          input, output,</span><br><span class="line">          committer,</span><br><span class="line">          reporter, split);</span><br><span class="line"></span><br><span class="line">    org.apache.hadoop.mapreduce.Mapper&lt;INKEY,INVALUE,OUTKEY,OUTVALUE&gt;.Context</span><br><span class="line">        mapperContext =</span><br><span class="line">          new WrappedMapper&lt;INKEY, INVALUE, OUTKEY, OUTVALUE&gt;().getMapContext(</span><br><span class="line">              mapContext);</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">      input.initialize(split, mapperContext);</span><br><span class="line">      mapper.run(mapperContext);</span><br><span class="line">      mapPhase.complete();</span><br><span class="line">      setPhase(TaskStatus.Phase.SORT);</span><br><span class="line">      statusUpdate(umbilical);</span><br><span class="line">      input.close();</span><br><span class="line">      input = null;</span><br><span class="line">      output.close(mapperContext);</span><br><span class="line">      output = null;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      closeQuietly(input);</span><br><span class="line">      closeQuietly(output, mapperContext);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>我们先看下面try cacth里面的东西,一般看源码,try语句块里面的东西是比较重要的,<br />
在try语句块当中,我们看到了,mapper对象通过run方法运行我们开发编写的map方法,<br />
并且且有自己的输入输出.<br />
然后从头开始捋,首先通过反射将我们编写的Map对象赋值给Mapper,中间注释的跳过,直接开门见山,我们看一下Mapper的输入类,进入到NewTrackingRecordReader</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NewTrackingRecordReader(org.apache.hadoop.mapreduce.InputSplit split,</span><br><span class="line">       org.apache.hadoop.mapreduce.InputFormat&lt;K, V&gt; inputFormat,</span><br><span class="line">       TaskReporter reporter,</span><br><span class="line">       org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)</span><br><span class="line">       ...</span><br><span class="line">       throws InterruptedException, IOException &#123;</span><br><span class="line">     this.real = inputFormat.createRecordReader(split, taskContext);</span><br><span class="line">     ...</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>进入后发现这是个包装类,有nextKeyalue方法获取我们Map中所需要的键值对,而他调用的是real对象的nextKeyValue,<br />
而real对象就是我们的LineRecordReader类型.进入有一个初始化类initialize</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public void initialize(InputSplit genericSplit,</span><br><span class="line">                       TaskAttemptContext context) throws IOException &#123;</span><br><span class="line">  FileSplit split = (FileSplit) genericSplit;</span><br><span class="line">  Configuration job = context.getConfiguration();</span><br><span class="line">  this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);</span><br><span class="line">  start = split.getStart();</span><br><span class="line">  end = start + split.getLength();</span><br><span class="line">  final Path file = split.getPath();</span><br><span class="line"></span><br><span class="line">  // open the file and seek to the start of the split</span><br><span class="line">  final FileSystem fs = file.getFileSystem(job);</span><br><span class="line">  fileIn = fs.open(file);</span><br><span class="line">  ...</span><br><span class="line">    fileIn.seek(start);</span><br><span class="line">  ...</span><br><span class="line">  if (start != 0) &#123;</span><br><span class="line">     start += in.readLine(new Text(), 0,maxBytesToConsume(start));</span><br><span class="line">   &#125;</span><br><span class="line">   this.pos = start;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过seek方法获从自己相应的切片偏移量开始读取信息,<br />
最后一个判断是,默认跳过第一条数据的读取,因为切块的原因很有可能第一条信息不完整.然后我们知道,NewTrackingRecordReader在Context对象里而我们的LineRecordReader在NewTrackingRecordReader当中,所以其实最后context对象调用的nextKeyValue其实调用的是LineRecordReader的方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public boolean nextKeyValue() throws IOException &#123;</span><br></pre></td></tr></table></figure>
<pre><code>key.set(pos);
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</code></pre>
<p>而在nextKeyValue里面有一个key.set(pos)其实就是文件的行号赋值给key</p>
<h4 id="输出"><a class="markdownIt-Anchor" href="#输出"></a> 输出</h4>
<p>好了,输入看完了我们再看一下输出,重新回到MapTask类,我们点开输出类,NewOutputCollector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                      JobConf job,</span><br><span class="line">                      TaskUmbilicalProtocol umbilical,</span><br><span class="line">                      TaskReporter reporter</span><br><span class="line">                      ) throws IOException, ClassNotFoundException &#123;</span><br><span class="line">     collector = createSortingCollector(job, reporter);//获取排序的数据收集器</span><br><span class="line">     partitions = jobContext.getNumReduceTasks();//根据reduce数量进行分区,如果分区数量等于1使用默认的分区器将数据分区为一</span><br><span class="line">     if (partitions &gt; 1) &#123;</span><br><span class="line">       partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">         ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       partitioner = new org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public int getPartition(K key, V value, int numPartitions) &#123;</span><br><span class="line">           return partitions - 1;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>代码的中文注释已经比较详细了,我们继续走,看看排序收集器里面都是什么东西<br/><br />
打开createSortingCollector</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private &lt;KEY, VALUE&gt; MapOutputCollector&lt;KEY, VALUE&gt;</span><br><span class="line">         createSortingCollector(JobConf job, TaskReporter reporter) &#123;</span><br><span class="line">     Class&lt;?&gt;[] collectorClasses = job.getClasses(</span><br><span class="line">      JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);</span><br><span class="line">    int remainingCollectors = collectorClasses.length;</span><br><span class="line">    Exception lastException = null;</span><br><span class="line">    for (Class clazz : collectorClasses) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        if (!MapOutputCollector.class.isAssignableFrom(clazz)) &#123;</span><br><span class="line">          throw new IOException(&quot;Invalid output collector class: &quot; + clazz.getName() +</span><br><span class="line">            &quot; (does not implement MapOutputCollector)&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        Class&lt;? extends MapOutputCollector&gt; subclazz =</span><br><span class="line">          clazz.asSubclass(MapOutputCollector.class);</span><br><span class="line">        LOG.debug(&quot;Trying map output collector class: &quot; + subclazz.getName());</span><br><span class="line">        MapOutputCollector&lt;KEY, VALUE&gt; collector =</span><br><span class="line">          ReflectionUtils.newInstance(subclazz, job);</span><br><span class="line">        collector.init(context);</span><br><span class="line">        LOG.info(&quot;Map output collector class = &quot; + collector.getClass().getName());</span><br><span class="line">        return collector;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>最后知道了,输出数据的排序收集器就是唤醒缓冲区MapOutputBuffer的子类,打开他的init方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">public void init(MapOutputCollector.Context context</span><br><span class="line">                    ) throws IOException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      ...</span><br><span class="line">      //sanity checks</span><br><span class="line">      final float spillper =</span><br><span class="line">        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);</span><br><span class="line">      final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);     </span><br><span class="line">      ...</span><br><span class="line">      sorter = ReflectionUtils.newInstance(job.getClass(&quot;map.sort.class&quot;,</span><br><span class="line">            QuickSort.class, IndexedSorter.class), job);</span><br><span class="line">    </span><br><span class="line">      ...</span><br><span class="line">      // k/v serialization</span><br><span class="line">      comparator = job.getOutputKeyComparator();</span><br><span class="line">      keyClass = (Class&lt;K&gt;)job.getMapOutputKeyClass();</span><br><span class="line">      valClass = (Class&lt;V&gt;)job.getMapOutputValueClass();</span><br><span class="line">      serializationFactory = new SerializationFactory(job);</span><br><span class="line">      keySerializer = serializationFactory.getSerializer(keyClass);</span><br><span class="line">      keySerializer.open(bb);</span><br><span class="line">      valSerializer = serializationFactory.getSerializer(valClass);</span><br><span class="line">      valSerializer.open(bb);</span><br><span class="line">    </span><br><span class="line">      ...</span><br><span class="line">      // compression</span><br><span class="line">      if (job.getCompressMapOutput()) &#123;</span><br><span class="line">        Class&lt;? extends CompressionCodec&gt; codecClass =</span><br><span class="line">          job.getMapOutputCompressorClass(DefaultCodec.class);</span><br><span class="line">        codec = ReflectionUtils.newInstance(codecClass, job);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        codec = null;</span><br><span class="line">      &#125;</span><br><span class="line">      ...</span><br><span class="line">      if (combinerRunner != null) &#123;</span><br><span class="line">        final Counters.Counter combineOutputCounter =</span><br><span class="line">          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);</span><br><span class="line">        combineCollector= new CombineOutputCollector&lt;K,V&gt;(combineOutputCounter, reporter, job);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        combineCollector = null;</span><br><span class="line">      &#125;</span><br><span class="line">      ...</span><br><span class="line">      spillThread.setDaemon(true);</span><br><span class="line">      spillThread.setName(&quot;SpillThread&quot;);</span><br><span class="line">      spillLock.lock();</span><br><span class="line">      try &#123;</span><br><span class="line">        spillThread.start();</span><br><span class="line">        while (!spillThreadRunning) &#123;</span><br><span class="line">          spillDone.await();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; catch (InterruptedException e) &#123;</span><br><span class="line">        throw new IOException(&quot;Spill thread failed to initialize&quot;, e);</span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        spillLock.unlock();</span><br><span class="line">      &#125;</span><br><span class="line">      if (sortSpillException != null) &#123;</span><br><span class="line">        throw new IOException(&quot;Spill thread failed to initialize&quot;,</span><br><span class="line">            sortSpillException);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面的粗略的的分组,代码分别是</p>
<ul>
<li>设置缓冲取大小和溢写百分比(默认100M和0.8)</li>
<li>设置缓冲区数据的排序类(默认快速排序)</li>
<li>获取排序比较器(优先获取设置的比较类,没有的话取Writable类型默认的比较器)</li>
<li>将keyvalue键值对序列化</li>
<li>判断是否启用combiner,如果溢写的小文件数量超过3,则启用combiner合并</li>
<li>获取压缩对象</li>
<li>开启溢写线程</li>
</ul>
<p>这里多嘴几句,因为篇幅有限,先写下buffer的一些特性,以后可以在这个类的源码中验证:</p>
<ul>
<li>buffer本质还是字节数组</li>
<li>buffer有赤道的概念,即分界点,一边输入数据,一边输入索引</li>
<li>索引:固定宽度:16字节,4个int(partition,keystart,valuestart,valuelenth)</li>
<li>combiner默认发生在溢写之前,排序之后</li>
</ul>
<h4 id="reduce端源码"><a class="markdownIt-Anchor" href="#reduce端源码"></a> reduce端源码</h4>
<p>我们先看Reducer类的注释,头注释翻译过来大概意思就是<br/><br />
reducer主要做两件事,一件是拉取shuffle的数据,一件是对数据进行sort,这里的排序不是对数据进行在排序,因为map已经对数据进行过排序了,这里是对map排序过的数据文件进行归并.<br />
好了要点说完了,我们直接看ReudceTask的run方法<br />
其中有一句代码是这样的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">    RawKeyValueIterator rIter = null;</span><br><span class="line">    ShuffleConsumerPlugin shuffleConsumerPlugin = null;</span><br><span class="line">    shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line">    ShuffleConsumerPlugin.Context shuffleContext =</span><br><span class="line">      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical,</span><br><span class="line">                  super.lDirAlloc, reporter, codec,</span><br><span class="line">                  combinerClass, combineCollector,</span><br><span class="line">                  spilledRecordsCounter, reduceCombineInputCounter,</span><br><span class="line">                  shuffledMapsCounter,</span><br><span class="line">                  reduceShuffleBytes, failedShuffleCounter,</span><br><span class="line">                  mergedMapOutputsCounter,</span><br><span class="line">                  taskStatus, copyPhase, sortPhase, this,</span><br><span class="line">                  mapOutputFile, localMapFiles);</span><br><span class="line">    shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line"></span><br><span class="line">rIter = shuffleConsumerPlugin.run();</span><br></pre></td></tr></table></figure>
<p>最后一句,通过shfulle插件获取迭代器,我们知道基本reduce的数据都是通过迭代器获取的<br/></p>
<h4 id="迭代器的使用"><a class="markdownIt-Anchor" href="#迭代器的使用"></a> 迭代器的使用</h4>
<p>为什么使用迭代器呢?因为我不可能将数据一次性的装进内存里,最好是通过迭代器维护一个对文件的指针,这样不仅遍历和实现分离,而且谁想要读取这个文件只要生成一个迭代器,维护自己的指针就可以,不会出现强指针或者同一个数据文件占多份内存的情况.<br />
然后我们追踪Reducer的的迭代器类,追踪路径如下<br />
context.getValues().iterator();  -&gt; ReudceContext -&gt; ReduceContextImpl<br />
进入reduce实现类之后最后有一个getValues方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public</span><br><span class="line">Iterable&lt;VALUEIN&gt; getValues() throws IOException, InterruptedException &#123;</span><br><span class="line">  return iterable;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>他返回一个Iterable对象,而Iterable只有一个方法,返回iterator对象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private ValueIterator iterator = new ValueIterator();</span><br><span class="line"> @Override</span><br><span class="line">    public Iterator&lt;VALUEIN&gt; iterator() &#123;</span><br><span class="line">      return iterator;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>而iterator绝对有hasNext对象和next方法(迭代器模式常识)<br />
我们看一下他的hasNext方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">   public boolean hasNext() &#123;</span><br><span class="line">     try &#123;</span><br><span class="line">       if (inReset &amp;&amp; backupStore.hasNext()) &#123;</span><br><span class="line">         return true;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125; catch (Exception e) &#123;</span><br><span class="line">       e.printStackTrace();</span><br><span class="line">       throw new RuntimeException(&quot;hasNext failed&quot;, e);</span><br><span class="line">     &#125;</span><br><span class="line">     return firstValue || nextKeyIsSame;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后有一个boolean值nextKeyIsSame,我们先记住它然后我们看ReducerContextImpl的另一个方法nextkey方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public boolean nextKey() throws IOException,InterruptedException &#123;</span><br><span class="line">    while (hasMore &amp;&amp; nextKeyIsSame) &#123;</span><br><span class="line">      nextKeyValue();</span><br><span class="line">    &#125;</span><br><span class="line">    if (hasMore) &#123;</span><br><span class="line">      if (inputKeyCounter != null) &#123;</span><br><span class="line">        inputKeyCounter.increment(1);</span><br><span class="line">      &#125;</span><br><span class="line">      return nextKeyValue();</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return false;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>最后nextKey方法会调用nextkeyValue,这里只给出nextKeyValue的最后一句代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0,</span><br><span class="line">                                     currentRawKey.getLength(),</span><br><span class="line">                                     nextKey.getData(),</span><br><span class="line">                                     nextKey.getPosition(),</span><br><span class="line">                                     nextKey.getLength() - nextKey.getPosition()</span><br><span class="line">                                         ) == 0;</span><br></pre></td></tr></table></figure>
<p>他会他通过判断器进行判断,下一个key是否和现在的key相等,把结果值赋值给nextKeyIsSame,对就是刚刚记住的nextKeyIsSame.<br />
也就是说,我们调用的values.hasNext方法,会判断nextKeyIsSame,下一个key是否相同,不同则返回false,触发reducor再次重新调用reduce方法.</p>
<h4 id="比较器的使用"><a class="markdownIt-Anchor" href="#比较器的使用"></a> 比较器的使用</h4>
<p>我们再看一下,ReducerTask的run方法,这里给出关键点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">RawComparator comparator = job.getOutputValueGroupingComparator();</span><br><span class="line"></span><br><span class="line">public RawComparator getOutputValueGroupingComparator() &#123;</span><br><span class="line">    Class&lt;? extends RawComparator&gt; theClass = getClass(</span><br><span class="line">      JobContext.GROUP_COMPARATOR_CLASS, null, RawComparator.class);</span><br><span class="line">    if (theClass == null) &#123;</span><br><span class="line">      return getOutputKeyComparator();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return ReflectionUtils.newInstance(theClass, this);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这个是获取分组比较器的方法,优先使用用户的分组比较器,如果用户的分组比较器为null,则使用默认的key的Writable类型包含的比较器.<br />
而reduce也有排序比较器通过getOutputKeyComparator()获取,<br />
再加上map的排序比较器,我们有三个比较器可以自定义也可取默认的比较器,mapreduce给了我们很灵活的选择取加工数据.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/hadoop%E7%94%9F%E6%80%81/" rel="tag"># hadoop生态</a>
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag"># 分布式</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/08/19/mapreduce%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/" rel="prev" title="mapreduce&yarn笔记总结 01">
                  <i class="fa fa-chevron-left"></i> mapreduce&yarn笔记总结 01
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/08/27/hive-%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" rel="next" title="hive-笔记总结">
                  hive-笔记总结 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lemcoden</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="0" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  <script class="next-config" data-name="mermaid" type="application/json">{&quot;enable&quot;:true,&quot;theme&quot;:&quot;forest&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mermaid@8.9.3&#x2F;dist&#x2F;mermaid.min.js&quot;,&quot;integrity&quot;:&quot;sha256-OyJHvRcZHaRR6Ig73ppxF4QXk8HzvfgTprRWkulCkfY&#x3D;&quot;}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script class="next-config" data-name="nprogress" type="application/json">{&quot;enable&quot;:true,&quot;spinner&quot;:true}</script>
  <script src="/js/third-party/nprogress.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;js&quot;:{&quot;url&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;,&quot;integrity&quot;:&quot;sha256-ncNI9OXOS5Ek4tzVYiOMmN&#x2F;KKCPZ6V0Cpv2P&#x2F;zHntiA&#x3D;&quot;}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
